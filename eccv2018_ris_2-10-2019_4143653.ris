TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Super-Identity Convolutional Neural Network for Face Hallucination
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Kaipeng
A1  - Zhang, Zhanpeng
A1  - Cheng, Chia-Wen
A1  - Hsu, Winston H.
A1  - Qiao, Yu
A1  - Liu, Wei
A1  - Zhang, Tong
AD  - National Taiwan University, Taipei, TaiwanSenseTime Group Limited, Beijing, ChinaThe University of Texas at Austin, Austin; TX, United StatesShenzhen Key Lab of Computer Vision and Pattern Recognition, Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, ChinaTencent AI Lab, Beijing, China
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978867
SP  - 196
EP  - 211
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Face hallucination is a generative task to super-resolve the facial image with low resolution while human perception of face heavily relies on identity information. However, previous face hallucination approaches largely ignore facial identity recovery. This paper proposes Super-Identity Convolutional Neural Network (SICNN) to recover identity information for generating faces closed to the real identity. Specifically, we define a super-identity loss to measure the identity difference between a hallucinated face and its corresponding high-resolution face within the hypersphere identity metric space. However, directly using this loss will lead to a Dynamic Domain Divergence problem, which is caused by the large margin between the high-resolution domain and the hallucination domain. To overcome this challenge, we present a domain-integrated training approach by constructing a robust identity metric for faces from these two domains. Extensive experimental evaluations demonstrate that the proposed SICNN achieves superior visual quality over the state-of-the-art methods on a challenging task to super-resolve 12  14 faces with an 8  upscaling factor. In addition, SICNN significantly improves the recognizability of ultra-low-resolution faces.  2018, Springer Nature Switzerland AG.
KW  - Neural networks
KW  - Computer vision
KW  - Convolution
U2  - Convolutional neural network
U2  - Divergence problems
U2  - Experimental evaluation
U2  - Face hallucination
U2  - Identity information
U2  - State-of-the-art methods
U2  - Super identity
U2  - Visual qualities
DO  - 10.1007/978-3-030-01252-6_12
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_12
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Mutual Learning to Adapt for Joint Human Parsing and Pose Estimation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Nie, Xuecheng
A1  - Feng, Jiashi
A1  - Yan, Shuicheng
AD  - ECE Department, National University of Singapore, Singapore, SingaporeQihoo 360 AI Institute, Beijing, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976868
SP  - 519
EP  - 534
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper presents a novel Mutual Learning to Adapt model (MuLA) for joint human parsing and pose estimation. It effectively exploits mutual benefits from both tasks and simultaneously boosts their performance. Different from existing post-processing or multi-task learning based methods, MuLA predicts dynamic task-specific model parameters via recurrently leveraging guidance information from its parallel tasks. Thus MuLA can fast adapt parsing and pose models to provide more powerful representations by incorporating information from their counterparts, giving more robust and accurate results. MuLA is implemented with convolutional neural networks and end-to-end trainable. Comprehensive experiments on benchmarks LIP and extended PASCAL-Person-Part demonstrate the effectiveness of the proposed MuLA model with superior performance to well established baselines.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Benchmarking
KW  - Neural networks
U2  - Convolutional neural network
U2  - Guidance information
U2  - Human parsing
U2  - Human pose estimations
U2  - Multitask learning
U2  - Mutual learning
U2  - Pose estimation
U2  - Post processing
DO  - 10.1007/978-3-030-01228-1_31
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_31
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Pose Partition Networks for Multi-person Pose Estimation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Nie, Xuecheng
A1  - Feng, Jiashi
A1  - Xing, Junliang
A1  - Yan, Shuicheng
AD  - ECE Department, National University of Singapore, Singapore, SingaporeInstitute of Automation, Chinese Academy of Sciences, Beijing, ChinaQihoo 360 AI Institute, Beijing, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976880
SP  - 705
EP  - 720
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper proposes a novel Pose Partition Network (PPN) to address the challenging multi-person pose estimation problem. The proposed PPN is favorably featured by low complexity and high accuracy of joint detection and partition. In particular, PPN performs dense regressions from global joint candidates within a specific embedding space, which is parameterized by centroids of persons, to efficiently generate robust person detection and joint partition. Then, PPN infers body joint configurations through conducting graph partition for each person detection locally, utilizing reliable global affinity cues. In this way, PPN reduces computation complexity and improves multi-person pose estimation significantly. We implement PPN with the Hourglass architecture as the backbone network to simultaneously learn joint detector and dense regressor. Extensive experiments on benchmarks MPII Human Pose Multi-Person, extended PASCAL-Person-Part, and WAF show the efficiency of PPN with new state-of-the-art performance.  2018, Springer Nature Switzerland AG.
KW  - Complex networks
KW  - Benchmarking
KW  - Computer vision
U2  - Back-bone network
U2  - Computation complexity
U2  - Dense regression
U2  - Joint configuration
U2  - Joint detection
U2  - Person detection
U2  - Pose estimation
U2  - State-of-the-art performance
DO  - 10.1007/978-3-030-01228-1_42
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_42
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Highly-economized multi-view binary compression for scalable image clustering
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Zheng
A1  - Liu, Li
A1  - Qin, Jie
A1  - Zhu, Fan
A1  - Shen, Fumin
A1  - Xu, Yong
A1  - Shao, Ling
A1  - Shen, Heng Tao
AD  - Harbin Institute of Technology (Shenzhen), Shenzhen, ChinaThe University of Queensland, Brisbane, AustraliaInception Institute of Artificial Intelligence, Abu Dhabi, United Arab EmiratesComputer Vision Laboratory, ETH Zurich, Zurich, SwitzerlandUniversity of Electronic Science and Technology of China, Chengdu, China
VL  - 11216 LNCS
PY  - 2018
U1  - 20184305977469
SP  - 731
EP  - 748
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - How to economically cluster large-scale multi-view images is a long-standing problem in computer vision. To tackle this challenge, we introduce a novel approach named Highly-economized Scalable Image Clustering (HSIC) that radically surpasses conventional image clustering methods via binary compression. We intuitively unify the binary representation learning and efficient binary cluster structure learning into a joint framework. In particular, common binary representations are learned by exploiting both sharable and individual information across multiple views to capture their underlying correlations. Meanwhile, cluster assignment with robust binary centroids is also performed via effective discrete optimization under 21 -norm constraint. By this means, heavy continuous-valued Euclidean distance computations can be successfully reduced by efficient binary XOR operations during the clustering procedure. To our best knowledge, HSIC is the first binary clustering work specifically designed for scalable multi-view image clustering. Extensive experimental results on four large-scale image datasets show that HSIC consistently outperforms the state-of-the-art approaches, whilst significantly reducing computational time and memory footprint.  Springer Nature Switzerland AG 2018.
KW  - Image compression
KW  - Cluster analysis
KW  - Computer vision
KW  - Optimization
U2  - Binary clustering
U2  - Binary representations
U2  - Computational time and memory
U2  - Euclidean distance computations
U2  - Image clustering
U2  - Large-scale image datasets
U2  - Multi-views
U2  - State-of-the-art approach
DO  - 10.1007/978-3-030-01258-8_44
L2  - http://dx.doi.org/10.1007/978-3-030-01258-8_44
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Semi-dense 3D Reconstruction with a Stereo Event Camera
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhou, Yi
A1  - Gallego, Guillermo
A1  - Rebecq, Henri
A1  - Kneip, Laurent
A1  - Li, Hongdong
A1  - Scaramuzza, Davide
AD  - Australian National University, Canberra, AustraliaAustralian Centre for Robotic Vision, Brisbane, AustraliaDepartments of Informatics and Neuroinformatics, University of Zurich and ETH Zurich, Zurich, SwitzerlandSchool of Information Science and Technology, ShanghaiTech University, Shanghai, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977692
SP  - 242
EP  - 258
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Event cameras are bio-inspired sensors that offer several advantages, such as low latency, high-speed and high dynamic range, to tackle challenging scenarios in computer vision. This paper presents a solution to the problem of 3D reconstruction from data captured by a stereo event-camera rig moving in a static scene, such as in the context of stereo Simultaneous Localization and Mapping. The proposed method consists of the optimization of an energy function designed to exploit small-baseline spatio-temporal consistency of events triggered across both stereo image planes. To improve the density of the reconstruction and to reduce the uncertainty of the estimation, a probabilistic depth-fusion strategy is also developed. The resulting method has no special requirements on either the motion of the stereo event-camera rig or on prior knowledge about the scene. Experiments demonstrate our method can deal with both texture-rich scenes as well as sparse scenes, outperforming state-of-the-art stereo methods based on event data image representations.  2018, Springer Nature Switzerland AG.
KW  - Stereo image processing
KW  - Cameras
KW  - Computer vision
KW  - Image reconstruction
U2  - 3D reconstruction
U2  - Bioinspired sensors
U2  - Energy functions
U2  - Fusion strategies
U2  - High dynamic range
U2  - Simultaneous localization and mapping
U2  - Spatio-temporal consistencies
U2  - State of the art
DO  - 10.1007/978-3-030-01246-5_15
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_15
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Coloring with words: Guiding image colorization through text-based palette generation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Bahng, Hyojin
A1  - Yoo, Seungjoo
A1  - Cho, Wonwoong
A1  - Park, David Keetae
A1  - Wu, Ziming
A1  - Ma, Xiaojuan
A1  - Choo, Jaegul
AD  - Korea University, Seoul, Korea, Republic ofHong Kong University of Science and Technology, Hong KongClova AI Research, NAVER Corporation, Bundang-gu, Korea, Republic of
VL  - 11216 LNCS
PY  - 2018
U1  - 20184305977450
SP  - 443
EP  - 459
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper proposes a novel approach to generate multiple color palettes that reflect the semantics of input text and then colorize a given grayscale image according to the generated color palette. In contrast to existing approaches, our model can understand rich text, whether it is a single word, a phrase, or a sentence, and generate multiple possible palettes from it. For this task, we introduce our manually curated dataset called Palette-and-Text (PAT). Our proposed model called Text2Colors consists of two conditional generative adversarial networks: the text-to-palette generation networks and the palette-based colorization networks. The former captures the semantics of the text input and produce relevant color palettes. The latter colorizes a grayscale image using the generated color palette. Our evaluation results show that people preferred our generated palettes over ground truth palettes and that our model can effectively reflect the given palette when colorizing an image.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Color
KW  - Semantics
U2  - Adversarial networks
U2  - Color palette
U2  - Evaluation results
U2  - Gray-scale images
U2  - Ground truth
U2  - Image colorizations
U2  - Single words
U2  - Text input
DO  - 10.1007/978-3-030-01258-8_27
L2  - http://dx.doi.org/10.1007/978-3-030-01258-8_27
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Switchable temporal propagation network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Sifei
A1  - Zhong, Guangyu
A1  - De Mello, Shalini
A1  - Gu, Jinwei
A1  - Jampani, Varun
A1  - Yang, Ming-Hsuan
A1  - Kautz, Jan
AD  - NVIDIA, Santa Clara, United StatesUC Merced, Merced, United StatesDalian University of Technology, Dalian, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977629
SP  - 89
EP  - 104
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Videos contain highly redundant information between frames. Such redundancy has been studied extensively in video compression and encoding, but is less explored for more advanced video processing. In this paper, we propose a learnable unified framework for propagating a variety of visual properties of video images, including but not limited to color, high dynamic range (HDR), and segmentation mask, where the properties are available for only a few key-frames. Our approach is based on a temporal propagation network (TPN), which models the transition-related affinity between a pair of frames in a purely data-driven manner. We theoretically prove two essential properties of TPN: (a) by regularizing the global transformation matrix as orthogonal, the style energy of the property can be well preserved during propagation; and (b) such regularization can be achieved by the proposed switchable TPN with bi-directional training on pairs of frames. We apply the switchable TPN to three tasks: colorizing a gray-scale video based on a few colored key-frames, generating an HDR video from a low dynamic range (LDR) video and a few HDR frames, and propagating a segmentation mask from the first frame in videos. Experimental results show that our approach is significantly more accurate and efficient than the state-of-the-art methods.  Springer Nature Switzerland AG 2018.
KW  - Video signal processing
KW  - Computer vision
KW  - Image compression
KW  - Image segmentation
KW  - Linear transformations
U2  - Global transformation
U2  - High dynamic range
U2  - Low dynamic range
U2  - Segmentation masks
U2  - State-of-the-art methods
U2  - Temporal propagation
U2  - Unified framework
U2  - Visual properties
DO  - 10.1007/978-3-030-01234-2_6
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_6
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Multi-fiber Networks for Video Recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Yunpeng
A1  - Kalantidis, Yannis
A1  - Li, Jianshu
A1  - Yan, Shuicheng
A1  - Feng, Jiashi
AD  - National University of Singapore, Singapore, SingaporeFacebook Research, Menlo Park, United StatesQihoo 360 AI Institute, Beijing, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977700
SP  - 364
EP  - 380
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we aim to reduce the computational cost of spatio-temporal deep neural networks, making them run as fast as their 2D counterparts while preserving state-of-the-art accuracy on video recognition benchmarks. To this end, we present the novel Multi-Fiber architecture that slices a complex neural network into an ensemble of lightweight networks or fibers that run through the network. To facilitate information flow between fibers we further incorporate multiplexer modules and end up with an architecture that reduces the computational cost of 3D networks by an order of magnitude, while increasing recognition performance at the same time. Extensive experimental results show that our multi-fiber architecture significantly boosts the efficiency of existing convolution networks for both image and video recognition tasks, achieving state-of-the-art performance on UCF-101, HMDB-51 and Kinetics datasets. Our proposed model requires over 9  and 13  less computations than the I3D [1] and R(2+1)D [2] models, respectively, yet providing higher accuracy.  2018, Springer Nature Switzerland AG.
KW  - Network architecture
KW  - Classification (of information)
KW  - Computer vision
KW  - Cost reduction
KW  - Deep learning
KW  - Deep neural networks
KW  - Fibers
KW  - Image recognition
KW  - Neural networks
U2  - Action recognition
U2  - Complex neural networks
U2  - Computational costs
U2  - Information flows
U2  - State of the art
U2  - State-of-the-art performance
U2  - Video
U2  - Video recognition
DO  - 10.1007/978-3-030-01246-5_22
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_22
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - DeepIM: Deep iterative matching for 6D pose estimation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Yi
A1  - Wang, Gu
A1  - Ji, Xiangyang
A1  - Xiang, Yu
A1  - Fox, Dieter
AD  - Tsinghua University and BNRist, Beijing, ChinaUniversity of Washington and NVIDIA Research, Seattle, United States
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977421
SP  - 695
EP  - 711
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Estimating the 6D pose of objects from images is an important problem in various applications such as robot manipulation and virtual reality. While direct regression of images to object poses has limited accuracy, matching rendered images of an object against the input image can produce accurate results. In this work, we propose a novel deep neural network for 6D pose matching named DeepIM. Given an initial pose estimation, our network is able to iteratively refine the pose by matching the rendered image against the observed image. The network is trained to predict a relative pose transformation using an untangled representation of 3D location and 3D orientation and an iterative training process. Experiments on two commonly used benchmarks for 6D pose estimation demonstrate that DeepIM achieves large improvements over state-of-the-art methods. We furthermore show that DeepIM is able to match previously unseen objects.  Springer Nature Switzerland AG 2018.
KW  - Rendering (computer graphics)
KW  - Computer vision
KW  - Deep neural networks
KW  - Iterative methods
KW  - Object recognition
KW  - Virtual reality
U2  - 3d object recognition
U2  - Iterative matching
U2  - Object pose
U2  - Pose estimation
U2  - Rendered images
U2  - Robot manipulation
U2  - State-of-the-art methods
U2  - Training process
DO  - 10.1007/978-3-030-01231-1_42
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_42
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Multi-scale Spatially-Asymmetric Recalibration for Image Classification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Yan
A1  - Xie, Lingxi
A1  - Qiao, Siyuan
A1  - Zhang, Ya
A1  - Zhang, Wenjun
A1  - Yuille, Alan L.
AD  - Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, ChinaDepartment of Computer Science, The Johns Hopkins University, Baltimore, United States
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008952
SP  - 523
EP  - 539
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Convolution is spatially-symmetric, i.e., the visual features are independent of its position in the image, which limits its ability to utilize contextual cues for visual recognition. This paper addresses this issue by introducing a recalibration process, which refers to the surrounding region of each neuron, computes an importance value and multiplies it to the original neural response. Our approach is named multi-scale spatially-asymmetric recalibration (MS-SAR), which extracts visual cues from surrounding regions at multiple scales, and designs a weighting scheme which is asymmetric in the spatial domain. MS-SAR is implemented in an efficient way, so that only small fractions of extra parameters and computations are required. We apply MS-SAR to several popular building blocks, including the residual block and the densely-connected block, and demonstrate its superior performance in both CIFAR and ILSVRC2012 classification tasks.  Springer Nature Switzerland AG 2018.
KW  - Image classification
KW  - Computer vision
KW  - Convolution
KW  - Neural networks
KW  - Radar imaging
KW  - Scales (weighing instruments)
U2  - Building blockes
U2  - Classification tasks
U2  - Convolutional neural network
U2  - Neural response
U2  - Recalibrations
U2  - Spatial domains
U2  - Surrounding regions
U2  - Visual recognition
DO  - 10.1007/978-3-030-01261-8_31
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_31
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Relaxation-Free Deep Hashing via Policy Gradient
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yuan, Xin
A1  - Ren, Liangliang
A1  - Lu, Jiwen
A1  - Zhou, Jie
AD  - Department of Automation, Tsinghua University, Beijing, China
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004525
SP  - 141
EP  - 157
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a simple yet effective relaxation-free method to learn more effective binary codes via policy gradient for scalable image search. While a variety of deep hashing methods have been proposed in recent years, most of them are confronted by the dilemma to obtain optimal binary codes in a truly end-to-end manner with non-smooth sign activations. Unlike existing methods which usually employ a general relaxation framework to adapt to the gradient-based algorithms, our approach formulates the non-smooth part of the hashing network as sampling with a stochastic policy, so that the retrieval performance degradation caused by the relaxation can be avoided. Specifically, our method directly generates the binary codes and maximizes the expectation of rewards for similarity preservation, where the network can be trained directly via policy gradient. Hence, the differentiation challenge for discrete optimization can be naturally addressed, which leads to effective gradients and binary codes. Extensive experimental results on three benchmark datasets validate the effectiveness of the proposed method.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Binary codes
KW  - Optimization
KW  - Stochastic systems
U2  - Benchmark datasets
U2  - Deep hashing
U2  - Discrete optimization
U2  - Gradient based algorithm
U2  - Optimal binary codes
U2  - Policy gradient
U2  - Relaxation-free
U2  - Retrieval performance
DO  - 10.1007/978-3-030-01225-0_9
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_9
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Collaborative Deep Reinforcement Learning for Multi-object Tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Ren, Liangliang
A1  - Lu, Jiwen
A1  - Wang, Zifeng
A1  - Tian, Qi
A1  - Zhou, Jie
AD  - Tsinghua University, Beijing, ChinaHuawei NoahS Ark Lab, Beijing, ChinaUniversity of Texas at San Antonio, San Antonio, United States
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977825
SP  - 605
EP  - 621
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a collaborative deep reinforcement learning (C-DRL) method for multi-object tracking. Most existing multi-object tracking methods employ the tracking-by-detection strategy which first detects objects in each frame and then associates them across different frames. However, the performance of these methods rely heavily on the detection results, which are usually unsatisfied in many real applications, especially in crowded scenes. To address this, we develop a deep prediction-decision network in our C-DRL, which simultaneously detects and predicts objects under a unified network via deep reinforcement learning. Specifically, we consider each object as an agent and track it via the prediction network, and seek the optimal tracked results by exploiting the collaborative interactions of different agents and environments via the decision network. Experimental results on the challenging MOT15 and MOT16 benchmarks are presented to show the effectiveness of our approach.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - C (programming language)
KW  - Computer vision
KW  - Object detection
KW  - Reinforcement learning
KW  - Tracking (position)
U2  - Collaborative interaction
U2  - Decision network
U2  - Multi-object tracking
U2  - Multi-objects
U2  - Object Tracking
U2  - Real applications
U2  - Tracking by detections
U2  - Unified networks
DO  - 10.1007/978-3-030-01219-9_36
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_36
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Quaternion convolutional neural networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhu, Xuanyu
A1  - Xu, Yi
A1  - Xu, Hongteng
A1  - Chen, Changjian
AD  - Shanghai Jiao Tong University, Shanghai, ChinaInfinia ML, Inc., Durham, United StatesDuke University, Durham; NC, United States
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006075
SP  - 645
EP  - 661
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Neural networks in the real domain have been studied for a long time and achieved promising results in many vision tasks for recent years. However, the extensions of the neural network models in other number fields and their potential applications are not fully-investigated yet. Focusing on color images, which can be naturally represented as quaternion matrices, we propose a quaternion convolutional neural network (QCNN) model to obtain more representative features. In particular, we re-design the basic modules like convolution layer and fully-connected layer in the quaternion domain, which can be used to establish fully-quaternion convolutional neural networks. Moreover, these modules are compatible with almost all deep learning techniques and can be plugged into traditional CNNs easily. We test our QCNN models in both color image classification and denoising tasks. Experimental results show that they outperform the real-valued CNNs with same structures.  Springer Nature Switzerland AG 2018.
KW  - Image denoising
KW  - Color
KW  - Computer vision
KW  - Convolution
KW  - Deep learning
KW  - Image classification
KW  - Neural networks
U2  - Color image de-noising
U2  - Color images
U2  - Convolutional neural network
U2  - Fully-connected layers
U2  - Learning techniques
U2  - Neural network model
U2  - Quaternion matrix
U2  - Quaternion-based layers
DO  - 10.1007/978-3-030-01237-3_39
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_39
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Occlusion-Aware R-CNN: Detecting Pedestrians in a Crowd
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Shifeng
A1  - Wen, Longyin
A1  - Bian, Xiao
A1  - Lei, Zhen
A1  - Li, Stan Z.
AD  - Center for Biometrics and Security Research, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, ChinaUniversity of Chinese Academy of Sciences, Beijing, ChinaGE Global Research, Niskayuna; NY, United StatesMacau University of Science and Technology, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977828
SP  - 657
EP  - 674
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Pedestrian detection in crowded scenes is a challenging problem since the pedestrians often gather together and occlude each other. In this paper, we propose a new occlusion-aware R-CNN (OR-CNN) to improve the detection accuracy in the crowd. Specifically, we design a new aggregation loss to enforce proposals to be close and locate compactly to the corresponding objects. Meanwhile, we use a new part occlusion-aware region of interest (PORoI) pooling unit to replace the RoI pooling layer in order to integrate the prior structure information of human body with visibility prediction into the network to handle occlusion. Our detector is trained in an end-to-end fashion, which achieves state-of-the-art results on three pedestrian detection datasets, i.e., CityPersons, ETH, and INRIA, and performs on-pair with the state-of-the-arts on Caltech.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Image segmentation
KW  - Visibility
U2  - Convolutional networks
U2  - Occlusion-aware
U2  - Pedestrian detection
U2  - Structure information
U2  - Visibility prediction
DO  - 10.1007/978-3-030-01219-9_39
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_39
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Weakly-supervised video summarization using variational encoder-decoder and web prior
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Cai, Sijia
A1  - Zuo, Wangmeng
A1  - Davis, Larry S.
A1  - Zhang, Lei
AD  - Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong KongDAMO Academy, Alibaba Group, Hangzhou, ChinaSchool of Computer Science and Technology, Harbin Institute of Technology, Harbin, ChinaDepartment of Computer Science, University of Maryland, College Park, United States
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020298
SP  - 193
EP  - 210
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Video summarization is a challenging under-constrained problem because the underlying summary of a single video strongly depends on users subjective understandings. Data-driven approaches, such as deep neural networks, can deal with the ambiguity inherent in this task to some extent, but it is extremely expensive to acquire the temporal annotations of a large-scale video dataset. To leverage the plentiful web-crawled videos to improve the performance of video summarization, we present a generative modelling framework to learn the latent semantic video representations to bridge the benchmark data and web data. Specifically, our framework couples two important components: a variational autoencoder for learning the latent semantics from web videos, and an encoder-attention-decoder for saliency estimation of raw video and summary generation. A loss term to learn the semantic matching between the generated summaries and web videos is presented, and the overall framework is further formulated into a unified conditional variational encoder-decoder, called variational encoder-summarizer-decoder (VESD). Experiments conducted on the challenging datasets CoSum and TVSum demonstrate the superior performance of the proposed VESD to existing state-of-the-art methods. The source code of this work can be found at https://github.com/cssjcai/vesd.  2018, Springer Nature Switzerland AG.
KW  - Semantic Web
KW  - Benchmarking
KW  - Computer vision
KW  - Constraint theory
KW  - Decoding
KW  - Deep neural networks
KW  - HTTP
KW  - Semantics
KW  - Signal encoding
KW  - Video recording
KW  - Web crawler
U2  - Auto encoders
U2  - Data-driven approach
U2  - Modelling framework
U2  - State-of-the-art methods
U2  - Summary generation
U2  - Under-constrained
U2  - Video representations
U2  - Video summarization
DO  - 10.1007/978-3-030-01264-9_12
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_12
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - CrossNet: An end-to-end reference-based super resolution network using cross-scale warping
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zheng, Haitian
A1  - Ji, Mengqi
A1  - Wang, Haoqian
A1  - Liu, Yebin
A1  - Fang, Lu
AD  - Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, Beijing, ChinaHong Kong University of Science and Technology, Clear Water Bay, Hong KongDepartment of Automation, Tsinghua University, Beijing, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977424
SP  - 87
EP  - 104
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The Reference-based Super-resolution (RefSR) super-resolves a low-resolution (LR) image given an external high-resolution (HR) reference image, where the reference image and LR image share similar viewpoint but with significant resolution gap (8). Existing RefSR methods work in a cascaded way such as patch matching followed by synthesis pipeline with two independently defined objective functions, leading to the inter-patch misalignment, grid effect and inefficient optimization. To resolve these issues, we present CrossNet, an end-to-end and fully-convolutional deep neural network using cross-scale warping. Our network contains image encoders, cross-scale warping layers, and fusion decoder: the encoder serves to extract multi-scale features from both the LR and the reference images; the cross-scale warping layers spatially aligns the reference feature map with the LR feature map; the decoder finally aggregates feature maps from both domains to synthesize the HR output. Using cross-scale warping, our network is able to perform spatial alignment at pixel-level in an end-to-end fashion, which improves the existing schemes [1, 2] both in precision (around 2 dB4adB) and efficiency (more than 100 times faster).  Springer Nature Switzerland AG 2018.
KW  - Optical resolving power
KW  - Computer vision
KW  - Decoding
KW  - Deep neural networks
KW  - Optical flows
KW  - Signal encoding
U2  - Encoder-decoder
U2  - Image synthesis
U2  - Light fields
U2  - Low resolution images
U2  - Multi-scale features
U2  - Objective functions
U2  - Spatial alignment
U2  - Super resolution
DO  - 10.1007/978-3-030-01231-1_6
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_6
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - TBN: Convolutional Neural Network with Ternary Inputs and Binary Weights
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wan, Diwen
A1  - Shen, Fumin
A1  - Liu, Li
A1  - Zhu, Fan
A1  - Qin, Jie
A1  - Shao, Ling
A1  - Shen, Heng Tao
AD  - Center for Future Media and School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, ChinaInception Institute of Artificial Intelligence, Abu Dhabi, United Arab EmiratesComputer Vision Lab, ETH Zurich, Zurich, Switzerland
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005967
SP  - 322
EP  - 339
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Despite the remarkable success of Convolutional Neural Networks (CNNs) on generalized visual tasks, high computational and memory costs restrict their comprehensive applications on consumer electronics (e.g., portable or smart wearable devices). Recent advancements in binarized networks have demonstrated progress on reducing computational and memory costs, however, they suffer from significant performance degradation comparing to their full-precision counterparts. Thus, a highly-economical yet effective CNN that is authentically applicable to consumer electronics is at urgent need. In this work, we propose a Ternary-Binary Network (TBN), which provides an efficient approximation to standard CNNs. Based on an accelerated ternary-binary matrix multiplication, TBN replaces the arithmetical operations in standard CNNs with efficient XOR, AND and bitcount operations, and thus provides an optimal tradeoff between memory, efficiency and performance. TBN demonstrates its consistent effectiveness when applied to various CNN architectures (e.g., AlexNet and ResNet) on multiple datasets of different scales, and provides 32 memory savings and faster convolutional operations. Meanwhile, TBN can outperform XNOR-Network by upto 5.5% (top-1 accuracy) on the ImageNet classification task, and upto 4.4% (mAP score) on the PASCAL VOC object detection task.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Acceleration
KW  - Compaction
KW  - Convolution
KW  - Neural networks
KW  - Object detection
KW  - Vision
U2  - Arithmetical operations
U2  - Binary operations
U2  - Classification tasks
U2  - Convolutional neural network
U2  - Efficiency and performance
U2  - Multiple data sets
U2  - Optimal tradeoffs
U2  - Performance degradation
DO  - 10.1007/978-3-030-01216-8_20
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_20
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Graininess-aware deep feature learning for pedestrian detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Lin, Chunze
A1  - Lu, Jiwen
A1  - Wang, Gang
A1  - Zhou, Jie
AD  - Tsinghua University, Beijing, ChinaAlibaba AI Labs, Hangzhou, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977519
SP  - 745
EP  - 761
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a graininess-aware deep feature learning method for pedestrian detection. Unlike most existing pedestrian detection methods which only consider low resolution feature maps, we incorporate fine-grained information into convolutional features to make them more discriminative for human body parts. Specifically, we propose a pedestrian attention mechanism which efficiently identifies pedestrian regions. Our method encodes fine-grained attention masks into convolutional feature maps, which significantly suppresses background interference and highlights pedestrians. Hence, our graininess-aware features become more focused on pedestrians, in particular those of small size and with occlusion. We further introduce a zoom-in-zoom-out module, which enhances the features by incorporating local details and context information. We integrate these two modules into a deep neural network, forming an end-to-end trainable pedestrian detector. Comprehensive experimental results on four challenging pedestrian benchmarks demonstrate the effectiveness of the proposed approach.  Springer Nature Switzerland AG 2018.
KW  - Feature extraction
KW  - Computer vision
KW  - Convolution
KW  - Deep learning
KW  - Deep neural networks
U2  - Attention
U2  - Attention mechanisms
U2  - Context information
U2  - Deep feature learning
U2  - Graininess
U2  - Human bodies
U2  - Low resolution
U2  - Pedestrian detection
DO  - 10.1007/978-3-030-01240-3_45
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_45
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - The unmanned aerial vehicle benchmark: Object detection and tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Du, Dawei
A1  - Qi, Yuankai
A1  - Yu, Hongyang
A1  - Yang, Yifan
A1  - Duan, Kaiwen
A1  - Li, Guorong
A1  - Zhang, Weigang
A1  - Huang, Qingming
A1  - Tian, Qi
AD  - University of Chinese Academy of Sciences, Beijing, ChinaHarbin Institute of Technology, Harbin, ChinaHarbin Institute of Technology, Weihai, ChinaHuawei Noahs Ark Lab, Shenzhen, ChinaUniversity of Texas at San Antonio, San Antonio, United States
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978779
SP  - 375
EP  - 391
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - With the advantage of high mobility, Unmanned Aerial Vehicles (UAVs) are used to fuel numerous important applications in computer vision, delivering more efficiency and convenience than surveillance cameras with fixed camera angle, scale and view. However, very limited UAV datasets are proposed, and they focus only on a specific task such as visual tracking or object detection in relatively constrained scenarios. Consequently, it is of great importance to develop an unconstrained UAV benchmark to boost related researches. In this paper, we construct a new UAV benchmark focusing on complex scenarios with new level challenges. Selected from 10 hours raw videos, about 80, 000 representative frames are fully annotated with bounding boxes as well as up to 14 kinds of attributes (e.g., weather condition, flying altitude, camera view, vehicle category, and occlusion) for three fundamental computer vision tasks: object detection, single object tracking, and multiple object tracking. Then, a detailed quantitative study is performed using most recent state-of-the-art algorithms for each task. Experimental results show that the current state-of-the-art methods perform relative worse on our dataset, due to the new challenges appeared in UAV based real scenes, e.g., high density, small object, and camera motion. To our knowledge, our work is the first time to explore such issues in unconstrained scenes comprehensively. The dataset and all the experimental results are available in https://sites.google.com/site/daviddo0323/.  Springer Nature Switzerland AG 2018.
KW  - Aircraft detection
KW  - Antennas
KW  - Cameras
KW  - Computer vision
KW  - Object detection
KW  - Object recognition
KW  - Security systems
KW  - Unmanned aerial vehicles (UAV)
U2  - Camera motions
U2  - Multiple object tracking
U2  - Object detection and tracking
U2  - Quantitative study
U2  - Single object
U2  - State-of-the-art methods
U2  - Surveillance cameras
U2  - Visual Tracking
DO  - 10.1007/978-3-030-01249-6_23
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_23
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Egocentric Activity Prediction via Event Modulated Attention
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Shen, Yang
A1  - Ni, Bingbing
A1  - Li, Zefan
A1  - Zhuang, Ning
AD  - SJTU-UCLA Joint Center for Machine Perception and Inference, Shanghai Jiao Tong University, Shanghai, China
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005959
SP  - 202
EP  - 217
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Predicting future activities from an egocentric viewpoint is of particular interest in assisted living. However, state-of-the-art egocentric activity understanding techniques are mostly NOT capable of predictive tasks, as their synchronous processing architecture performs poorly in either modeling event dependency or pruning temporal redundant features. This work explicitly addresses these issues by proposing an asynchronous gaze-event driven attentive activity prediction network. This network is built on a gaze-event extraction module inspired by the fact that gaze moving in/out of a certain object most probably indicates the occurrence/ending of a certain activity. The extracted gaze events are input to: (1) an asynchronous module which reasons about the temporal dependency between events and (2) a synchronous module which softly attends to informative temporal durations for more compact and discriminative feature extraction. Both modules are seamlessly integrated for collaborative prediction. Extensive experimental results on egocentric activity prediction as well as recognition well demonstrate the effectiveness of the proposed method.  2018, Springer Nature Switzerland AG.
KW  - Forecasting
KW  - Assisted living
KW  - Computer architecture
KW  - Computer vision
KW  - Extraction
U2  - Asynchronous
U2  - Attention
U2  - Egocentric video
U2  - Event
U2  - Gaze
DO  - 10.1007/978-3-030-01216-8_13
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_13
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Multi-scale residual network for image super-resolution
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Juncheng
A1  - Fang, Faming
A1  - Mei, Kangfu
A1  - Zhang, Guixu
AD  - Shanghai Key Laboratory of Multidimensional Information Processing, Department of Computer Science and Technology, East China Normal University, Shanghai, ChinaSchool of Computer Science and Information Engineering, Jiangxi Normal University, Nanchang, China
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006068
SP  - 527
EP  - 542
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recent studies have shown that deep neural networks can significantly improve the quality of single-image super-resolution. Current researches tend to use deeper convolutional neural networks to enhance performance. However, blindly increasing the depth of the network cannot ameliorate the network effectively. Worse still, with the depth of the network increases, more problems occurred in the training process and more training tricks are needed. In this paper, we propose a novel multi-scale residual network (MSRN) to fully exploit the image features, which outperform most of the state-of-the-art methods. Based on the residual block, we introduce convolution kernels of different sizes to adaptively detect the image features in different scales. Meanwhile, we let these features interact with each other to get the most efficacious image information, we call this structure Multi-scale Residual Block (MSRB). Furthermore, the outputs of each MSRB are used as the hierarchical features for global feature fusion. Finally, all these features are sent to the reconstruction module for recovering the high-quality image.  Springer Nature Switzerland AG 2018.
KW  - Image enhancement
KW  - Computer vision
KW  - Convolution
KW  - Deep neural networks
KW  - Neural networks
KW  - Optical resolving power
U2  - Convolution kernel
U2  - Convolutional neural network
U2  - Hierarchical features
U2  - High quality images
U2  - Image super resolutions
U2  - Multi-scale
U2  - State-of-the-art methods
U2  - Super resolution
DO  - 10.1007/978-3-030-01237-3_32
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_32
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Improving generalization via scalable neighborhood component analysis
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wu, Zhirong
A1  - Efros, Alexei A.
A1  - Yu, Stella X.
AD  - UC Berkeley/ICSI, Berkeley, United StatesMicrosoft Research Asia, Beijing, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977620
SP  - 712
EP  - 728
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Current major approaches to visual recognition follow an end-to-end formulation that classifies an input image into one of the pre-determined set of semantic categories. Parametric softmax classifiers are a common choice for such a closed world with fixed categories, especially when big labeled data is available during training. However, this becomes problematic for open-set scenarios where new categories are encountered with very few examples for learning a generalizable parametric classifier. We adopt a non-parametric approach for visual recognition by optimizing feature embeddings instead of parametric classifiers. We use a deep neural network to learn the visual feature that preserves the neighborhood structure in the semantic space, based on the Neighborhood Component Analysis (NCA) criterion. Limited by its computational bottlenecks, we devise a mechanism to use augmented memory to scale NCA for large datasets and very deep networks. Our experiments deliver not only remarkable performance on ImageNet classification for such a simple non-parametric method, but most importantly a more generalizable feature representation for sub-category discovery and few-shot recognition.  Springer Nature Switzerland AG 2018.
KW  - Classifiers
KW  - Computer vision
KW  - Deep neural networks
KW  - Image classification
KW  - Nearest neighbor search
KW  - Object recognition
KW  - Semantics
U2  - Few-shot learning
U2  - K-nearest neighbors
U2  - Large-scale objects
U2  - Neighborhood component analysis
U2  - Transfer learning
DO  - 10.1007/978-3-030-01234-2_42
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_42
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Generative semantic manipulation with mask-contrasting GAN
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liang, Xiaodan
A1  - Zhang, Hao
A1  - Lin, Liang
A1  - Xing, Eric
AD  - Carnegie Mellon University, Pittsburgh, United StatesSun Yat-sen University, Guangzhou, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008955
SP  - 574
EP  - 590
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Despite the promising results on paired/unpaired image-to-image translation achieved by Generative Adversarial Networks (GANs), prior works often only transfer the low-level information (e.g. color or texture changes), but fail to manipulate high-level semantic meanings (e.g., geometric structure or content) of different object regions. On the other hand, while some researches can synthesize compelling real-world images given a class label or caption, they cannot condition on arbitrary shapes or structures, which largely limits their application scenarios and interpretive capability of model results. In this work, we focus on a more challenging semantic manipulation task, aiming at modifying the semantic meaning of an object while preserving its own characteristics (e.g. viewpoints and shapes), such as cow  sheep, motor  bicycle, cat  dog. To tackle such large semantic changes, we introduce a contrasting GAN (contrast-GAN) with a novel adversarial contrasting objective which is able to perform all types of semantic translations with one category-conditional generator. Instead of directly making the synthesized samples close to target data as previous GANs did, our adversarial contrasting objective optimizes over the distance comparisons between samples, that is, enforcing the manipulated data be semantically closer to the real data with target category than the input data. Equipped with the new contrasting objective, a novel mask-conditional contrast-GAN architecture is proposed to enable disentangle image background with object semantic changes. Extensive qualitative and quantitative experiments on several semantic manipulation tasks on ImageNet and MSCOCO dataset show considerable performance gain by our contrast-GAN over other conditional GANs.  Springer Nature Switzerland AG 2018.
KW  - Semantics
KW  - Computer vision
U2  - Adversarial networks
U2  - Application scenario
U2  - Geometric structure
U2  - High level semantics
U2  - Image semantics
U2  - Image translation
U2  - Quantitative experiments
U2  - Semantic translations
DO  - 10.1007/978-3-030-01261-8_34
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_34
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Geometric Constrained Joint Lane Segmentation and Lane Boundary Detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Jie
A1  - Xu, Yi
A1  - Ni, Bingbing
A1  - Duan, Zhenyu
AD  - SJTU-UCLA Joint Center for Machine Perception and Inference, Shanghai Jiao Tong University, Shanghai; 200240, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977709
SP  - 502
EP  - 518
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Lane detection is playing an indispensable role in advanced driver assistance systems. The existing approaches for lane detection can be categorized as lane area segmentation and lane boundary detection. Most of these methods abandon a great quantity of complementary information, such as geometric priors, when exploiting the lane area and the lane boundaries alternatively. In this paper, we establish a multiple-task learning framework to segment lane areas and detect lane boundaries simultaneously. The main contributions of the proposed framework are highlighted in two facets: (1) We put forward a multiple-task learning framework with mutually interlinked sub-structures between lane segmentation and lane boundary detection to improve overall performance. (2) A novel loss function is proposed with two geometric constraints considered, as assumed that the lane boundary is predicted as the outer contour of the lane area while the lane area is predicted as the area integration result within the lane boundary lines. With an end-to-end training process, these improvements extremely enhance the robustness and accuracy of our approach on several metrics. The proposed framework is evaluated on KITTI dataset, CULane dataset and RVD dataset. Compared with the state of the arts, our approach achieves the best performance on the metrics and a robust detection in varied traffic scenes.  2018, Springer Nature Switzerland AG.
KW  - Image segmentation
KW  - Advanced driver assistance systems
KW  - Automobile drivers
KW  - Computer vision
KW  - Geometry
KW  - Semantics
U2  - Boundary detection
U2  - Geometric constraint
U2  - Geometric priors
U2  - Lane segmentation
U2  - Robust detection
U2  - Semantic segmentation
U2  - State of the art
U2  - Training process
DO  - 10.1007/978-3-030-01246-5_30
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_30
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Joint Camera Spectral Sensitivity Selection and Hyperspectral Image Recovery
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Fu, Ying
A1  - Zhang, Tao
A1  - Zheng, Yinqiang
A1  - Zhang, Debing
A1  - Huang, Hua
AD  - Beijing Laboratory of Intelligent Information Technology, School of Computer Science and Technology, Beijing Institute of Technology, Beijing; 100081, ChinaNational Institute of Informatics, Tokyo; 101-8430, JapanDeepGlint, Beijing; 100091, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977838
SP  - 812
EP  - 828
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Hyperspectral image (HSI) recovery from a single RGB image has attracted much attention, whose performance has recently been shown to be sensitive to the camera spectral sensitivity (CSS). In this paper, we present an efficient convolutional neural network (CNN) based method, which can jointly select the optimal CSS from a candidate dataset and learn a mapping to recover HSI from a single RGB image captured with this algorithmically selected camera. Given a specific CSS, we first present a HSI recovery network, which accounts for the underlying characteristics of the HSI, including spectral nonlinear mapping and spatial similarity. Later, we append a CSS selection layer onto the recovery network, and the optimal CSS can thus be automatically determined from the network weights under the nonnegative sparse constraint. Experimental results show that our HSI recovery network outperforms state-of-the-art methods in terms of both quantitative metrics and perceptive quality, and the selection layer always returns a CSS consistent to the best one determined by exhaustive search.  2018, Springer Nature Switzerland AG.
KW  - Spectroscopy
KW  - Cameras
KW  - Computer system recovery
KW  - Computer vision
KW  - Neural networks
KW  - Photomapping
KW  - Recovery
U2  - Convolutional Neural Networks (CNN)
U2  - Network weights
U2  - Nonlinear mappings
U2  - Quantitative metrics
U2  - Recovery network
U2  - Spatial similarity
U2  - Spectral sensitivity
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01219-9_48
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_48
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Goal-Oriented Visual Question Generation via Intermediate Rewards
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Junjie
A1  - Wu, Qi
A1  - Shen, Chunhua
A1  - Zhang, Jian
A1  - Lu, Jianfeng
A1  - van den Hengel, Anton
AD  - School of Electrical and Data Engineering, University of Technology Sydney, Sydney, AustraliaAustralian Insititute for Machine Learning, The University of Adelaide, Adelaide, AustraliaSchool of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976847
SP  - 189
EP  - 204
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Despite significant progress in a variety of vision-and-language problems, developing a method capable of asking intelligent, goal-oriented questions about images is proven to be an inscrutable challenge. Towards this end, we propose a Deep Reinforcement Learning framework based on three new intermediate rewards, namely goal-achieved, progressive and informativeness that encourage the generation of succinct questions, which in turn uncover valuable information towards the overall goal. By directly optimizing for questions that work quickly towards fulfilling the overall goal, we avoid the tendency of existing methods to generate long series of inane queries that add little value. We evaluate our model on the GuessWhat?! dataset and show that the resulting questions can help a standard Guesser identify a specific object in an image at a much higher success rate.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Deep learning
KW  - Reinforcement learning
U2  - Goal-oriented
U2  - Informative ness
U2  - Intermediate rewards
U2  - Language problems
DO  - 10.1007/978-3-030-01228-1_12
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_12
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Sketchyscene: Richly-annotated scene sketches
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zou, Changqing
A1  - Yu, Qian
A1  - Du, Ruofei
A1  - Mo, Haoran
A1  - Song, Yi-Zhe
A1  - Xiang, Tao
A1  - Gao, Chengying
A1  - Chen, Baoquan
A1  - Zhang, Hao
AD  - University of Maryland, College Park, United StatesQueen Mary University of London, London, United KingdomSun Yat-sen University, Guangzhou, ChinaShandong University, Jinan, ChinaSimon Fraser University, Burnaby, Canada
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006175
SP  - 438
EP  - 454
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We contribute the first large-scale dataset of scene sketches, SketchyScene, with the goal of advancing research on sketch understanding at both the object and scene level. The dataset is created through a novel and carefully designed crowdsourcing pipeline, enabling users to efficiently generate large quantities of realistic and diverse scene sketches. SketchyScene contains more than 29,000 scene-level sketches, 7,000+ pairs of scene templates and photos, and 11,000+ object sketches. All objects in the scene sketches have ground-truth semantic and instance masks. The dataset is also highly scalable and extensible, easily allowing augmenting and/or changing scene composition. We demonstrate the potential impact of SketchyScene by training new computational models for semantic segmentation of scene sketches and showing how the new dataset enables several applications including image retrieval, sketch colorization, editing, and captioning, etc. The dataset and code can be found at https://github.com/SketchyScene/SketchyScene.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Image segmentation
KW  - Semantics
U2  - Computational model
U2  - Large-scale dataset
U2  - Object and scenes
U2  - Potential impacts
U2  - Scene sketch
U2  - Semantic segmentation
U2  - Sketch dataset
U2  - Sketch understanding
DO  - 10.1007/978-3-030-01267-0_26
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_26
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Multi-view to Novel View: Synthesizing Novel Views With Self-learned Confidence
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Sun, Shao-Hua
A1  - Huh, Minyoung
A1  - Liao, Yuan-Hong
A1  - Zhang, Ning
A1  - Lim, Joseph J.
AD  - University of Southern California, Los Angeles, United StatesCarnegie Mellon University, Pittsburgh, United StatesNational Tsing Hua University, Hsinchu, TaiwanSnap Inc., Venice, United States
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977797
SP  - 162
EP  - 178
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we address the task of multi-view novel view synthesis, where we are interested in synthesizing a target image with an arbitrary camera pose from given source images. We propose an end-to-end trainable framework that learns to exploit multiple viewpoints to synthesize a novel view without any 3D supervision. Specifically, our model consists of a flow prediction module and a pixel generation module to directly leverage information presented in source views as well as hallucinate missing pixels from statistical priors. To merge the predictions produced by the two modules given multi-view source images, we introduce a self-learned confidence aggregation mechanism. We evaluate our model on images rendered from 3D object models as well as real and synthesized scenes. We demonstrate that our model is able to achieve state-of-the-art results as well as progressively improve its predictions when more source images are available.  2018, Springer Nature Switzerland AG.
KW  - Image enhancement
KW  - Computer vision
KW  - Forecasting
KW  - Pixels
U2  - Aggregation mechanism
U2  - Flow prediction
U2  - Multi-views
U2  - Multiple viewpoints
U2  - Novel view synthesis
U2  - Source images
U2  - State of the art
U2  - Target images
DO  - 10.1007/978-3-030-01219-9_10
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_10
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Summarizing first-person videos from third persons’ points of views
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Ho, Hsuan-I.
A1  - Chiu, Wei-Chen
A1  - Wang, Yu-Chiang Frank
AD  - Department of Electrical Engineering, National Taiwan University, TaiwanDepartment of Computer Science, National Chiao Tung University, Taiwan
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006201
SP  - 72
EP  - 89
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Video highlight or summarization is among interesting topics in computer vision, which benefits a variety of applications like viewing, searching, or storage. However, most existing studies rely on training data of third-person videos, which cannot easily generalize to highlight the first-person ones. With the goal of deriving an effective model to summarize first-person videos, we propose a novel deep neural network architecture for describing and discriminating vital spatiotemporal information across videos with different points of view. Our proposed model is realized in a semi-supervised setting, in which fully annotated third-person videos, unlabeled first-person videos, and a small number of annotated first-person ones are presented during training. In our experiments, qualitative and quantitative evaluations on both benchmarks and our collected first-person video datasets are presented.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Deep neural networks
KW  - Digital storage
KW  - Network architecture
U2  - First-person visions
U2  - Metric learning
U2  - Quantitative evaluation
U2  - Semi-supervised
U2  - Spatiotemporal information
U2  - Transfer learning
U2  - Video datasets
U2  - Video summarization
DO  - 10.1007/978-3-030-01267-0_5
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_5
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - An adversarial approach to hard triplet generation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhao, Yiru
A1  - Jin, Zhongming
A1  - Qi, Guo-jun
A1  - Lu, Hongtao
A1  - Hua, Xian-sheng
AD  - Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, ChinaAlibaba Damo Academy, Alibaba Group, Hangzhou, ChinaLaboratory for MAchine Perception and LEarning, University of Central Florida, Orlando, United States
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977504
SP  - 508
EP  - 524
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - While deep neural networks have demonstrated competitive results for many visual recognition and image retrieval tasks, the major challenge lies in distinguishing similar images from different categories (i.e., hard negative examples) while clustering images with large variations from the same category (i.e., hard positive examples). The current state-of-the-art is to mine the most hard triplet examples from the mini-batch to train the network. However, mining-based methods tend to look into these triplets that are hard in terms of the current estimated network, rather than deliberately generating those hard triplets that really matter in globally optimizing the network. For this purpose, we propose an adversarial network for Hard Triplet Generation (HTG) to optimize the network ability in distinguishing similar examples of different categories as well as grouping varied examples of the same categories. We evaluate our method on the real-world challenging datasets, such as CUB200-2011, CARS196, DeepFashion and VehicleID datasets, and show that our method outperforms the state-of-the-art methods significantly.  Springer Nature Switzerland AG 2018.
KW  - Image retrieval
KW  - Computer vision
KW  - Deep neural networks
U2  - Adversarial nets
U2  - Adversarial networks
U2  - Hard examples
U2  - Negative examples
U2  - Positive examples
U2  - State of the art
U2  - State-of-the-art methods
U2  - Visual recognition
DO  - 10.1007/978-3-030-01240-3_31
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_31
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Interaction-Aware Spatio-Temporal Pyramid Attention Networks for Action Classification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Du, Yang
A1  - Yuan, Chunfeng
A1  - Li, Bing
A1  - Zhao, Lili
A1  - Li, Yangxi
A1  - Hu, Weiming
AD  - University of Chinese Academy of Sciences, Beijing, ChinaCAS Center for Excellence in Brain Science and Intelligence Technology, National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, ChinaMeitu, Mainland China, ChinaNational Computer network Emergency Response technical Team/Coordination Center of China, Ho Chi Minh City, Viet Nam
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978929
SP  - 388
EP  - 404
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Local features at neighboring spatial positions in feature maps have high correlation since their receptive fields are often overlapped. Self-attention usually uses the weighted sum (or other functions) with internal elements of each local feature to obtain its weight score, which ignores interactions among local features. To address this, we propose an effective interaction-aware self-attention model inspired by PCA to learn attention maps. Furthermore, since different layers in a deep network capture feature maps of different scales, we use these feature maps to construct a spatial pyramid and then utilize multi-scale information to obtain more accurate attention scores, which are used to weight the local features in all spatial positions of feature maps to calculate attention maps. Moreover, our spatial pyramid attention is unrestricted to the number of its input feature maps so it is easily extended to a spatio-temporal version. Finally, our model is embedded in general CNNs to form end-to-end attention networks for action classification. Experimental results show that our method achieves the state-of-the-art results on the UCF101, HMDB51 and untrimmed Charades.  2018, Springer Nature Switzerland AG.
KW  - Scales (weighing instruments)
KW  - Computer vision
U2  - Action classifications
U2  - Different layers
U2  - Effective interactions
U2  - Multi-scale informations
U2  - Receptive fields
U2  - Spatial positions
U2  - Spatial pyramids
U2  - State of the art
DO  - 10.1007/978-3-030-01270-0_23
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_23
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning class prototypes via structure alignment for zero-shot recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Jiang, Huajie
A1  - Wang, Ruiping
A1  - Shan, Shiguang
A1  - Chen, Xilin
AD  - Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing; 100190, ChinaShanghai Institute of Microsystem and Information Technology, CAS, Shanghai; 200050, ChinaShanghaiTech University, Shanghai; 200031, ChinaUniversity of Chinese Academy of Sciences, Beijing; 100049, China
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978729
SP  - 121
EP  - 138
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Zero-shot learning (ZSL) aims to recognize objects of novel classes without any training samples of specific classes, which is achieved by exploiting the semantic information and auxiliary datasets. Recently most ZSL approaches focus on learning visual-semantic embeddings to transfer knowledge from the auxiliary datasets to the novel classes. However, few works study whether the semantic information is discriminative or not for the recognition task. To tackle such problem, we propose a coupled dictionary learning approach to align the visual-semantic structures using the class prototypes, where the discriminative information lying in the visual space is utilized to improve the less discriminative semantic space. Then, zero-shot recognition can be performed in different spaces by the simple nearest neighbor approach using the learned class prototypes. Extensive experiments on four benchmark datasets show the effectiveness of the proposed approach.  Springer Nature Switzerland AG 2018.
KW  - Semantics
KW  - Computer vision
U2  - Benchmark datasets
U2  - Class prototypes
U2  - Dictionary learning
U2  - Nearest-neighbor approaches
U2  - Semantic information
U2  - Structure alignment
U2  - Visual semantics
U2  - Zero-shot learning
DO  - 10.1007/978-3-030-01249-6_8
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_8
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep kalman filtering network for video compression artifact reduction
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Lu, Guo
A1  - Ouyang, Wanli
A1  - Xu, Dong
A1  - Zhang, Xiaoyun
A1  - Gao, Zhiyong
A1  - Sun, Ming-Ting
AD  - Shanghai Jiao Tong University, Shanghai, ChinaThe University of Sydney, Sydney, AustraliaSenseTime Computer Vision Research Group, The University of Sydney, Sydney, AustraliaUniversity of Washington, Seattle, United States
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020323
SP  - 591
EP  - 608
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - When lossy video compression algorithms are applied, compression artifacts often appear in videos, making decoded videos unpleasant for human visual systems. In this paper, we model the video artifact reduction task as a Kalman filtering procedure and restore decoded frames through a deep Kalman filtering network. Different from the existing works using the noisy previous decoded frames as temporal information in the restoration problem, we utilize the less noisy previous restored frame and build a recursive filtering scheme based on the Kalman model. This strategy can provide more accurate and consistent temporal information, which produces higher quality restoration results. In addition, the strong prior information of prediction residual is also exploited for restoration through a well designed neural network. These two components are combined under the Kalman framework and optimized through the deep Kalman filtering network. Our approach can well bridge the gap between the model-based methods and learning-based methods by integrating the recursive nature of the Kalman model and highly non-linear transformation ability of deep neural network. Experimental results on the benchmark dataset demonstrate the effectiveness of our proposed method.  2018, Springer Nature Switzerland AG.
KW  - Kalman filters
KW  - Computer vision
KW  - Decoding
KW  - Deep neural networks
KW  - Image compression
KW  - Image reconstruction
KW  - Information filtering
KW  - Linear transformations
KW  - Mathematical transformations
KW  - Restoration
U2  - Compression artifacts
U2  - Kalman model
U2  - Learning-based methods
U2  - Non-linear transformations
U2  - Recursive filtering
U2  - Restoration problems
U2  - Video compression algorithms
U2  - Video restoration
DO  - 10.1007/978-3-030-01264-9_35
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_35
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Pose guided human video generation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yang, Ceyuan
A1  - Wang, Zhe
A1  - Zhu, Xinge
A1  - Huang, Chen
A1  - Shi, Jianping
A1  - Lin, Dahua
AD  - CUHK-SenseTime Joint Lab, CUHK, Shatin, Hong KongSenseTime Research, Beijing, ChinaCarnegie Mellon University, Pittsburgh, United States
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978768
SP  - 204
EP  - 219
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Due to the emergence of Generative Adversarial Networks, video synthesis has witnessed exceptional breakthroughs. However, existing methods lack a proper representation to explicitly control the dynamics in videos. Human pose, on the other hand, can represent motion patterns intrinsically and interpretably, and impose the geometric constraints regardless of appearance. In this paper, we propose a pose guided method to synthesize human videos in a disentangled way: plausible motion prediction and coherent appearance generation. In the first stage, a Pose Sequence Generative Adversarial Network (PSGAN) learns in an adversarial manner to yield pose sequences conditioned on the class label. In the second stage, a Semantic Consistent Generative Adversarial Network (SCGAN) generates video frames from the poses while preserving coherent appearances in the input image. By enforcing semantic consistency between the generated and ground-truth poses at a high feature level, our SCGAN is robust to noisy or abnormal poses. Extensive experiments on both human action and human face datasets manifest the superiority of the proposed method over other state-of-the-arts.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Motion estimation
KW  - Semantics
U2  - Adversarial networks
U2  - Geometric constraint
U2  - Motion prediction
U2  - Pose synthesis
U2  - Semantic consistency
U2  - State of the art
U2  - Video generation
U2  - Video synthesis
DO  - 10.1007/978-3-030-01249-6_13
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_13
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Recurrent squeeze-and-excitation context aggregation net for single image deraining
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Xia
A1  - Wu, Jianlong
A1  - Lin, Zhouchen
A1  - Liu, Hong
A1  - Zha, Hongbin
AD  - Key Laboratory of Machine Perception, Shenzhen Graduate School, Peking University, Beijing, ChinaKey Laboratory of Machine Perception (MOE), School of EECS, Peking University, Beijing, ChinaCooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977591
SP  - 262
EP  - 277
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Rain streaks can severely degrade the visibility, which causes many current computer vision algorithms fail to work. So it is necessary to remove the rain from images. We propose a novel deep network architecture based on deep convolutional and recurrent neural networks for single image deraining. As contextual information is very important for rain removal, we first adopt the dilated convolutional neural network to acquire large receptive field. To better fit the rain removal task, we also modify the network. In heavy rain, rain streaks have various directions and shapes, which can be regarded as the accumulation of multiple rain streak layers. We assign different alpha-values to various rain streak layers according to the intensity and transparency by incorporating the squeeze-and-excitation block. Since rain streak layers overlap with each other, it is not easy to remove the rain in one stage. So we further decompose the rain removal into multiple stages. Recurrent neural network is incorporated to preserve the useful information in previous stages and benefit the rain removal in later stages. We conduct extensive experiments on both synthetic and real-world datasets. Our proposed method outperforms the state-of-the-art approaches under all evaluation metrics. Codes and supplementary material are available at our project webpage: https://xialipku.github.io/RESCAN.  Springer Nature Switzerland AG 2018.
KW  - Recurrent neural networks
KW  - Computer vision
KW  - Convolution
KW  - Network architecture
KW  - Rain
U2  - Computer vision algorithms
U2  - Contextual information
U2  - Convolutional neural network
U2  - Evaluation metrics
U2  - Image deraining
U2  - Real-world datasets
U2  - Squeeze and excitation block
U2  - State-of-the-art approach
DO  - 10.1007/978-3-030-01234-2_16
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_16
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Task-driven webpage saliency
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zheng, Quanlong
A1  - Jiao, Jianbo
A1  - Cao, Ying
A1  - Lau, Rynson W. H.
AD  - Department of Computer Science, City University of Hong Kong, Hong Kong, Hong KongUniversity of Illinois at Urbana-Champaign, Urbana, United States
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020304
SP  - 300
EP  - 316
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we present an end-to-end learning framework for predicting task-driven visual saliency on webpages. Given a webpage, we propose a convolutional neural network to predict where people look at it under different task conditions. Inspired by the observation that given a specific task, human attention is strongly correlated with certain semantic components on a webpage (e.g., images, buttons and input boxes), our network explicitly disentangles saliency prediction into two independent sub-tasks: task-specific attention shift prediction and task-free saliency prediction. The task-specific branch estimates task-driven attention shift over a webpage from its semantic components, while the task-free branch infers visual saliency induced by visual features of the webpage. The outputs of the two branches are combined to produce the final prediction. Such a task decomposition framework allows us to efficiently learn our model from a small-scale task-driven saliency dataset with sparse labels (captured under a single task condition). Experimental results show that our method outperforms the baselines and prior works, achieving state-of-the-art performance on a newly collected benchmark dataset for task-driven webpage saliency detection.  2018, Springer Nature Switzerland AG.
KW  - Websites
KW  - Benchmarking
KW  - Computer vision
KW  - Forecasting
KW  - Neural networks
KW  - Semantics
KW  - Visualization
U2  - Convolutional neural network
U2  - Learning frameworks
U2  - Saliency detection
U2  - Semantic components
U2  - State-of-the-art performance
U2  - Task decomposition
U2  - Task-specific saliency
U2  - Web-page analysis
DO  - 10.1007/978-3-030-01264-9_18
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_18
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - A segmentation-aware deep fusion network for compressed sensing MRI
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Fan, Zhiwen
A1  - Sun, Liyan
A1  - Ding, Xinghao
A1  - Huang, Yue
A1  - Cai, Congbo
A1  - Paisley, John
AD  - Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, Xiamen; Fujian, ChinaDepartment of Electrical Engineering, Columbia University, New York; NY, United States
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977418
SP  - 55
EP  - 70
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Compressed sensing MRI is a classic inverse problem in the field of computational imaging, accelerating the MR imaging by measuring less k-space data. The deep neural network models provide the stronger representation ability and faster reconstruction compared with shallow optimization-based methods. However, in the existing deep-based CS-MRI models, the high-level semantic supervision information from massive segmentation-labels in MRI dataset is overlooked. In this paper, we proposed a segmentation-aware deep fusion network called SADFN for compressed sensing MRI. The multilayer feature aggregation (MLFA) method is introduced here to fuse all the features from different layers in the segmentation network. Then, the aggregated feature maps containing semantic information are provided to each layer in the reconstruction network with a feature fusion strategy. This guarantees the reconstruction network is aware of the different regions in the image it reconstructs, simplifying the function mapping. We prove the utility of the cross-layer and cross-task information fusion strategy by comparative study. Extensive experiments on brain segmentation benchmark MRBrainS and BratS15 validated that the proposed SADFN model achieves state-of-the-art accuracy in compressed sensing MRI. This paper provides a novel approach to guide the low-level visual task using the information from mid- or high-level task.  Springer Nature Switzerland AG 2018.
KW  - Magnetic resonance imaging
KW  - Compressed sensing
KW  - Computer vision
KW  - Deep neural networks
KW  - Image segmentation
KW  - Inverse problems
KW  - Medical imaging
KW  - Semantics
U2  - Comparative studies
U2  - Computational imaging
U2  - Feature aggregation
U2  - High level semantics
U2  - Neural network model
U2  - Optimization based methods
U2  - Reconstruction networks
U2  - Semantic information
DO  - 10.1007/978-3-030-01231-1_4
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_4
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Receptive Field Block Net for Accurate and Fast Object Detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Songtao
A1  - Huang, Di
A1  - Wang, Yunhong
AD  - Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing; 100191, China
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978880
SP  - 404
EP  - 419
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Current top-performing object detectors depend on deep CNN backbones, such as ResNet-101 and Inception, benefiting from their powerful feature representations but suffering from high computational costs. Conversely, some lightweight model based detectors fulfil real time processing, while their accuracies are often criticized. In this paper, we explore an alternative to build a fast and accurate detector by strengthening lightweight features using a hand-crafted mechanism. Inspired by the structure of Receptive Fields (RFs) in human visual systems, we propose a novel RF Block (RFB) module, which takes the relationship between the size and eccentricity of RFs into account, to enhance the feature discriminability and robustness. We further assemble RFB to the top of SSD, constructing the RFB Net detector. To evaluate its effectiveness, experiments are conducted on two major benchmarks and the results show that RFB Net is able to reach the performance of advanced very deep detectors while keeping the real-time speed. Code is available at https://github.com/ruinmessi/RFBNet.  2018, Springer Nature Switzerland AG.
KW  - Object detection
KW  - Benchmarking
KW  - Computer vision
KW  - Feature extraction
KW  - Object recognition
U2  - Computational costs
U2  - Discriminability
U2  - Feature representation
U2  - Human Visual System
U2  - Object detectors
U2  - Real time
U2  - Realtime processing
U2  - Receptive fields
DO  - 10.1007/978-3-030-01252-6_24
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_24
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep Video Generation, Prediction and Completion of Human Action Sequences
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Cai, Haoye
A1  - Bai, Chunyan
A1  - Tai, Yu-Wing
A1  - Tang, Chi-Keung
AD  - Hong Kong University of Science and Technology, Clear Water Bay, Hong KongTencent Youtu, Shenzhen, ChinaStanford University, Stanford; CA; 94305, United StatesCarnegie Mellon University, Pittsburgh; PA; 15213, United States
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005970
SP  - 374
EP  - 390
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Current video generation/prediction/completion results are limited, due to the severe ill-posedness inherent in these three problems. In this paper, we focus on human action videos, and propose a general, two-stage deep framework to generate human action videos with no constraints or arbitrary number of constraints, which uniformly addresses the three problems: video generation given no input frames, video prediction given the first few frames, and video completion given the first and last frames. To solve video generation from scratch, we build a two-stage framework where we first train a deep generative model that generates human pose sequences from random noise, and then train a skeleton-to-image network to synthesize human action videos given the human pose sequences generated. To solve video prediction and completion, we exploit our trained model and conduct optimization over the latent space to generate videos that best suit the given input frame constraints. With our novel method, we sidestep the original ill-posed problems and produce for the first time high-quality video generation/prediction/completion results of much longer duration. We present quantitative and qualitative evaluations to show that our approach outperforms state-of-the-art methods in all three tasks.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Forecasting
U2  - Arbitrary number
U2  - Generative model
U2  - High quality video
U2  - Ill posed problem
U2  - Qualitative evaluations
U2  - State-of-the-art methods
U2  - Video completion
U2  - Video generation
DO  - 10.1007/978-3-030-01216-8_23
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_23
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Orthogonal deep features decomposition for age-invariant face recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Yitong
A1  - Gong, Dihong
A1  - Zhou, Zheng
A1  - Ji, Xing
A1  - Wang, Hao
A1  - Li, Zhifeng
A1  - Liu, Wei
A1  - Zhang, Tong
AD  - Tencent AI Lab, Beijing, China
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006196
SP  - 764
EP  - 779
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - As facial appearance is subject to significant intra-class variations caused by the aging process over time, age-invariant face recognition (AIFR) remains a major challenge in face recognition community. To reduce the intra-class discrepancy caused by the aging, in this paper we propose a novel approach (namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep face features. Specifically, we decompose deep face features into two orthogonal components to represent age-related and identity-related features. As a result, identity-related features that are robust to aging are then used for AIFR. Besides, for complementing the existing cross-age datasets and advancing the research in this field, we construct a brand-new large-scale Cross-Age Face dataset (CAF). Extensive experiments conducted on the three public domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have shown the effectiveness of the proposed approach and the value of the constructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most popular general face recognition (GFR) dataset LFW additionally demonstrates the comparable generalization performance on GFR.  Springer Nature Switzerland AG 2018.
KW  - Face recognition
KW  - Benchmarking
KW  - Computer vision
KW  - Neural networks
KW  - Room and pillar mining
U2  - Convolutional neural network
U2  - Cross-age face dataset
U2  - Face features
U2  - Facial appearance
U2  - Generalization performance
U2  - Intra-class variation
U2  - Orthogonal components
U2  - Public domains
DO  - 10.1007/978-3-030-01267-0_45
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_45
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning to navigate for fine-grained classification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yang, Ze
A1  - Luo, Tiange
A1  - Wang, Dong
A1  - Hu, Zhiqiang
A1  - Gao, Jun
A1  - Wang, Liwei
AD  - Key Laboratory of Machine Perception, MOE, School of EECS, Peking University, Beijing, ChinaCenter for Data Science, Beijing Institute of Big Data Research, Peking University, Beijing, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020313
SP  - 438
EP  - 454
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Fine-grained classification is challenging due to the difficulty of finding discriminative features. Finding those subtle traits that fully characterize the object is not straightforward. To handle this circumstance, we propose a novel self-supervision mechanism to effectively localize informative regions without the need of bounding-box/part annotations. Our model, termed NTS-Net for Navigator-Teacher-Scrutinizer Network, consists of a Navigator agent, a Teacher agent and a Scrutinizer agent. In consideration of intrinsic consistency between informativeness of the regions and their probability being ground-truth class, we design a novel training paradigm, which enables Navigator to detect most informative regions under the guidance from Teacher. After that, the Scrutinizer scrutinizes the proposed regions from Navigator and makes predictions. Our model can be viewed as a multi-agent cooperation, wherein agents benefit from each other, and make progress together. NTS-Net can be trained end-to-end, while provides accurate fine-grained classification predictions as well as highly informative regions during inference. We achieve state-of-the-art performance in extensive benchmark datasets.  2018, Springer Nature Switzerland AG.
KW  - Teaching
KW  - Benchmarking
KW  - Computer vision
KW  - Multi agent systems
KW  - Personnel training
U2  - Benchmark datasets
U2  - Classification prediction
U2  - Discriminative features
U2  - Ground truth
U2  - Informative ness
U2  - Multi agent cooperation
U2  - State-of-the-art performance
U2  - Supervision mechanisms
DO  - 10.1007/978-3-030-01264-9_26
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_26
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Real-time ‘Actor-Critic’ tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Boyu
A1  - Wang, Dong
A1  - Li, Peixia
A1  - Wang, Shuang
A1  - Lu, Huchuan
AD  - School of Information and Communication Engineering, Dalian University of Technology, Dalian, ChinaAlibaba Group, Hangzhou, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977596
SP  - 328
EP  - 345
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this work, we propose a novel tracking algorithm with real-time performance based on the Actor-Critic framework. This framework consists of two major components: Actor and Critic. The Actor model aims to infer the optimal choice in a continuous action space, which directly makes the tracker move the bounding box to the objects location in the current frame. For offline training, the Critic model is introduced to form a Actor-Critic framework with reinforcement learning and outputs a Q-value to guide the learning process of both Actor and Critic deep networks. Then, we modify the original deep deterministic policy gradient algorithm to effectively train our Actor-Critic model for the tracking task. For online tracking, the Actor model provides a dynamic search strategy to locate the tracked object efficiently and the Critic model acts as a verification module to make our tracker more robust. To the best of our knowledge, this work is the first attempt to exploit the continuous action and Actor-Critic framework for visual tracking. Extensive experimental results on popular benchmarks demonstrate that the proposed tracker performs favorably against many state-of-the-art methods, with real-time performance.  Springer Nature Switzerland AG 2018.
KW  - Reinforcement learning
KW  - Benchmarking
KW  - Computer vision
U2  - Continuous actions
U2  - Off-line training
U2  - On-line tracking
U2  - Real time performance
U2  - Real time tracking
U2  - State-of-the-art methods
U2  - Tracking algorithm
U2  - Visual Tracking
DO  - 10.1007/978-3-030-01234-2_20
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_20
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - ReenactGAN: Learning to Reenact Faces via Boundary Transfer
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wu, Wayne
A1  - Zhang, Yunxuan
A1  - Li, Cheng
A1  - Qian, Chen
A1  - Loy, Chen Change
AD  - SenseTime Research, Beijing, ChinaNanyang Technological University, Singapore, Singapore
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977716
SP  - 622
EP  - 638
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present a novel learning-based framework for face reenactment. The proposed method, known as ReenactGAN, is capable of transferring facial movements and expressions from an arbitrary persons monocular video input to a target persons video. Instead of performing a direct transfer in the pixel space, which could result in structural artifacts, we first map the source face onto a boundary latent space. A transformer is subsequently used to adapt the source faces boundary to the targets boundary. Finally, a target-specific decoder is used to generate the reenacted target face. Thanks to the effective and reliable boundary-based transfer, our method can perform photo-realistic face reenactment. In addition, ReenactGAN is appealing in that the whole reenactment process is purely feed-forward, and thus the reenactment process can run in real-time (30 FPS on one GTX 1080 GPU). Dataset and model are publicly available on our project page (Project Page: https://wywu.github.io/projects/ReenactGAN/ReenactGAN.html).  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Three dimensional computer graphics
U2  - Direct transfer
U2  - Face alignment
U2  - Face generation
U2  - Face reenactment
U2  - Facial movements
U2  - Feed forward
U2  - Monocular video
U2  - Photo-realistic
DO  - 10.1007/978-3-030-01246-5_37
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_37
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Factorizable Net: An Efficient Subgraph-Based Framework for Scene Graph Generation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Yikang
A1  - Ouyang, Wanli
A1  - Zhou, Bolei
A1  - Shi, Jianping
A1  - Zhang, Chao
A1  - Wang, Xiaogang
AD  - The Chinese University of Hong Kong, Hong KongSenseTime Computer Vision Research Group, The University of Sydney, Sydney, AustraliaMIT CSAIL, Cambridge, United StatesSensetime Ltd., Beijing, ChinaSamsung Telecommunication Research Institute, Beijing, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977699
SP  - 346
EP  - 363
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Generating scene graph to describe the object interactions inside an image gains increasing interests these years. However, most of the previous methods use complicated structures with slow inference speed or rely on the external data, which limits the usage of the model in real-life scenarios. To improve the efficiency of scene graph generation, we propose a subgraph-based connection graph to concisely represent the scene graph during the inference. A bottom-up clustering method is first used to factorize the entire graph into subgraphs, where each subgraph contains several objects and a subset of their relationships. By replacing the numerous relationship representations of the scene graph with fewer subgraph and object features, the computation in the intermediate stage is significantly reduced. In addition, spatial information is maintained by the subgraph features, which is leveraged by our proposed Spatial-weighted Message Passing (SMP) structure and Spatial-sensitive Relation Inference (SRI) module to facilitate the relationship recognition. On the recent Visual Relationship Detection and Visual Genome datasets, our method outperforms the state-of-the-art method in both accuracy and speed. Code has been made publicly available (https://github.com/yikang-li/FactorizableNet ).  2018, Springer Nature Switzerland AG.
KW  - Visual languages
KW  - Computer vision
KW  - Message passing
U2  - Clustering methods
U2  - Complicated structures
U2  - Intermediate stage
U2  - Object interactions
U2  - Scene graph
U2  - Scene understanding
U2  - Spatial informations
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01246-5_21
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_21
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - SkipNet: Learning dynamic routing in convolutional networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Xin
A1  - Yu, Fisher
A1  - Dou, Zi-Yi
A1  - Darrell, Trevor
A1  - Gonzalez, Joseph E.
AD  - University of California, Berkeley, United StatesNanjing University, Nanjing, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008945
SP  - 420
EP  - 436
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - While deeper convolutional networks are needed to achieve maximum accuracy in visual perception tasks, for many inputs shallower networks are sufficient. We exploit this observation by learning to skip convolutional layers on a per-input basis. We introduce SkipNet, a modified residual network, that uses a gating network to selectively skip convolutional blocks based on the activations of the previous layer. We formulate the dynamic skipping problem in the context of sequential decision making and propose a hybrid learning algorithm that combines supervised learning and reinforcement learning to address the challenges of non-differentiable skipping decisions. We show SkipNet reduces computation by 30 - 90 % while preserving the accuracy of the original model on four benchmark datasets and outperforms the state-of-the-art dynamic networks and static compression methods. We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped.  Springer Nature Switzerland AG 2018.
KW  - Learning algorithms
KW  - Computer vision
KW  - Convolution
KW  - Decision making
KW  - Dynamic routing algorithms
KW  - Reinforcement learning
U2  - Benchmark datasets
U2  - Convolutional networks
U2  - Hybrid learning algorithm
U2  - Maximum accuracies
U2  - Non-differentiable
U2  - Sequential decision making
U2  - Static compression
U2  - Visual perception
DO  - 10.1007/978-3-030-01261-8_25
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_25
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep multi-task learning to recognise subtle facial expressions of mental states
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Hu, Guosheng
A1  - Liu, Li
A1  - Yuan, Yang
A1  - Yu, Zehao
A1  - Hua, Yang
A1  - Zhang, Zhihong
A1  - Shen, Fumin
A1  - Shao, Ling
A1  - Hospedales, Timothy
A1  - Robertson, Neil
A1  - Yang, Yongxin
AD  - Anyvision, Queens Road, Belfast, United KingdomECIT, Queens University of Belfast, Belfast, United KingdomInception Institute of Artificial Intelligence, Abu Dhabi, United Arab EmiratesSoftware Department, Xiamen University, Xiamen, ChinaUniversity of Electronic Science and Technology of China, Chengdu, ChinaSchool of Informatics, University of Edinburgh, Edinburgh, United KingdomYangs Accounting Consultancy Ltd., London, United Kingdom
VL  - 11216 LNCS
PY  - 2018
U1  - 20184305977474
SP  - 106
EP  - 123
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Facial expression recognition is a topical task. However, very little research investigates subtle expression recognition, which is important for mental activity analysis, deception detection, etc. We address subtle expression recognition through convolutional neural networks (CNNs) by developing multi-task learning (MTL) methods to effectively leverage a side task: facial landmark detection. Existing MTL methods follow a design pattern of shared bottom CNN layers and task-specific top layers. However, the sharing architecture is usually heuristically chosen, as it is difficult to decide which layers should be shared. Our approach is composed of (1) a novel MTL framework that automatically learns which layers to share through optimisation under tensor trace norm regularisation and (2) an invariant representation learning approach that allows the CNN to leverage tasks defined on disjoint datasets without suffering from dataset distribution shift. To advance subtle expression recognition, we contribute a Large-scale Subtle Emotions and Mental States in the Wild database (LSEMSW). LSEMSW includes a variety of cognitive states as well as basic emotions. It contains 176K images, manually annotated with 13 emotions, and thus provides the first subtle expression dataset large enough for training deep CNNs. Evaluations on LSEMSW and 300-W (landmark) databases show the effectiveness of the proposed methods. In addition, we investigate transferring knowledge learned from LSEMSW database to traditional (non-subtle) expression recognition. We achieve very competitive performance on Oulu-Casia NIRVis and CK+ databases via transfer learning.  Springer Nature Switzerland AG 2018.
KW  - Face recognition
KW  - Computer vision
KW  - Database systems
KW  - Deep learning
KW  - Linearization
KW  - Neural networks
U2  - Competitive performance
U2  - Convolutional neural network
U2  - Deception detection
U2  - Expression recognition
U2  - Facial expression recognition
U2  - Facial landmark detection
U2  - Invariant representation
U2  - Sharing architectures
DO  - 10.1007/978-3-030-01258-8_7
L2  - http://dx.doi.org/10.1007/978-3-030-01258-8_7
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Proximal dehaze-net: A prior learning-based deep network for single image dehazing
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yang, Dong
A1  - Sun, Jian
AD  - Xian Jiaotong University, Xian; 710049, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977621
SP  - 729
EP  - 746
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Photos taken in hazy weather are usually covered with white masks and often lose important details. In this paper, we propose a novel deep learning approach for single image dehazing by learning dark channel and transmission priors. First, we build an energy model for dehazing using dark channel and transmission priors and design an iterative optimization algorithm using proximal operators for these two priors. Second, we unfold the iterative algorithm to be a deep network, dubbed as proximal dehaze-net, by learning the proximal operators using convolutional neural networks. Our network combines the advantages of traditional prior-based dehazing methods and deep learning methods by incorporating haze-related prior learning into deep network. Experiments show that our method achieves state-of-the-art performance for single image dehazing.  Springer Nature Switzerland AG 2018.
KW  - Demulsification
KW  - Computer vision
KW  - Deep neural networks
KW  - Iterative methods
KW  - Neural networks
U2  - Convolutional neural network
U2  - Iterative algorithm
U2  - Iterative optimization algorithms
U2  - Learning approach
U2  - Learning methods
U2  - Prior learning
U2  - Single image dehazing
U2  - State-of-the-art performance
DO  - 10.1007/978-3-030-01234-2_43
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_43
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Triplet loss in siamese network for object tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Dong, Xingping
A1  - Shen, Jianbing
AD  - Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, ChinaInception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008948
SP  - 472
EP  - 488
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Object tracking is still a critical and challenging problem with many applications in computer vision. For this challenge, more and more researchers pay attention to applying deep learning to get powerful feature for better tracking accuracy. In this paper, a novel triplet loss is proposed to extract expressive deep feature for object tracking by adding it into Siamese network framework instead of pairwise loss for training. Without adding any inputs, our approach is able to utilize more elements for training to achieve more powerful feature via the combination of original samples. Furthermore, we propose a theoretical analysis by combining comparison of gradients and back-propagation, to prove the effectiveness of our method. In experiments, we apply the proposed triplet loss for three real-time trackers based on Siamese network. And the results on several popular tracking benchmarks show our variants operate at almost the same frame-rate with baseline trackers and achieve superior tracking performance than them, as well as the comparable accuracy with recent state-of-the-art real-time trackers.  Springer Nature Switzerland AG 2018.
KW  - Tracking (position)
KW  - Backpropagation
KW  - Benchmarking
KW  - Computer vision
KW  - Deep learning
U2  - Network frameworks
U2  - Object Tracking
U2  - Original sample
U2  - Real time
U2  - Real-time tracker
U2  - Recent state
U2  - Tracking accuracy
U2  - Tracking performance
DO  - 10.1007/978-3-030-01261-8_28
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_28
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Affinity Derivation and Graph Merge for Instance Segmentation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Yiding
A1  - Yang, Siyu
A1  - Li, Bin
A1  - Zhou, Wengang
A1  - Xu, Jizheng
A1  - Li, Houqiang
A1  - Lu, Yan
AD  - Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, ChinaBeihang University, Beijing, ChinaMicrosoft Research, Beijing, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977832
SP  - 708
EP  - 724
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present an instance segmentation scheme based on pixel affinity information, which is the relationship of two pixels belonging to the same instance. In our scheme, we use two neural networks with similar structures. One predicts the pixel level semantic score and the other is designed to derive pixel affinities. Regarding pixels as the vertexes and affinities as edges, we then propose a simple yet effective graph merge algorithm to cluster pixels into instances. Experiments show that our scheme generates fine grained instance masks. With Cityscape training data, the proposed scheme achieves 27.3 AP on test set.  2018, Springer Nature Switzerland AG.
KW  - Pixels
KW  - Computer vision
KW  - Semantics
U2  - Fine grained
U2  - Pixel affinities
U2  - Pixel level
U2  - Proposal-free
U2  - Segmentation scheme
U2  - Test sets
U2  - Training data
DO  - 10.1007/978-3-030-01219-9_42
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_42
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - PyramidBox: A context-assisted single shot face detector
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Tang, Xu
A1  - Du, Daniel K.
A1  - He, Zeqiang
A1  - Liu, Jingtuo
AD  - Baidu Inc., Beijing, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977523
SP  - 812
EP  - 828
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Face detection has been well studied for many years and one of remaining challenges is to detect small, blurred and partially occluded faces in uncontrolled environment. This paper proposes a novel context-assisted single shot face detector, named PyramidBox to handle the hard face detection problem. Observing the importance of the context, we improve the utilization of contextual information in the following three aspects. First, we design a novel context anchor to supervise high-level contextual feature learning by a semi-supervised method, which we call it PyramidAnchors. Second, we propose the Low-level Feature Pyramid Network to combine adequate high-level context semantic feature and Low-level facial feature together, which also allows the PyramidBox to predict faces of all scales in a single shot. Third, we introduce a context-sensitive structure to increase the capacity of prediction network to improve the final accuracy of output. In addition, we use the method of Data-anchor-sampling to augment the training samples across different scales, which increases the diversity of training data for smaller faces. By exploiting the value of context, PyramidBox achieves superior performance among the state-of-the-art over the two common face detection benchmarks, FDDB and WIDER FACE. Our code is available in PaddlePaddle: https://github.com/PaddlePaddle/models/tree/develop/fluid/face_detection.  Springer Nature Switzerland AG 2018.
KW  - Face recognition
KW  - Benchmarking
KW  - Computer vision
KW  - Semantics
KW  - Supervised learning
U2  - Context
U2  - Contextual feature
U2  - Contextual information
U2  - Face detection problem
U2  - Low-level features
U2  - PyramidBox
U2  - Semi-supervised method
U2  - Single shots
DO  - 10.1007/978-3-030-01240-3_49
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_49
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Joint 3d face reconstruction and dense alignment with position map regression network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Feng, Yao
A1  - Wu, Fan
A1  - Shao, Xiaohu
A1  - Wang, Yanfeng
A1  - Zhou, Xi
AD  - Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, ChinaCloudWalk Technology, Guangzhou, ChinaCIGIT, Chinese Academy of Sciences, Chongqing, ChinaUniversity of Chinese Academy of Sciences, Beijing, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020321
SP  - 557
EP  - 574
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We propose a straightforward method that simultaneously reconstructs the 3D facial structure and provides dense alignment. To achieve this, we design a 2D representation called UV position map which records the 3D shape of a complete face in UV space, then train a simple Convolutional Neural Network to regress it from a single 2D image. We also integrate a weight mask into the loss function during training to improve the performance of the network. Our method does not rely on any prior face model, and can reconstruct full facial geometry along with semantic meaning. Meanwhile, our network is very light-weighted and spends only 9.8ams to process an image, which is extremely faster than previous works. Experiments on multiple challenging datasets show that our method surpasses other state-of-the-art methods on both reconstruction and alignment tasks by a large margin. Code is available at https://github.com/YadiraF/PRNet.  2018, Springer Nature Switzerland AG.
KW  - Image reconstruction
KW  - Computer vision
KW  - Neural networks
KW  - Semantics
U2  - 3D face reconstruction
U2  - Convolutional neural network
U2  - Face alignment
U2  - Facial geometry
U2  - Facial structure
U2  - Loss functions
U2  - State-of-the-art methods
U2  - Straight-forward method
DO  - 10.1007/978-3-030-01264-9_33
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_33
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Bidirectional feature pyramid network with recurrent attention residual modules for shadow detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhu, Lei
A1  - Deng, Zijun
A1  - Hu, Xiaowei
A1  - Fu, Chi-Wing
A1  - Xu, Xuemiao
A1  - Qin, Jing
A1  - Heng, Pheng-Ann
AD  - The Chinese University of Hong Kong, Hong KongThe Hong Kong Polytechnic University, Hong KongSouth China University of Technology, Guangzhou, ChinaGuangdong Provincial Key Lab of Computational Intelligence and Cyberspace Information, South China University of Technology, Guangzhou, ChinaShenzhen Key Laboratory of Virtual Reality and Human Interaction Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977426
SP  - 122
EP  - 137
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper presents a network to detect shadows by exploring and combining global context in deep layers and local context in shallow layers of a deep convolutional neural network (CNN). There are two technical contributions in our network design. First, we formulate the recurrent attention residual (RAR) module to combine the contexts in two adjacent CNN layers and learn an attention map to select a residual and then refine the context features. Second, we develop a bidirectional feature pyramid network (BFPN) to aggregate shadow contexts spanned across different CNN layers by deploying two series of RAR modules in the network to iteratively combine and refine context features: one series to refine context features from deep to shallow layers, and another series from shallow to deep layers. Hence, we can better suppress false detections and enhance shadow details at the same time. We evaluate our network on two common shadow detection benchmark datasets: SBU and UCF. Experimental results show that our network outperforms the best existing method with 34.88% reduction on SBU and 34.57% reduction on UCF for the balance error rate.  Springer Nature Switzerland AG 2018.
KW  - Feature extraction
KW  - Computer vision
KW  - Deep neural networks
KW  - Iterative methods
KW  - Neural networks
U2  - Benchmark datasets
U2  - Context features
U2  - Deep convolutional neural networks
U2  - False detections
U2  - Feature pyramid
U2  - Local contexts
U2  - Shadow detections
U2  - Technical contribution
DO  - 10.1007/978-3-030-01231-1_8
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_8
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - 3D recurrent neural networks with context fusion for point cloud semantic segmentation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Ye, Xiaoqing
A1  - Li, Jiamao
A1  - Huang, Hexiao
A1  - Du, Liang
A1  - Zhang, Xiaolin
AD  - Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, Shanghai, ChinaUniversity of Chinese Academy of Sciences, Beijing, ChinaSchool of Science and Technology, Shanghai Open University, Shanghai, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977601
SP  - 415
EP  - 430
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Semantic segmentation of 3D unstructured point clouds remains an open research problem. Recent works predict semantic labels of 3D points by virtue of neural networks but take limited context knowledge into consideration. In this paper, a novel end-to-end approach for unstructured point cloud semantic segmentation, named 3P-RNN, is proposed to exploit the inherent contextual features. First the efficient pointwise pyramid pooling module is investigated to capture local structures at various densities by taking multi-scale neighborhood into account. Then the two-direction hierarchical recurrent neural networks (RNNs) are utilized to explore long-range spatial dependencies. Each recurrent layer takes as input the local features derived from unrolled cells and sweeps the 3D space along two directions successively to integrate structure knowledge. On challenging indoor and outdoor 3D datasets, the proposed framework demonstrates robust performance superior to state-of-the-arts.  Springer Nature Switzerland AG 2018.
KW  - Recurrent neural networks
KW  - Computer vision
KW  - Semantic Web
KW  - Semantics
U2  - Contextual feature
U2  - Point wise
U2  - Recurrent neural network (RNNs)
U2  - Research problems
U2  - Robust performance
U2  - Semantic segmentation
U2  - Spatial dependencies
U2  - Unstructured point clouds
DO  - 10.1007/978-3-030-01234-2_25
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_25
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Sub-GAN: An Unsupervised Generative Model via Subspaces
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liang, Jie
A1  - Yang, Jufeng
A1  - Lee, Hsin-Ying
A1  - Wang, Kai
A1  - Yang, Ming-Hsuan
AD  - Nankai University, Tianjin, ChinaUniversity of California, Merced, United StatesGoogle Cloud, Merced, United States
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978901
SP  - 726
EP  - 743
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The recent years have witnessed significant growth in constructing robust generative models to capture informative distributions of natural data. However, it is difficult to fully exploit the distribution of complex data, like images and videos, due to the high dimensionality of ambient space. Sequentially, how to effectively guide the training of generative models is a crucial issue. In this paper, we present a subspace-based generative adversarial network (Sub-GAN) which simultaneously disentangles multiple latent subspaces and generates diverse samples correspondingly. Since the high-dimensional natural data usually lies on a union of low-dimensional subspaces which contain semantically extensive structure, Sub-GAN incorporates a novel clusterer that can interact with the generator and discriminator via subspace information. Unlike the traditional generative models, the proposed Sub-GAN can control the diversity of the generated samples via the multiplicity of the learned subspaces. Moreover, the Sub-GAN follows an unsupervised fashion to explore not only the visual classes but the latent continuous attributes. We demonstrate that our model can discover meaningful visual attributes which is hard to be annotated via strong supervision, e.g., the writing style of digits, thus avoid the mode collapse problem. Extensive experimental results show the competitive performance of the proposed method for both generating diverse images with satisfied quality and discovering discriminative latent subspaces.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Image retrieval
U2  - Adversarial networks
U2  - Competitive performance
U2  - Continuous attribute
U2  - Generative model
U2  - High dimensionality
U2  - High-dimensional
U2  - Low-dimensional subspace
U2  - Visual attributes
DO  - 10.1007/978-3-030-01252-6_43
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_43
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Evaluating Capability of Deep Neural Networks for Image Classification via Information Plane
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Cheng, Hao
A1  - Lian, Dongze
A1  - Gao, Shenghua
A1  - Geng, Yanlin
AD  - ShanghaiTech University, Shanghai, China
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978866
SP  - 181
EP  - 195
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Inspired by the pioneering work of information bottleneck principle for Deep Neural Networks (DNNs) analysis, we design an information plane based framework to evaluate the capability of DNNs for image classification tasks, which not only helps understand the capability of DNNs, but also helps us choose a neural network which leads to higher classification accuracy more efficiently. Further, with experiments, the relationship among the model accuracy, I(X; T) and I(T; Y) are analyzed, where I(X; T) and I(T; Y) are the mutual information of DNNs output T with input X and label Y. We also show the information plane is more informative than loss curve and apply mutual information to infer the models capability for recognizing objects of each class. Our studies would facilitate a better understanding of DNNs.  2018, Springer Nature Switzerland AG.
KW  - Classification (of information)
KW  - Computer vision
KW  - Deep neural networks
KW  - Image classification
KW  - Neural networks
U2  - Classification accuracy
U2  - Information bottleneck
U2  - Information bottleneck principles
U2  - Model accuracy
U2  - Mutual informations
DO  - 10.1007/978-3-030-01252-6_11
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_11
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Robust anchor embedding for unsupervised video person re-identification in the wild
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Ye, Mang
A1  - Lan, Xiangyuan
A1  - Yuen, Pong C.
AD  - Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977586
SP  - 176
EP  - 193
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper addresses the scalability and robustness issues of estimating labels from imbalanced unlabeled data for unsupervised video-based person re-identification (re-ID). To achieve it, we propose a novel Robust AnChor Embedding (RACE) framework via deep feature representation learning for large-scale unsupervised video re-ID. Within this framework, anchor sequences representing different persons are firstly selected to formulate an anchor graph which also initializes the CNN model to get discriminative feature representations for later label estimation. To accurately estimate labels from unlabeled sequences with noisy frames, robust anchor embedding is introduced based on the regularized affine hull. Efficiency is ensured with kNN anchors embedding instead of the whole anchor set under manifold assumptions. After that, a robust and efficient top-k counts label prediction strategy is proposed to predict the labels of unlabeled image sequences. With the newly estimated labeled sequences, the unified anchor embedding framework enables the feature learning process to be further facilitated. Extensive experimental results on the large-scale dataset show that the proposed method outperforms existing unsupervised video re-ID methods.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Deep learning
U2  - Discriminative features
U2  - Feature representation
U2  - Label predictions
U2  - Large-scale dataset
U2  - Person re identifications
U2  - Robust anchor embedding
U2  - Unlabeled sequences
U2  - Unsupervised person re-id
DO  - 10.1007/978-3-030-01234-2_11
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_11
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - HBE: Hand branch ensemble network for real-time 3d hand pose estimation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhou, Yidan
A1  - Lu, Jian
A1  - Du, Kuo
A1  - Lin, Xiangbo
A1  - Sun, Yi
A1  - Ma, Xiaohong
AD  - Dalian University of Technology, Dalian, ChinaDalian University, Dalian, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020319
SP  - 521
EP  - 536
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The goal of this paper is to estimate the 3D coordinates of the hand joints from a single depth image. To give consideration to both the accuracy and the real time performance, we design a novel three-branch Convolutional Neural Networks named Hand Branch Ensemble network (HBE), where the three branches correspond to the three parts of a hand: the thumb, the index finger and the other fingers. The structural design inspiration of the HBE network comes from the understanding of the differences in the functional importance of different fingers. In addition, a feature ensemble layer along with a low-dimensional embedding layer ensures the overall hand shape constraints. The experimental results on three public datasets demonstrate that our approach achieves comparable or better performance to state-of-the-art methods with less training data, shorter training time and faster frame rate.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Convolution
KW  - Neural networks
KW  - Structural design
U2  - 3D hand pose estimations
U2  - Convolutional neural network
U2  - Depth image
U2  - Ensemble networks
U2  - Hand pose estimations
U2  - Low dimensional embedding
U2  - Real time performance
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01264-9_31
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_31
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep attention neural tensor network for visual question answering
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Bai, Yalong
A1  - Fu, Jianlong
A1  - Zhao, Tiejun
A1  - Mei, Tao
AD  - Harbin Institute of Technology, Harbin, ChinaJD AI Research, Beijing, ChinaMicrosoft Research Asia, Beijing, China
VL  - 11216 LNCS
PY  - 2018
U1  - 20184305977442
SP  - 21
EP  - 37
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Visual question answering (VQA) has drawn great attention in cross-modal learning problems, which enables a machine to answer a natural language question given a reference image. Significant progress has been made by learning rich embedding features from images and questions by bilinear models, while neglects the key role from answers. In this paper, we propose a novel deep attention neural tensor network (DA-NTN) for visual question answering, which can discover the joint correlations over images, questions and answers with tensor-based representations. First, we model one of the pairwise interaction (e.g., image and question) by bilinear features, which is further encoded with the third dimension (e.g., answer) to be a triplet by bilinear tensor product. Second, we decompose the correlation of different triplets by different answer and question types, and further propose a slice-wise attention module on tensor to select the most discriminative reasoning process for inference. Third, we optimize the proposed DA-NTN by learning a label regression with KL-divergence losses. Such a design enables scalable training and fast convergence over a large number of answer set. We integrate the proposed DA-NTN structure into the state-of-the-art VQA models (e.g., MLB and MUTAN). Extensive experiments demonstrate the superior accuracy than the original MLB and MUTAN models, with 1.98%, 1.70% relative increases on VQA-2.0 dataset, respectively.  Springer Nature Switzerland AG 2018.
KW  - Tensors
KW  - Computer vision
KW  - Natural language processing systems
KW  - Visual languages
U2  - Fast convergence
U2  - Learning problem
U2  - Natural language questions
U2  - Open-ended VQA
U2  - Pairwise interaction
U2  - Question Answering
U2  - Reasoning process
U2  - State of the art
DO  - 10.1007/978-3-030-01258-8_2
L2  - http://dx.doi.org/10.1007/978-3-030-01258-8_2
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - LQ-Nets: Learned quantization for highly accurate and compact deep neural networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Dongqing
A1  - Yang, Jiaolong
A1  - Ye, Dongqiangzi
A1  - Hua, Gang
AD  - Microsoft Research, Beijing, ChinaMicrosoft Cloud and AI, Redmond, United States
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006058
SP  - 373
EP  - 390
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Although weight and activation quantization is an effective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using fixed, handcrafted quantization schemes such as uniform or logarithmic quantization. Our method for learning the quantizers applies to both network weights and activations with arbitrary-bit precision, and our quantizers are easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets show that our method works consistently well for various network structures such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous quantization methods in terms of accuracy by an appreciable margin. Code available at https://github.com/Microsoft/LQ-Nets.  Springer Nature Switzerland AG 2018.
KW  - Deep neural networks
KW  - Chemical activation
KW  - Compaction
KW  - Computer vision
U2  - Effective approaches
U2  - Highly accurate
U2  - Network structures
U2  - Precision model
U2  - Prediction accuracy
U2  - Quantization
U2  - Quantization schemes
U2  - Quantized models
DO  - 10.1007/978-3-030-01237-3_23
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_23
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Generative adversarial network with spatial attention for face attribute editing
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Gang
A1  - Kan, Meina
A1  - Shan, Shiguang
A1  - Chen, Xilin
AD  - Key Lab of Intelligent Information Processing of Chinese Academy of Sciences, Institute of Computing Technology, CAS, Beijing; 100190, ChinaUniversity of Chinese Academy of Sciences, Beijing; 100049, ChinaCAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977403
SP  - 422
EP  - 437
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Face attribute editing aims at editing the face image with the given attribute. Most existing works employ Generative Adversarial Network (GAN) to operate face attribute editing. However, these methods inevitably change the attribute-irrelevant regions, as shown in Fig. 1. Therefore, we introduce the spatial attention mechanism into GAN framework (referred to as SaGAN), to only alter the attribute-specific region and keep the rest unchanged. Our approach SaGAN consists of a generator and a discriminator. The generator contains an attribute manipulation network (AMN) to edit the face image, and a spatial attention network (SAN) to localize the attribute-specific region which restricts the alternation of AMN within this region. The discriminator endeavors to distinguish the generated images from the real ones, and classify the face attribute. Experiments demonstrate that our approach can achieve promising visual results, and keep those attribute-irrelevant regions unchanged. Besides, our approach can benefit the face recognition by data augmentation.  Springer Nature Switzerland AG 2018.
KW  - Face recognition
KW  - Computer vision
U2  - Adversarial networks
U2  - Data augmentation
U2  - Face attribute editing
U2  - Face images
U2  - Spatial attention
DO  - 10.1007/978-3-030-01231-1_26
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_26
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Appearance-based gaze estimation via evaluation-guided asymmetric regression
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Cheng, Yihua
A1  - Lu, Feng
A1  - Zhang, Xucong
AD  - State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, ChinaBeijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, Beijing, ChinaMax Planck Institute for Informatics, Saarland Informatics Campus, Saarbrucken, Germany
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020341
SP  - 105
EP  - 121
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Eye gaze estimation has been increasingly demanded by recent intelligent systems to accomplish a range of interaction-related tasks, by using simple eye images as input. However, learning the highly complex regression between eye images and gaze directions is nontrivial, and thus the problem is yet to be solved efficiently. In this paper, we propose the Asymmetric Regression-Evaluation Network (ARE-Net), and try to improve the gaze estimation performance to its full extent. At the core of our method is the notion of two eye asymmetry observed during gaze estimation for the left and right eyes. Inspired by this, we design the multi-stream ARE-Net; one asymmetric regression network (AR-Net) predicts 3D gaze directions for both eyes with a novel asymmetric strategy, and the evaluation network (E-Net) adaptively adjusts the strategy by evaluating the two eyes in terms of their performance during optimization. By training the whole network, our method achieves promising results and surpasses the state-of-the-art methods on multiple public datasets.  2018, Springer Nature Switzerland AG.
KW  - Regression analysis
KW  - Computer vision
KW  - Intelligent systems
U2  - Appearance based
U2  - Asymmetric regression
U2  - Eye appearance
U2  - Eye images
U2  - Gaze direction
U2  - Gaze estimation
U2  - Multi-stream
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01264-9_7
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_7
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - ExFuse: Enhancing feature fusion for semantic segmentation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Zhenli
A1  - Zhang, Xiangyu
A1  - Peng, Chao
A1  - Xue, Xiangyang
A1  - Sun, Jian
AD  - Fudan University, Shanghai, ChinaMegvii Inc., Beijing, China
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978772
SP  - 273
EP  - 288
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Modern semantic segmentation frameworks usually combine low-level and high-level features from pre-trained backbone convolutional models to boost performance. In this paper, we first point out that a simple fusion of low-level and high-level features could be less effective because of the gap in semantic levels and spatial resolution. We find that introducing semantic information into low-level features and high-resolution details into high-level features is more effective for the later fusion. Based on this observation, we propose a new framework, named ExFuse, to bridge the gap between low-level and high-level features thus significantly improve the segmentation quality by 4.0% in total. Furthermore, we evaluate our approach on the challenging PASCAL VOC 2012 segmentation benchmark and achieve 87.9% mean IoU, which outperforms the previous state-of-the-art results.  Springer Nature Switzerland AG 2018.
KW  - Semantics
KW  - Computer vision
KW  - Convolution
KW  - Neural networks
U2  - Convolutional model
U2  - Convolutional neural network
U2  - High-level features
U2  - High-Resolution Details
U2  - Segmentation quality
U2  - Semantic information
U2  - Semantic segmentation
U2  - Spatial resolution
DO  - 10.1007/978-3-030-01249-6_17
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_17
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Joint representation and truncated inference learning for correlation filter based tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yao, Yingjie
A1  - Wu, Xiaohe
A1  - Zhang, Lei
A1  - Shan, Shiguang
A1  - Zuo, Wangmeng
AD  - Harbin Institute of Technology, Harbin; 150001, ChinaUniversity of Pittsburgh, 3362 Fifth Avenue, Pittsburgh; PA; 15213, United StatesInstitute of Computing Technology, CAS, Beijing; 100049, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977507
SP  - 560
EP  - 575
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Correlation filter (CF) based trackers generally include two modules, i.e., feature representation and on-line model adaptation. In existing off-line deep learning models for CF trackers, the model adaptation usually is either abandoned or has closed-form solution to make it feasible to learn deep representation in an end-to-end manner. However, such solutions fail to exploit the advances in CF models, and cannot achieve competitive accuracy in comparison with the state-of-the-art CF trackers. In this paper, we investigate the joint learning of deep representation and model adaptation, where an updater network is introduced for better tracking on future frame by taking current frame representation, tracking result, and last CF tracker as input. By modeling the representor as convolutional neural network (CNN), we truncate the alternating direction method of multipliers (ADMM) and interpret it as a deep network of updater, resulting in our model for learning representation and truncated inference (RTINet). Experiments demonstrate that our RTINet tracker achieves favorable tracking accuracy against the state-of-the-art trackers and its rapid version can run at a real-time speed of 24 fps. The code and pre-trained models will be publicly available at https://github.com/tourmaline612/RTINet.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Computer vision
KW  - Convolution
KW  - Neural networks
U2  - Alternating direction method of multiplier (ADMM)
U2  - Closed form solutions
U2  - Convolutional neural network
U2  - Convolutional Neural Networks (CNN)
U2  - Correlation filters
U2  - Feature representation
U2  - Tracking accuracy
U2  - Visual Tracking
DO  - 10.1007/978-3-030-01240-3_34
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_34
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Associating inter-image salient instances for weakly supervised semantic segmentation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Fan, Ruochen
A1  - Hou, Qibin
A1  - Cheng, Ming-Ming
A1  - Yu, Gang
A1  - Martin, Ralph R.
A1  - Hu, Shi-Min
AD  - Tsinghua University, Beijing, ChinaNankai University, Tianjin, ChinaMegvii Inc., Beijing, ChinaCardiff University, Cardiff; CF243AA, United Kingdom
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977495
SP  - 371
EP  - 388
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Effectively bridging between image level keyword annotations and corresponding image pixels is one of the main challenges in weakly supervised semantic segmentation. In this paper, we use an instance-level salient object detector to automatically generate salient instances (candidate objects) for training images. Using similarity features extracted from each salient instance in the whole training set, we build a similarity graph, then use a graph partitioning algorithm to separate it into multiple subgraphs, each of which is associated with a single keyword (tag). Our graph-partitioning-based clustering algorithm allows us to consider the relationships between all salient instances in the training set as well as the information within them. We further show that with the help of attention information, our clustering algorithm is able to correct certain wrong assignments, leading to more accurate results. The proposed framework is general, and any state-of-the-art fully-supervised network structure can be incorporated to learn the segmentation network. When working with DeepLab for semantic segmentation, our method outperforms state-of-the-art weakly supervised alternatives by a large margin, achieving 65.6 % mIoU on the PASCAL VOC 2012 dataset. We also combine our method with Mask R-CNN for instance segmentation, and demonstrated for the first time the ability of weakly supervised instance segmentation using only keyword annotations.  Springer Nature Switzerland AG 2018.
KW  - Clustering algorithms
KW  - Computer vision
KW  - Graph theory
KW  - Image segmentation
KW  - Object detection
KW  - Semantics
U2  - Graph Partitioning
U2  - Graph partitioning algorithms
U2  - Salient objects
U2  - Semantic segmentation
U2  - State of the art
U2  - Supervised network
U2  - Training image
U2  - Weak supervision
DO  - 10.1007/978-3-030-01240-3_23
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_23
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - SpiderCNN: Deep learning on point sets with parameterized convolutional filters
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Xu, Yifan
A1  - Fan, Tianqi
A1  - Xu, Mingye
A1  - Zeng, Long
A1  - Qiao, Yu
AD  - Tsinghua University, Beijing, ChinaGuangdong Key Lab of Computer Vision and Virtual Reality, SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006089
SP  - 90
EP  - 105
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Deep neural networks have enjoyed remarkable success for various vision tasks, however it remains challenging to apply CNNs to domains lacking a regular underlying structures such as 3D point clouds. Towards this we propose a novel convolutional architecture, termed SpiderCNN, to efficiently extract geometric features from point clouds. SpiderCNN is comprised of units called SpiderConv, which extend convolutional operations from regular grids to irregular point sets that can be embedded in Rn, by parametrizing a family of convolutional filters. We design the filter as a product of a simple step function that captures local geodesic information and a Taylor polynomial that ensures the expressiveness. SpiderCNN inherits the multi-scale hierarchical architecture from classical CNNs, which allows it to extract semantic deep features. Experiments on ModelNet40 demonstrate that SpiderCNN achieves state-of-the-art accuracy 92.4% on standard benchmarks, and shows competitive performance on segmentation task.  Springer Nature Switzerland AG 2018.
KW  - Deep neural networks
KW  - Benchmarking
KW  - Computer vision
KW  - Convolution
KW  - Geometry
KW  - Network architecture
KW  - Neural networks
KW  - Product design
KW  - Semantics
U2  - Competitive performance
U2  - Convolutional neural network
U2  - Geometric feature
U2  - Hierarchical architectures
U2  - Point cloud
U2  - State of the art
U2  - Step functions
U2  - Taylor polynomials
DO  - 10.1007/978-3-030-01237-3_6
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_6
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep metric learning with hierarchical triplet loss
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Ge, Weifeng
A1  - Huang, Weilin
A1  - Dong, Dengke
A1  - Scott, Matthew R.
AD  - Malong Technologies, Shenzhen, ChinaShenzhen Malong Artificial Intelligence Research Center, Shenzhen, ChinaThe University of Hong Kong, Pok Fu Lam, Hong Kong
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977393
SP  - 272
EP  - 288
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present a novel hierarchical triplet loss (HTL) capable of automatically collecting informative training samples (triplets) via a defined hierarchical tree that encodes global context information. This allows us to cope with the main limitation of random sampling in training a conventional triplet loss, which is a central issue for deep metric learning. Our main contributions are two-fold. (i) we construct a hierarchical class-level tree where neighboring classes are merged recursively. The hierarchical structure naturally captures the intrinsic data distribution over the whole dataset. (ii) we formulate the problem of triplet collection by introducing a new violate margin, which is computed dynamically based on the designed hierarchical tree. This allows it to automatically select meaningful hard samples with the guide of global context. It encourages the model to learn more discriminative features from visual similar classes, leading to faster convergence and better performance. Our method is evaluated on the tasks of image retrieval and face recognition, where it outperforms the standard triplet loss substantially by 1%18%, and achieves new state-of-the-art performance on a number of benchmarks.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Benchmarking
KW  - Computer vision
KW  - Face recognition
KW  - Forestry
KW  - Image retrieval
U2  - Discriminative features
U2  - Faster convergence
U2  - Hierarchical structures
U2  - Hierarchical tree
U2  - Metric learning
U2  - Random sampling
U2  - State-of-the-art performance
U2  - Training sample
DO  - 10.1007/978-3-030-01231-1_17
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_17
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Selective zero-shot classification with augmented attributes
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Song, Jie
A1  - Shen, Chengchao
A1  - Lei, Jie
A1  - Zeng, An-Xiang
A1  - Ou, Kairi
A1  - Tao, Dacheng
A1  - Song, Mingli
AD  - College of Computer Science and Technology, Zhejiang University, Hangzhou, ChinaAlibaba Group, Hangzhou, ChinaUBTECH Sydney AI Centre, SIT, FEIT, University of Sydney, Camperdown, Australia
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977501
SP  - 474
EP  - 490
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we introduce a selective zero-shot classification problem: how can the classifier avoid making dubious predictions? Existing attribute-based zero-shot classification methods are shown to work poorly in the selective classification scenario. We argue the under-complete human defined attribute vocabulary accounts for the poor performance. We propose a selective zero-shot classifier based on both the human defined and the automatically discovered residual attributes. The proposed classifier is constructed by firstly learning the defined and the residual attributes jointly. Then the predictions are conducted within the subspace of the defined attributes. Finally, the prediction confidence is measured by both the defined and the residual attributes. Experiments conducted on several benchmarks demonstrate that our classifier produces a superior performance to other methods under the risk-coverage trade-off metric.  Springer Nature Switzerland AG 2018.
KW  - Economic and social effects
KW  - Benchmarking
KW  - Computer vision
KW  - Forecasting
KW  - Risk management
U2  - Attribute-based
U2  - Defined attributes
U2  - Poor performance
U2  - Prediction confidence
U2  - Residual attributes
U2  - Risk coverage
U2  - Shot classification
U2  - Trade off
DO  - 10.1007/978-3-030-01240-3_29
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_29
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Video re-localization
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Feng, Yang
A1  - Ma, Lin
A1  - Liu, Wei
A1  - Zhang, Tong
A1  - Luo, Jiebo
AD  - Tencent AI Lab, Shenzhen, ChinaUniversity of Rochester, Rochester, United States
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020328
SP  - 55
EP  - 70
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Many methods have been developed to help people find the video content they want efficiently. However, there are still some unsolved problems in this area. For example, given a query video and a reference video, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query video? We define a distinctively new task, namely video re-localization, to address this need. Video re-localization is an important enabling technology with many applications, such as fast seeking in videos, video copy detection, as well as video surveillance. Meanwhile, it is also a challenging research task because the visual appearance of a semantic concept in videos can have large variations. The first hurdle to clear for the video re-localization task is the lack of existing datasets. It is labor expensive to collect pairs of videos with semantic coherence or correspondence, and label the corresponding segments. We first exploit and reorganize the videos in ActivityNet to form a new dataset for video re-localization research, which consists of about 10,000 videos of diverse visual appearances associated with the localized boundary information. Subsequently, we propose an innovative cross gated bilinear matching model such that every time-step in the reference video is matched against the attentively weighted query video. Consequently, the prediction of the starting and ending time is formulated as a classification problem based on the matching results. Extensive experimental results show that the proposed method outperforms the baseline methods. Our code is available at: https://github.com/fengyang0317/video_reloc.  2018, Springer Nature Switzerland AG.
KW  - Security systems
KW  - Computer vision
KW  - Semantics
KW  - Visualization
U2  - Bilinear matching
U2  - Boundary information
U2  - Cross gating
U2  - Enabling technologies
U2  - Re-localization
U2  - Video copy detection
U2  - Video surveillance
U2  - Visual appearance
DO  - 10.1007/978-3-030-01264-9_4
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_4
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Training binary weight networks via semi-binary decomposition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Hu, Qinghao
A1  - Li, Gang
A1  - Wang, Peisong
A1  - Zhang, Yifan
A1  - Cheng, Jian
AD  - National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, ChinaUniversity of Chinese Academy of Sciences, Beijing, ChinaCenter for Excellence in Brain Science and Intelligence Technology, Beijing, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008960
SP  - 657
EP  - 673
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recently binary weight networks have attracted lots of attentions due to their high computational efficiency and small parameter size. Yet they still suffer from large accuracy drops because of their limited representation capacity. In this paper, we propose a novel semi-binary decomposition method which decomposes a matrix into two binary matrices and a diagonal matrix. Since the matrix product of binary matrices has more numerical values than binary matrix, the proposed semi-binary decomposition has more representation capacity. Besides, we propose an alternating optimization method to solve the semi-binary decomposition problem while keeping binary constraints. Extensive experiments on AlexNet, ResNet-18, and ResNet-50 demonstrate that our method outperforms state-of-the-art methods by a large margin (5% higher in top1 accuracy). We also implement binary weight AlexNet on FPGA platform, which shows that our proposed method can achieve  9  speed-ups while reducing the consumption of on-chip memory and dedicated multipliers significantly.  Springer Nature Switzerland AG 2018.
KW  - Multiplying circuits
KW  - Computational efficiency
KW  - Computer vision
KW  - Deep neural networks
U2  - Alternating optimizations
U2  - Binary constraints
U2  - Binary decompositions
U2  - Deep networks
U2  - Diagonal matrices
U2  - Matrix products
U2  - Numerical values
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01261-8_39
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_39
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Transferable adversarial perturbations
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhou, Wen
A1  - Hou, Xin
A1  - Chen, Yongjun
A1  - Tang, Mengyun
A1  - Huang, Xiangqi
A1  - Gan, Xiang
A1  - Yang, Yong
AD  - Basic Research Group, Security Platform Department, Tencent, Beijing, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020315
SP  - 471
EP  - 486
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - State-of-the-art deep neural network classifiers are highly vulnerable to adversarial examples which are designed to mislead classifiers with a very small perturbation. However, the performance of black-box attacks (without knowledge of the model parameters) against deployed models always degrades significantly. In this paper, We propose a novel way of perturbations for adversarial examples to enable black-box transfer. We first show that maximizing distance between natural images and their adversarial examples in the intermediate feature maps can improve both white-box attacks (with knowledge of the model parameters) and black-box attacks. We also show that smooth regularization on adversarial perturbations enables transferring across models. Extensive experimental results show that our approach outperforms state-of-the-art methods both in white-box and black-box attacks.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Deep neural networks
KW  - Image enhancement
U2  - Adversarial perturbations
U2  - Black boxes
U2  - Model parameters
U2  - Neural network classifier
U2  - Small perturbations
U2  - State of the art
U2  - State-of-the-art methods
U2  - Transferability
DO  - 10.1007/978-3-030-01264-9_28
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_28
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Pyramid Dilated Deeper ConvLSTM for Video Salient Object Detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Song, Hongmei
A1  - Wang, Wenguan
A1  - Zhao, Sanyuan
A1  - Shen, Jianbing
A1  - Lam, Kin-Man
AD  - Beijing Lab of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, ChinaInception Institute of Artificial Intelligence, Abu Dhabi, United Arab EmiratesThe Hong Kong Polytechnic University, Kowloon, Hong Kong
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978902
SP  - 744
EP  - 760
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper proposes a fast video salient object detection model, based on a novel recurrent network architecture, named Pyramid Dilated Bidirectional ConvLSTM (PDB-ConvLSTM). A Pyramid Dilated Convolution (PDC) module is first designed for simultaneously extracting spatial features at multiple scales. These spatial features are then concatenated and fed into an extended Deeper Bidirectional ConvLSTM (DB-ConvLSTM) to learn spatiotemporal information. Forward and backward ConvLSTM units are placed in two layers and connected in a cascaded way, encouraging information flow between the bi-directional streams and leading to deeper feature extraction. We further augment DB-ConvLSTM with a PDC-like structure, by adopting several dilated DB-ConvLSTMs to extract multi-scale spatiotemporal information. Extensive experimental results show that our method outperforms previous video saliency models in a large margin, with a real-time speed of 20 fps on a single GPU. With unsupervised video object segmentation as an example application, the proposed model (with a CRF-based post-process) achieves state-of-the-art results on two popular benchmarks, well demonstrating its superior performance and high applicability.  2018, Springer Nature Switzerland AG.
KW  - Object detection
KW  - Benchmarking
KW  - Computer vision
KW  - Image segmentation
KW  - Network architecture
KW  - Object recognition
U2  - Forward-and-backward
U2  - Information flows
U2  - Salient object detection
U2  - Spatial features
U2  - Spatiotemporal information
U2  - State of the art
U2  - Video saliencies
U2  - Video-object segmentation
DO  - 10.1007/978-3-030-01252-6_44
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_44
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Monocular Depth Estimation with Affinity, Vertical Pooling, and Label Enhancement
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Gan, Yukang
A1  - Xu, Xiangyu
A1  - Sun, Wenxiu
A1  - Lin, Liang
AD  - SenseTime, Beijing, ChinaSun Yat-sen University, Guangzhou, ChinaTsinghua University, Beijing, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977801
SP  - 232
EP  - 247
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Significant progress has been made in monocular depth estimation with Convolutional Neural Networks (CNNs). While absolute features, such as edges and textures, could be effectively extracted, the depth constraint of neighboring pixels, namely relative features, has been mostly ignored by recent CNN-based methods. To overcome this limitation, we explicitly model the relationships of different image locations with an affinity layer and combine absolute and relative features in an end-to-end network. In addition, we consider prior knowledge that major depth changes lie in the vertical direction, and thus, it is beneficial to capture long-range vertical features for refined depth estimation. In the proposed algorithm we introduce vertical pooling to aggregate image features vertically to improve the depth accuracy. Furthermore, since the Lidar depth ground truth is quite sparse, we enhance the depth labels by generating high-quality dense depth maps with off-the-shelf stereo matching method taking left-right image pairs as input. We also integrate multi-scale structure in our network to obtain global understanding of the image depth and exploit residual learning to help depth refinement. We demonstrate that the proposed algorithm performs favorably against state-of-the-art methods both qualitatively and quantitatively on the KITTI driving dataset.  2018, Springer Nature Switzerland AG.
KW  - Stereo image processing
KW  - Computer vision
KW  - Image enhancement
KW  - Neural networks
U2  - Affinity
U2  - Convolutional neural network
U2  - End-to-end network
U2  - Monocular depth
U2  - Multi-scale structures
U2  - State-of-the-art methods
U2  - Stereo matching method
U2  - Vertical direction
DO  - 10.1007/978-3-030-01219-9_14
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_14
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Pose-normalized image generation for person re-identification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Qian, Xuelin
A1  - Fu, Yanwei
A1  - Xiang, Tao
A1  - Wang, Wenxuan
A1  - Qiu, Jie
A1  - Wu, Yang
A1  - Jiang, Yu-Gang
A1  - Xue, Xiangyang
AD  - Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, ChinaSchool of Data Science, Fudan University, Shanghai, ChinaTencent AI Lab, Bellevue, United StatesQueen Mary University of London, London, United KingdomNara Institute of Science and Technology, Ikoma, Japan
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977514
SP  - 661
EP  - 678
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Person Re-identification (re-id) faces two major challenges: the lack of cross-view paired training data and learning discriminative identity-sensitive and view-invariant features in the presence of large pose variations. In this work, we address both problems by proposing a novel deep person image generation model for synthesizing realistic person images conditional on the pose. The model is based on a generative adversarial network (GAN) designed specifically for pose normalization in re-id, thus termed pose-normalization GAN (PN-GAN). With the synthesized images, we can learn a new type of deep re-id features free of the influence of pose variations. We show that these features are complementary to features learned with the original images. Importantly, a more realistic unsupervised learning setting is considered in this work, and our model is shown to have the potential to be generalizable to a new re-id dataset without any fine-tuning. The codes will be released at https://github.com/naiq/PN_GAN.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Artificial intelligence
KW  - Computer science
KW  - Computers
U2  - Adversarial networks
U2  - Image generations
U2  - Normalized image
U2  - Original images
U2  - Person re identifications
U2  - Person re-id
U2  - Pose normalization
U2  - Synthesized images
DO  - 10.1007/978-3-030-01240-3_40
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_40
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Joint task-recursive learning for semantic segmentation and depth estimation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Zhenyu
A1  - Cui, Zhen
A1  - Xu, Chunyan
A1  - Jie, Zequn
A1  - Li, Xiang
A1  - Yang, Jian
AD  - PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, ChinaTencent AI Lab, Shenzhen, China
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978770
SP  - 238
EP  - 255
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a novel joint Task-Recursive Learning (TRL) framework for the closing-loop semantic segmentation and monocular depth estimation tasks. TRL can recursively refine the results of both tasks through serialized task-level interactions. In order to mutually-boost for each other, we encapsulate the interaction into a specific Task-Attentional Module (TAM) to adaptively enhance some counterpart patterns of both tasks. Further, to make the inference more credible, we propagate previous learning experiences on both tasks into the next network evolution by explicitly concatenating previous responses. The sequence of task-level interactions are finally evolved along a coarse-to-fine scale space such that the required details may be reconstructed progressively. Extensive experiments on NYU-Depth v2 and SUN RGB-D datasets demonstrate that our method achieves state-of-the-art results for monocular depth estimation and semantic segmentation.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Computer vision
KW  - Recurrent neural networks
KW  - Semantics
U2  - Coarse to fine
U2  - Depth Estimation
U2  - Learning experiences
U2  - Network evolution
U2  - Recursive learning
U2  - Semantic segmentation
U2  - Specific tasks
U2  - State of the art
DO  - 10.1007/978-3-030-01249-6_15
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_15
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - CurriculumNet: Weakly supervised learning from large-scale web images
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Guo, Sheng
A1  - Huang, Weilin
A1  - Zhang, Haozhi
A1  - Zhuang, Chenfan
A1  - Dong, Dengke
A1  - Scott, Matthew R.
A1  - Huang, Dinglong
AD  - Malong Technologies, Shenzhen, ChinaShenzhen Malong Artificial Intelligence Research Center, Shenzhen, China
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978730
SP  - 139
EP  - 154
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present a simple yet efficient approach capable of training deep neural networks on large-scale weakly-supervised web images, which are crawled raw from the Internet by using text queries, without any human annotation. We develop a principled learning strategy by leveraging curriculum learning, with the goal of handling a massive amount of noisy labels and data imbalance effectively. We design a new learning curriculum by measuring the complexity of data using its distribution density in a feature space, and rank the complexity in an unsupervised manner. This allows for an efficient implementation of curriculum learning on large-scale web images, resulting in a high-performance CNN the model, where the negative impact of noisy labels is reduced substantially. Importantly, we show by experiments that those images with highly noisy labels can surprisingly improve the generalization capability of model, by serving as a manner of regularization. Our approaches obtain state-of-the-art performance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101. With an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on the WebVision challenge [18] for 1000-category classification. This result was the top performance by a wide margin, outperforming second place by a nearly 50% relative error rate. Code and models are available at: https://github.com/MalongTech/CurriculumNet.  Springer Nature Switzerland AG 2018.
KW  - Image enhancement
KW  - Benchmarking
KW  - Complex networks
KW  - Computer vision
KW  - Curricula
KW  - Deep neural networks
KW  - HTTP
U2  - Efficient implementation
U2  - Generalization capability
U2  - Large-scale
U2  - Noisy data
U2  - State-of-the-art performance
U2  - Weakly supervised
U2  - Weakly supervised learning
U2  - Web images
DO  - 10.1007/978-3-030-01249-6_9
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_9
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Mancs: A Multi-task Attentional Network with Curriculum Sampling for Person Re-Identification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Cheng
A1  - Zhang, Qian
A1  - Huang, Chang
A1  - Liu, Wenyu
A1  - Wang, Xinggang
AD  - School of EIC, Huazhong University of Science and Technology, Wuhan, ChinaHorizon Robotics Inc., Beijing, China
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004573
SP  - 384
EP  - 400
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We propose a novel deep network called Mancs that solves the person re-identification problem from the following aspects: fully utilizing the attention mechanism for the person misalignment problem and properly sampling for the ranking loss to obtain more stable person representation. Technically, we contribute a novel fully attentional block which is deeply supervised and can be plugged into any CNN, and a novel curriculum sampling method which is effective for training ranking losses. The learning tasks are integrated into a unified framework and jointly optimized. Experiments have been carried out on Market1501, CUHK03 and DukeMTMC. All the results show that Mancs can significantly outperform the previous state-of-the-arts. In addition, the effectiveness of the newly proposed ideas has been confirmed by extensive ablation studies.  2018, Springer Nature Switzerland AG.
KW  - Curricula
KW  - Computer vision
U2  - Attention
U2  - Attention mechanisms
U2  - Multitask learning
U2  - Person re identifications
U2  - Person re-ID
U2  - Sampling method
U2  - State of the art
U2  - Unified framework
DO  - 10.1007/978-3-030-01225-0_23
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_23
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Escaping from collapsing modes in a constrained space
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chang, Chia-Che
A1  - Lin, Chieh Hubert
A1  - Lee, Che-Rung
A1  - Juan, Da-Cheng
A1  - Wei, Wei
A1  - Chen, Hwann-Tzong
AD  - Department of Computer Science, National Tsing Hua University, Hsinchu, TaiwanGoogle AI, Mountain View; CA, United States
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977588
SP  - 212
EP  - 227
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Generative adversarial networks (GANs) often suffer from unpredictable mode-collapsing during training. We study the issue of mode collapse of Boundary Equilibrium Generative Adversarial Network (BEGAN), which is one of the state-of-the-art generative models. Despite its potential of generating high-quality images, we find that BEGAN tends to collapse at some modes after a period of training. We propose a new model, called BEGAN with a Constrained Space (BEGAN-CS), which includes a latent-space constraint in the loss function. We show that BEGAN-CS can significantly improve training stability and suppress mode collapse without either increasing the model complexity or degrading the image quality. Further, we visualize the distribution of latent vectors to elucidate the effect of latent-space constraint. The experimental results show that our method has additional advantages of being able to train on small datasets and to generate images similar to a given real image yet with variations of designated attributes on-the-fly.  Springer Nature Switzerland AG 2018.
KW  - Image enhancement
KW  - Computer vision
KW  - Vector spaces
U2  - Adversarial networks
U2  - Boundary equilibrium
U2  - Constrained space
U2  - Generative model
U2  - High quality images
U2  - Model complexity
U2  - Space constraints
U2  - State of the art
DO  - 10.1007/978-3-030-01234-2_13
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_13
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Leveraging motion priors in videos for improving human segmentation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Yu-Ting
A1  - Chang, Wen-Yen
A1  - Lu, Hai-Lun
A1  - Wu, Tingfan
A1  - Sun, Min
AD  - National Tsing Hua University, TaiwanUmbo Computer Vision, Taiwan
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977589
SP  - 228
EP  - 244
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Despite many advances in deep-learning based semantic segmentation, performance drop due to distribution mismatch is often encountered in the real world. Recently, a few domain adaptation and active learning approaches have been proposed to mitigate the performance drop. However, very little attention has been made toward leveraging information in videos which are naturally captured in most camera systems. In this work, we propose to leverage motion prior in videos for improving human segmentation in a weakly-supervised active learning setting. By extracting motion information using optical flow in videos, we can extract candidate foreground motion segments (referred to as motion prior) potentially corresponding to human segments. We propose to learn a memory-network-based policy model to select strong candidate segments (referred to as strong motion prior) through reinforcement learning. The selected segments have high precision and are directly used to finetune the model. In a newly collected surveillance camera dataset and a publicly available UrbanStreet dataset, our proposed method improves the performance of human segmentation across multiple scenes and modalities (i.e., RGB to Infrared (IR)). Last but not least, our method is empirically complementary to existing domain adaptation approaches such that additional performance gain is achieved by combining our weakly-supervised active learning approach with domain adaptation approaches.  Springer Nature Switzerland AG 2018.
KW  - Security systems
KW  - Artificial intelligence
KW  - Cameras
KW  - Computer vision
KW  - Deep learning
KW  - Drops
KW  - Reinforcement learning
KW  - Semantics
U2  - Active Learning
U2  - Domain adaptation
U2  - Existing domains
U2  - Foreground motions
U2  - Human segmentation
U2  - Motion information
U2  - Semantic segmentation
U2  - Surveillance cameras
DO  - 10.1007/978-3-030-01234-2_14
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_14
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning to dodge a bullet: Concyclic view morphing via deep learning
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Jin, Shi
A1  - Liu, Ruiynag
A1  - Ji, Yu
A1  - Ye, Jinwei
A1  - Yu, Jingyi
AD  - ShanghaiTech University, Shanghai, ChinaPlex-VR, Baton Rouge; LA, United StatesLouisiana State University, Baton Rouge; LA, United States
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020300
SP  - 230
EP  - 246
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The bullet-time effect, presented in feature film The Matrix, has been widely adopted in feature films and TV commercials to create an amazing stopping-time illusion. Producing such visual effects, however, typically requires using a large number of cameras/images surrounding the subject. In this paper, we present a learning-based solution that is capable of producing the bullet-time effect from only a small set of images. Specifically, we present a view morphing framework that can synthesize smooth and realistic transitions along a circular view path using as few as three reference images. We apply a novel cyclic rectification technique to align the reference images onto a common circle and then feed the rectified results into a deep network to predict its motion field and per-pixel visibility for new view interpolation. Comprehensive experiments on synthetic and real data show that our new framework outperforms the state-of-the-art and provides an inexpensive and practical solution for producing the bullet-time effects.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Computer vision
KW  - Motion pictures
KW  - Neural networks
U2  - Bullet time
U2  - Convolutional Neural Networks (CNN)
U2  - Image based rendering
U2  - Practical solutions
U2  - State of the art
U2  - Synthetic and real data
U2  - View interpolation
U2  - View morphing
DO  - 10.1007/978-3-030-01264-9_14
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_14
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Video Object Segmentation by Learning Location-Sensitive Embeddings
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Ci, Hai
A1  - Wang, Chunyu
A1  - Wang, Yizhou
AD  - EECS, Peking University, Beijing, ChinaMicrosoft Research Asia, Beijing, ChinaDeepwise AI Lab, Beijing, China
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978888
SP  - 524
EP  - 539
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We address the problem of video object segmentation which outputs the masks of a target object throughout a video given only a bounding box in the first frame. There are two main challenges to this task. First, the background may contain similar objects as the target. Second, the appearance of the target object may change drastically over time. To tackle these challenges, we propose an end-to-end training network which accomplishes foreground predictions by leveraging the location-sensitive embeddings which are capable to distinguish the pixels of similar objects. To deal with appearance changes, for a test video, we propose a robust model adaptation method which pre-scans the whole video, generates pseudo foreground/background labels and retrains the model based on the labels. Our method outperforms the state-of-the-art methods on the DAVIS and the SegTrack v2 datasets.  2018, Springer Nature Switzerland AG.
KW  - Image segmentation
KW  - Computer vision
KW  - Location
KW  - Motion compensation
U2  - Embeddings
U2  - Foreground/background
U2  - Model-based OPC
U2  - Robust modeling
U2  - State-of-the-art methods
U2  - Target object
U2  - Training network
U2  - Video-object segmentation
DO  - 10.1007/978-3-030-01252-6_31
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_31
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Hierarchical Bilinear Pooling for Fine-Grained Visual Recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yu, Chaojian
A1  - Zhao, Xinyi
A1  - Zheng, Qi
A1  - Zhang, Peng
A1  - You, Xinge
AD  - School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978942
SP  - 595
EP  - 610
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Fine-grained visual recognition is challenging because it highly relies on the modeling of various semantic parts and fine-grained feature learning. Bilinear pooling based models have been shown to be effective at fine-grained recognition, while most previous approaches neglect the fact that inter-layer part feature interaction and fine-grained feature learning are mutually correlated and can reinforce each other. In this paper, we present a novel model to address these issues. First, a cross-layer bilinear pooling approach is proposed to capture the inter-layer part feature relations, which results in superior performance compared with other bilinear pooling based approaches. Second, we propose a novel hierarchical bilinear pooling framework to integrate multiple cross-layer bilinear features to enhance their representation capability. Our formulation is intuitive, efficient and achieves state-of-the-art results on the widely used fine-grained recognition datasets.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Semantics
U2  - Cross-layer interaction
U2  - Feature interactions
U2  - Feature learning
U2  - Fine grained
U2  - Hierarchical bilinear pooling
U2  - Semantic parts
U2  - State of the art
U2  - Visual recognition
DO  - 10.1007/978-3-030-01270-0_35
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_35
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Single image water hazard detection using FCN with reflection attention units
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Han, Xiaofeng
A1  - Nguyen, Chuong
A1  - You, Shaodi
A1  - Lu, Jianfeng
AD  - Nanjing University of Science and Technology, Nanjing; Jiangsu; 210094, ChinaAustralian National University, Canberra; ACT; 2600, AustraliaCSIRO DATA61, Canberra; ACT; 2601, AustraliaAustralian Centre of Excellence for Robotic Vision, 2 George Street, Brisbane; 4001, Australia
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977425
SP  - 105
EP  - 121
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Water bodies, such as puddles and flooded areas, on and off road pose significant risks to autonomous cars. Detecting water from moving camera is a challenging task as water surface is highly refractive, and its appearance varies with viewing angle, surrounding scene, weather conditions. In this paper, we present a water puddle detection method based on a Fully Convolutional Network (FCN) with our newly proposed Reflection Attention Units (RAUs). An RAU is a deep network unit designed to embody the physics of reflection on water surface from sky and nearby scene. To verify the performance of our proposed method, we collect 11455 color stereo images with polarizers, and 985 of left images are annotated and divided into 2 datasets: On Road (ONR) dataset and Off Road (OFR) dataset. We show that FCN-8s with RAUs improves significantly precision and recall metrics as compared to FCN-8s, DeepLab V2 and Gaussian Mixture Model (GMM). We also show that focal loss function can improve the performance of FCN-8s network due to the extreme imbalance of water versus ground classification problem.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Convolution
KW  - Deep learning
KW  - Gaussian distribution
KW  - Hazards
KW  - Off road vehicles
KW  - Roads and streets
KW  - Stereo image processing
KW  - Surface waters
U2  - Color stereo images
U2  - Convolutional networks
U2  - Detection methods
U2  - Gaussian Mixture Model
U2  - Hazard detection
U2  - Precision and recall
U2  - Road hazards
U2  - Water puddle
DO  - 10.1007/978-3-030-01231-1_7
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_7
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - SRDA: Generating instance segmentation annotation via scanning, reasoning and domain adaptation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Xu, Wenqiang
A1  - Li, Yonglu
A1  - Lu, Cewu
AD  - Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China
VL  - 11216 LNCS
PY  - 2018
U1  - 20184305977475
SP  - 124
EP  - 140
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Instance segmentation is a problem of significance in computer vision. However, preparing annotated data for this task is extremely time-consuming and costly. By combining the advantages of 3D scanning, reasoning, and GAN-based domain adaptation techniques, we introduce a novel pipeline named SRDA to obtain large quantities of training samples with very minor effort. Our pipeline is well-suited to scenes that can be scanned, i.e. most indoor and some outdoor scenarios. To evaluate our performance, we build three representative scenes and a new dataset, with 3D models of various common objects categories and annotated real-world scene images. Extensive experiments show that our pipeline can achieve decent instance segmentation performance given very low human labor cost.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Pipelines
KW  - Scanning
KW  - Wages
U2  - 3D-scanning
U2  - Domain adaptation
U2  - Human labor
U2  - Physical reasoning
U2  - Real-world
U2  - Scene image
U2  - Segmentation performance
U2  - Training sample
DO  - 10.1007/978-3-030-01258-8_8
L2  - http://dx.doi.org/10.1007/978-3-030-01258-8_8
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Dynamic conditional networks for few-shot learning
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhao, Fang
A1  - Zhao, Jian
A1  - Yan, Shuicheng
A1  - Feng, Jiashi
AD  - National University of Singapore, Singapore, SingaporeNational University of Defense Technology, Hunan, ChinaQihoo 360 AI Institute, Beijing, China
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006168
SP  - 20
EP  - 36
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper proposes a novel Dynamic Conditional Convolutional Network (DCCN) to handle conditional few-shot learning, i.e, only a few training samples are available for each condition. DCCN consists of dual subnets: DyConvNet contains a dynamic convolutional layer with a bank of basis filters; CondiNet predicts a set of adaptive weights from conditional inputs to linearly combine the basis filters. In this manner, a specific convolutional kernel can be dynamically obtained for each conditional input. The filter bank is shared between all conditions thus only a low-dimension weight vector needs to be learned. This significantly facilitates the parameter learning across different conditions when training data are limited. We evaluate DCCN on four tasks which can be formulated as conditional model learning, including specific object counting, multi-modal image classification, phrase grounding and identity based face generation. Extensive experiments demonstrate the superiority of the proposed model in the conditional few-shot learning setting.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Computer vision
KW  - Convolution
KW  - Filter banks
U2  - Adaptive weights
U2  - Conditional models
U2  - Convolutional kernel
U2  - Convolutional networks
U2  - Few-shot learning
U2  - Learning settings
U2  - Parameter learning
U2  - Training sample
DO  - 10.1007/978-3-030-01267-0_2
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_2
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yin, Guojun
A1  - Sheng, Lu
A1  - Liu, Bin
A1  - Yu, Nenghai
A1  - Wang, Xiaogang
A1  - Shao, Jing
A1  - Loy, Chen Change
AD  - Key Laboratory of Electromagnetic Space Information, University of Science and Technology of China, The Chinese Academy of Sciences, Hefei, ChinaThe Chinese University of Hong Kong, Shatin, ChinaSenseTime Research, Beijing, ChinaNanyang Technological University, Singapore, Singapore
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977808
SP  - 330
EP  - 347
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recognizing visual relationships subject-predicate-object among any pair of localized objects is pivotal for image understanding. Previous studies have shown remarkable progress in exploiting linguistic priors or external textual information to improve the performance. In this work, we investigate an orthogonal perspective based on feature interactions. We show that by encouraging deep message propagation and interactions between local object features and global predicate features, one can achieve compelling performance in recognizing complex relationships without using any linguistic priors. To this end, we present two new pooling cells to encourage feature interactions: (i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that inversely pools local object features to the corresponding area of global predicate features. (ii) Pyramid ROI Pooling Cell, which broadcasts global predicate features to reinforce local object features. The two cells constitute a Spatiality-Context-Appearance Module (SCA-M), which can be further stacked consecutively to form our final Zoom-Net. We further shed light on how one could resolve ambiguous and noisy object and predicate annotations by Intra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual Genome dataset demonstrate the effectiveness of our feature-oriented approach compared to state-of-the-art methods (Acc@1 11.42% from 8.16%) that depend on explicit modeling of linguistic interactions. We further show that SCA-M can be incorporated seamlessly into existing approaches to improve the performance by a large margin.  2018, Springer Nature Switzerland AG.
KW  - Cells
KW  - Computer vision
KW  - Cytology
KW  - Forestry
KW  - Linguistics
U2  - Complex relationships
U2  - Explicit modeling
U2  - Feature interactions
U2  - Feature-oriented approaches
U2  - Hierarchical tree
U2  - Message propagation
U2  - State-of-the-art methods
U2  - Textual information
DO  - 10.1007/978-3-030-01219-9_20
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_20
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Attention-GAN for Object Transfiguration in Wild Images
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Xinyuan
A1  - Xu, Chang
A1  - Yang, Xiaokang
A1  - Tao, Dacheng
AD  - MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, ChinaCentre for Artificial Intelligence, SIT, FEIT, University of Technology Sydney, Sydney, AustraliaUBTECH Sydney AI Centre, SIT, FEIT, University of Sydney, Sydney, Australia
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005957
SP  - 167
EP  - 184
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper studies the object transfiguration problem in wild images. The generative network in classical GANs for object transfiguration often undertakes a dual responsibility: to detect the objects of interests and to convert the object from source domainto another domain. In contrast, we decompose the generative network into two separated networks, each of which is only dedicated to one particular sub-task. The attention network predicts spatial attention maps of images, and the transformation network focuses on translating objects. Attention maps produced by attention network are encouraged to be sparse, so that major attention can be paid on objects of interests. No matter before or after object transfiguration, attention maps should remain constant. In addition, learning attention network can receive more instructions, given the available segmentation annotations of images. Experimental results demonstrate the necessity of investigating attention in object transfiguration, and that the proposed algorithm can learn accurate attention to improve quality of generated images.  2018, Springer Nature Switzerland AG.
KW  - Image enhancement
KW  - Computer vision
KW  - Image segmentation
KW  - Object detection
U2  - Adversarial networks
U2  - Attention mechanisms
U2  - Spatial attention
U2  - Subtasks
DO  - 10.1007/978-3-030-01216-8_11
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_11
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - End-to-End View Synthesis for Light Field Imaging with Pseudo 4DCNN
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Yunlong
A1  - Liu, Fei
A1  - Wang, Zilei
A1  - Hou, Guangqi
A1  - Sun, Zhenan
A1  - Tan, Tieniu
AD  - University of Science and Technology of China, Hefei, ChinaCenter for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005968
SP  - 340
EP  - 355
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Limited angular resolution has become the main bottleneck of microlens-based plenoptic cameras towards practical vision applications. Existing view synthesis methods mainly break the task into two steps, i.e. depth estimating and view warping, which are usually inefficient and produce artifacts over depth ambiguities. In this paper, an end-to-end deep learning framework is proposed to solve these problems by exploring Pseudo 4DCNN. Specifically, 2D strided convolutions operated on stacked EPIs and detail-restoration 3D CNNs connected with angular conversion are assembled to build the Pseudo 4DCNN. The key advantage is to efficiently synthesize dense 4D light fields from a sparse set of input views. The learning framework is well formulated as an entirely trainable problem, and all the weights can be recursively updated with standard backpropagation. The proposed framework is compared with state-of-the-art approaches on both genuine and synthetic light field databases, which achieves significant improvements of both image quality (+2dB higher) and computational efficiency (over 10X faster). Furthermore, the proposed framework shows good performances in real-world applications such as biometrics and depth estimation.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Backpropagation
KW  - Computational efficiency
KW  - Deep learning
KW  - Image enhancement
U2  - Angular resolution
U2  - End to end
U2  - Learning frameworks
U2  - Light fields
U2  - Pseudo 4DCNN
U2  - State-of-the-art approach
U2  - View synthesis
U2  - Vision applications
DO  - 10.1007/978-3-030-01216-8_21
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_21
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - BSN: Boundary Sensitive Network for Temporal Action Proposal Generation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Lin, Tianwei
A1  - Zhao, Xu
A1  - Su, Haisheng
A1  - Wang, Chongjing
A1  - Yang, Ming
AD  - Department of Automation, Shanghai Jiao Tong University, Shanghai, ChinaChina Academy of Information and Communications Technology, Beijing, China
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004558
SP  - 3
EP  - 21
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover truth action instances with high recall and high overlap using relatively fewer proposals. To address these difficulties, we introduce an effective proposal generation method, named Boundary-Sensitive Network (BSN), which adopts local to global fashion. Locally, BSN first locates temporal boundaries with high probabilities, then directly combines these boundaries as proposals. Globally, with Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the confidence of whether a proposal contains an action within its region. We conduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14, where BSN outperforms other state-of-the-art temporal action proposal generation methods with high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classifiers, our method significantly improves the state-of-the-art temporal action detection performance.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Artificial intelligence
KW  - Computer science
KW  - Computers
U2  - Action classifier
U2  - Detection performance
U2  - Generation method
U2  - Real world videos
U2  - State of the art
U2  - Temporal action proposal generation
U2  - Temporal precision
U2  - Untrimmed video
DO  - 10.1007/978-3-030-01225-0_1
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_1
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Pairwise body-part attention for recognizing human-object interactions
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Fang, Hao-Shu
A1  - Cao, Jinkun
A1  - Tai, Yu-Wing
A1  - Lu, Cewu
AD  - Shanghai Jiao Tong University, Shanghai, ChinaTencent YouTu Lab, Shanghai, China
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978797
SP  - 52
EP  - 68
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In human-object interactions (HOI) recognition, conventional methods consider the human body as a whole and pay a uniform attention to the entire body region. They ignore the fact that normally, human interacts with an object by using some parts of the body. In this paper, we argue that different body parts should be paid with different attention in HOI recognition, and the correlations between different body parts should be further considered. This is because our body parts always work collaboratively. We propose a new pairwise body-part attention model which can learn to focus on crucial parts, and their correlations for HOI recognition. A novel attention based feature selection method and a feature representation scheme that can capture pairwise correlations between body parts are introduced in the model. Our proposed approach achieved 10% relative improvement (36.1 mAP  39.9 mAP) over the state-of-the-art results in HOI recognition on the HICO dataset. We will make our model and source codes publicly available.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Artificial intelligence
KW  - Computer science
KW  - Computers
U2  - Attention model
U2  - Body parts
U2  - Conventional methods
U2  - Feature representation
U2  - Feature selection methods
U2  - Human-object interaction
U2  - Pairwise correlation
U2  - State of the art
DO  - 10.1007/978-3-030-01249-6_4
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_4
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep High Dynamic Range Imaging with Large Foreground Motions
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wu, Shangzhe
A1  - Xu, Jiarui
A1  - Tai, Yu-Wing
A1  - Tang, Chi-Keung
AD  - The Hong Kong University of Science and Technology, Kowloon, Hong KongTencent Youtu, Shanghai, ChinaUniversity of Oxford, Oxford, United Kingdom
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005999
SP  - 120
EP  - 135
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper proposes the first non-flow-based deep framework for high dynamic range (HDR) imaging of dynamic scenes with large-scale foreground motions. In state-of-the-art deep HDR imaging, input images are first aligned using optical flows before merging, which are still error-prone due to occlusion and large motions. In stark contrast to flow-based methods, we formulate HDR imaging as an image translation problem without optical flows. Moreover, our simple translation network can automatically hallucinate plausible HDR details in the presence of total occlusion, saturation and under-exposure, which are otherwise almost impossible to recover by conventional optimization approaches. Our framework can also be extended for different reference images. We performed extensive qualitative and quantitative comparisons to show that our approach produces excellent results where color artifacts and geometric distortions are significantly reduced compared to existing state-of-the-art methods, and is robust across various inputs, including images without radiometric calibration.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Color photography
KW  - Optical flows
U2  - Computational photography
U2  - Conventional optimization
U2  - Foreground motions
U2  - Geometric distortion
U2  - High dynamic range imaging
U2  - Quantitative comparison
U2  - Radiometric calibrations
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01216-8_8
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_8
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Attention-aware deep adversarial hashing for cross-modal retrieval
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Xi
A1  - Lai, Hanjiang
A1  - Feng, Jiashi
AD  - School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, ChinaGuangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, ChinaDepartment of Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006186
SP  - 614
EP  - 629
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Due to the rapid growth of multi-modal data, hashing methods for cross-modal retrieval have received considerable attention. However, finding content similarities between different modalities of data is still challenging due to an existing heterogeneity gap. To further address this problem, we propose an adversarial hashing network with an attention mechanism to enhance the measurement of content similarities by selectively focusing on the informative parts of multi-modal data. The proposed new deep adversarial network consists of three building blocks: (1) the feature learning module to obtain the feature representations; (2) the attention module to generate an attention mask, which is used to divide the feature representations into the attended and unattended feature representations; and (3) the hashing module to learn hash functions that preserve the similarities between different modalities. In our framework, the attention and hashing modules are trained in an adversarial way: the attention module attempts to make the hashing module unable to preserve the similarities of multi-modal data w.r.t. the unattended feature representations, while the hashing module aims to preserve the similarities of multi-modal data w.r.t. the attended and unattended feature representations. Extensive evaluations on several benchmark datasets demonstrate that the proposed method brings substantial improvements over other state-of-the-art cross-modal hashing methods.  Springer Nature Switzerland AG 2018.
KW  - Historic preservation
KW  - Computer vision
KW  - Hash functions
KW  - Modal analysis
U2  - Adversarial learning
U2  - Adversarial networks
U2  - Attention mechanisms
U2  - Benchmark datasets
U2  - Content similarity
U2  - Cross-modal
U2  - Feature representation
U2  - Hashing
DO  - 10.1007/978-3-030-01267-0_36
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_36
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - DetNet: Design backbone for object detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Zeming
A1  - Peng, Chao
A1  - Yu, Gang
A1  - Zhang, Xiangyu
A1  - Deng, Yangdong
A1  - Sun, Jian
AD  - School of Software, Tsinghua University, Beijing, ChinaMegvii Inc. (Face++), Beijing, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977493
SP  - 339
EP  - 354
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recent CNN based object detectors, either one-stage methods like YOLO, SSD, and RetinaNet, or two-stage detectors like Faster R-CNN, R-FCN and FPN, are usually trying to directly finetune from ImageNet pre-trained models designed for the task of image classification. However, there has been little work discussing the backbone feature extractor specifically designed for the task of object detection. More importantly, there are several differences between the tasks of image classification and object detection. (i) Recent object detectors like FPN and RetinaNet usually involve extra stages against the task of image classification to handle the objects with various scales. (ii) Object detection not only needs to recognize the category of the object instances but also spatially locate them. Large downsampling factors bring large valid receptive field, which is good for image classification, but compromises the object location ability. Due to the gap between the image classification and object detection, we propose DetNet in this paper, which is a novel backbone network specifically designed for object detection. Moreover, DetNet includes the extra stages against traditional backbone network for image classification, while maintains high spatial resolution in deeper layers. Without any bells and whistles, state-of-the-art results have been obtained for both object detection and instance segmentation on the MSCOCO benchmark based on our DetNet (4.8G FLOPs) backbone. Codes will be released (https://github.com/zengarden/DetNet).  Springer Nature Switzerland AG 2018.
KW  - Object detection
KW  - Computer vision
KW  - Image classification
KW  - Neural networks
KW  - Object recognition
KW  - Signaling
U2  - Back-bone network
U2  - Convolutional neural network
U2  - Feature extractor
U2  - High spatial resolution
U2  - Object detectors
U2  - One-stage method
U2  - Receptive fields
U2  - State of the art
DO  - 10.1007/978-3-030-01240-3_21
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_21
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep domain generalization via conditional invariant adversarial networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Ya
A1  - Tian, Xinmei
A1  - Gong, Mingming
A1  - Liu, Yajing
A1  - Liu, Tongliang
A1  - Zhang, Kun
A1  - Tao, Dacheng
AD  - CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application Systems, University of Science and Technology of China, Hefei, ChinaDepartment of Philosophy, Carnegie Mellon University, Pittsburgh, United StatesDepartment of Biomedical Informatics, University of Pittsburgh, Pittsburgh, United StatesUBTECH Sydney AI Centre, SIT, FEIT, University of Sydney, Sydney, Australia
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006188
SP  - 647
EP  - 663
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Domain generalization aims to learn a classification model from multiple source domains and generalize it to unseen target domains. A critical problem in domain generalization involves learning domain-invariant representations. Let X and Y denote the features and the labels, respectively. Under the assumption that the conditional distribution P(Y|X) remains unchanged across domains, earlier approaches to domain generalization learned the invariant representation T(X) by minimizing the discrepancy of the marginal distribution P(T(X)). However, such an assumption of stable P(Y|X) does not necessarily hold in practice. In addition, the representation learning function T(X) is usually constrained to a simple linear transformation or shallow networks. To address the above two drawbacks, we propose an end-to-end conditional invariant deep domain generalization approach by leveraging deep neural networks for domain-invariant representation learning. The domain-invariance property is guaranteed through a conditional invariant adversarial network that can learn domain-invariant representations w.r.t. the joint distribution P(T(X), Y) if the target domain data are not severely class unbalanced. We perform various experiments to demonstrate the effectiveness of the proposed method.  Springer Nature Switzerland AG 2018.
KW  - Deep neural networks
KW  - Computer vision
KW  - Linear transformations
KW  - Mathematical transformations
U2  - Adversarial networks
U2  - Classification models
U2  - Conditional distribution
U2  - Domain generalization
U2  - Invariant representation
U2  - Joint distributions
U2  - Learning functions
U2  - Marginal distribution
DO  - 10.1007/978-3-030-01267-0_38
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_38
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Shift-net: Image inpainting via deep feature rearrangement
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yan, Zhaoyi
A1  - Li, Xiaoming
A1  - Li, Mu
A1  - Zuo, Wangmeng
A1  - Shan, Shiguang
AD  - School of Computer Science and Technology, Harbin Institute of Technology, Harbin, ChinaDepartment of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong KongInstitute of Computing Technology, CAS, Beijing; 100049, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020295
SP  - 3
EP  - 19
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Deep convolutional networks (CNNs) have exhibited their potential in image inpainting for producing plausible results. However, in most existing methods, e.g., context encoder, the missing parts are predicted by propagating the surrounding convolutional features through a fully connected layer, which intends to produce semantically plausible but blurry result. In this paper, we introduce a special shift-connection layer to the U-Net architecture, namely Shift-Net, for filling in missing regions of any shape with sharp structures and fine-detailed textures. To this end, the encoder feature of the known region is shifted to serve as an estimation of the missing parts. A guidance loss is introduced on decoder feature to minimize the distance between the decoder feature after fully connected layer and the ground-truth encoder feature of the missing parts. With such constraint, the decoder feature in missing region can be used to guide the shift of encoder feature in known region. An end-to-end learning algorithm is further developed to train the Shift-Net. Experiments on the Paris StreetView and Places datasets demonstrate the efficiency and effectiveness of our Shift-Net in producing sharper, fine-detailed, and visually plausible results. The codes and pre-trained models are available at https://github.com/Zhaoyi-Yan/Shift-Net.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Computer vision
KW  - Convolution
KW  - Decoding
KW  - Learning algorithms
KW  - Signal encoding
U2  - Convolutional networks
U2  - Feature rearrangement
U2  - Fully-connected layers
U2  - Ground truth
U2  - Image Inpainting
U2  - Inpainting
U2  - NET architecture
U2  - Sharp structure
DO  - 10.1007/978-3-030-01264-9_1
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_1
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - HairNet: Single-View Hair Reconstruction Using Convolutional Neural Networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhou, Yi
A1  - Hu, Liwen
A1  - Xing, Jun
A1  - Chen, Weikai
A1  - Kung, Han-Wei
A1  - Tong, Xin
A1  - Li, Hao
AD  - University of Southern California, Los Angeles, United StatesUSC Institute for Creative Technologies, Los Angeles, United StatesPinscreen, Santa Monica, United StatesMicrosoft Research Asia, Beijing, China
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978870
SP  - 249
EP  - 265
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We introduce a deep learning-based method to generate full 3D hair geometry from an unconstrained image. Our method can recover local strand details and has real-time performance. State-of-the-art hair modeling techniques rely on large hairstyle collections for nearest neighbor retrieval and then perform ad-hoc refinement. Our deep learning approach, in contrast, is highly efficient in storage and can run 1000 times faster while generating hair with 30K strands. The convolutional neural network takes the 2D orientation field of a hair image as input and generates strand features that are evenly distributed on the parameterized 2D scalp. We introduce a collision loss to synthesize more plausible hairstyles, and the visibility of each strand is also used as a weight term to improve the reconstruction accuracy. The encoder-decoder architecture of our network naturally provides a compact and continuous representation for hairstyles, which allows us to interpolate naturally between hairstyles. We use a large set of rendered synthetic hair models to train our network. Our method scales to real images because an intermediate 2D orientation field, automatically calculated from the real image, factors out the difference between synthetic and real hairs. We demonstrate the effectiveness and robustness of our method on a wide range of challenging real Internet pictures, and show reconstructed hair sequences from videos.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Computer vision
KW  - Convolution
KW  - Image reconstruction
KW  - Neural networks
U2  - Convolutional neural network
U2  - Encoder-decoder architecture
U2  - Hair
U2  - Learning-based methods
U2  - Orientation fields
U2  - Real time
U2  - Real time performance
U2  - Reconstruction accuracy
DO  - 10.1007/978-3-030-01252-6_15
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_15
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Museum Exhibit Identification Challenge for the Supervised Domain Adaptation and Beyond
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Koniusz, Piotr
A1  - Tas, Yusuf
A1  - Zhang, Hongguang
A1  - Harandi, Mehrtash
A1  - Porikli, Fatih
A1  - Zhang, Rui
AD  - Data61/CSIRO, Canberra, AustraliaAustralian National University, Canberra, AustraliaMonash University, Melbourne, AustraliaHubei University of Arts and Science, Xiangyang, China
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978956
SP  - 815
EP  - 833
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We study an open problem of artwork identification and propose a new dataset dubbed Open Museum Identification Challenge (Open MIC). It contains photos of exhibits captured in 10 distinct exhibition spaces of several museums which showcase paintings, timepieces, sculptures, glassware, relics, science exhibits, natural history pieces, ceramics, pottery, tools and indigenosus crafts. The goal of Open MIC is to stimulate research in domain adaptation, egocentric recognition and few-shot learning by providing a testbed complementary to the famous Office dataset which reaches ~90% accuracy. To form our dataset, we captured a number of images per art piece with a mobile phone and wearable cameras to form the source and target data splits, respectively. To achieve robust baselines, we build on a recent approach that aligns per-class scatter matrices of the source and target CNN streams. Moreover, we exploit the positive definite nature of such representations by using end-to-end Bregman divergences and the Riemannian metric. We present baselines such as training/evaluation per exhibition and training/evaluation on the combined set covering 866 exhibit identities. As each exhibition poses distinct challenges e.g., quality of lighting, motion blur, occlusions, clutter, viewpoint and scale variations, rotations, glares, transparency, non-planarity, clipping, we break down results w.r.t. these factors.  2018, Springer Nature Switzerland AG.
KW  - Exhibitions
KW  - Computer vision
KW  - Microwave integrated circuits
U2  - Bregman divergences
U2  - Domain adaptation
U2  - Museum exhibits
U2  - Natural history
U2  - Positive definite
U2  - Riemannian metrics
U2  - Scatter matrix
U2  - Wearable cameras
DO  - 10.1007/978-3-030-01270-0_48
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_48
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Extreme network compression via filter group approximation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Peng, Bo
A1  - Tan, Wenming
A1  - Li, Zheyang
A1  - Zhang, Shun
A1  - Xie, Di
A1  - Pu, Shiliang
AD  - Hikvision Research Institute, Hangzhou, China
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006053
SP  - 307
EP  - 323
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper we propose a novel decomposition method based on filter group approximation, which can significantly reduce the redundancy of deep convolutional neural networks (CNNs) while maintaining the majority of feature representation. Unlike other low-rank decomposition algorithms which operate on spatial or channel dimension of filters, our proposed method mainly focuses on exploiting the filter group structure for each layer. For several commonly used CNN models, including VGG and ResNet, our method can reduce over 80% floating-point operations (FLOPs) with less accuracy drop than state-of-the-art methods on various image classification datasets. Besides, experiments demonstrate that our method is conducive to alleviating degeneracy of the compressed network, which hurts the convergence and performance of the network.  Springer Nature Switzerland AG 2018.
KW  - Deep neural networks
KW  - Classification (of information)
KW  - Computer vision
KW  - Convolution
KW  - Digital arithmetic
KW  - Image classification
KW  - Neural networks
U2  - Classification datasets
U2  - Convolutional neural network
U2  - Deep convolutional neural networks
U2  - Feature representation
U2  - Floating point operations
U2  - Low-rank decomposition
U2  - Network compression
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01237-3_19
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_19
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Person search via a mask-guided two-stream CNN model
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Di
A1  - Zhang, Shanshan
A1  - Ouyang, Wanli
A1  - Yang, Jian
A1  - Tai, Ying
AD  - PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, ChinaSenseTime Computer Vision Research Group, The University of Sydney, Sydney; NSW, AustraliaYoutu Lab, Tencent, Shanghai, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977623
SP  - 764
EP  - 781
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this work, we tackle the problem of person search, which is a challenging task consisted of pedestrian detection and person re-identificationa(re-ID). Instead of sharing representations in a single joint model, we find that separating detector and re-ID feature extraction yields better performance. In order to extract more representative features for each identity, we propose a simple yet effective re-ID method, which models foreground person and original image patches individually, and obtains enriched representations from two separate CNN streams. On the standard person search benchmark datasets, we achieve mAP of 83.0 % and 32.6 % respectively for CUHK-SYSU and PRW, surpassing the state of the art by a large margin (more than 5 pp).  Springer Nature Switzerland AG 2018.
KW  - Feature extraction
KW  - Computer vision
U2  - Benchmark datasets
U2  - Foreground
U2  - Joint modeling
U2  - Original images
U2  - Pedestrian detection
U2  - Person re identifications
U2  - Person search
U2  - State of the art
DO  - 10.1007/978-3-030-01234-2_45
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_45
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Stereo computation for a single mixture image
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhong, Yiran
A1  - Dai, Yuchao
A1  - Li, Hongdong
AD  - Australian National University, Canberra, AustraliaNorthwestern Polytechnical University, Xian, ChinaData61 CSIRO, Canberra, AustraliaAustralian Centre for Robotic Vision, Canberra, Australia
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977499
SP  - 441
EP  - 456
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper proposes an original problem of stereo computation from a single mixture image  a challenging problem that had not been researched before. The goal is to separate (i.e., unmix) a single mixture image into two constitute image layers, such that the two layers form a left-right stereo image pair, from which a valid disparity map can be recovered. This is a severely illposed problem, from one input image one effectively aims to recover three (i.e., left image, right image and a disparity map). In this work we give a novel deep-learning based solution, by jointly solving the two subtasks of image layer separation as well as stereo matching. Training our deep net is a simple task, as it does not need to have disparity maps. Extensive experiments demonstrate the efficacy of our method.  Springer Nature Switzerland AG 2018.
KW  - Stereo image processing
KW  - Computer vision
KW  - Deep learning
KW  - Mixtures
KW  - Separation
KW  - Stereo vision
U2  - Anaglyph
U2  - Depth Estimation
U2  - Disparity map
U2  - Ill posed problem
U2  - Image layers
U2  - Image separation
U2  - Stereo image pairs
U2  - Stereo matching
DO  - 10.1007/978-3-030-01240-3_27
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_27
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Shufflenet V2: Practical guidelines for efficient cnn architecture design
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Ma, Ningning
A1  - Zhang, Xiangyu
A1  - Zheng, Hai-Tao
A1  - Sun, Jian
AD  - Megvii Inc (Face++), Beijing, ChinaTsinghua University, Beijing, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020342
SP  - 122
EP  - 138
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Currently, the neural network architecture design is mostly guided by the indirect metric of computation complexity, i.e., FLOPs. However, the direct metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical guidelines for efficient network design. Accordingly, a new architecture is presented, called ShuffleNet V2. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.  2018, Springer Nature Switzerland AG.
KW  - Network architecture
KW  - Computer vision
KW  - Efficiency
KW  - Memory architecture
U2  - Ablation experiments
U2  - Architecture designs
U2  - Computation complexity
U2  - Controlled experiment
U2  - Network design
U2  - Practical
U2  - Practical guidelines
U2  - State of the art
DO  - 10.1007/978-3-030-01264-9_8
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_8
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Incremental multi-graph matching via diversity and randomness based graph clustering
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yu, Tianshu
A1  - Yan, Junchi
A1  - Liu, Wei
A1  - Li, Baoxin
AD  - Arizona State University, Tempe, United StatesShanghai Jiao Tong University, Shanghai, ChinaTencent AI Lab, Shenzhen, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008450
SP  - 142
EP  - 158
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Multi-graph matching refers to finding correspondences across graphs, which are traditionally solved by matching all the graphs in a single batch. However in real-world applications, graphs are often collected incrementally, rather than once for all. In this paper, we present an incremental multi-graph matching approach, which deals with the arriving graph utilizing the previous matching results under the global consistency constraint. When a new graph arrives, rather than re-optimizing over all graphs, we propose to partition graphs into subsets with certain topological structure and conduct optimization within each subset. The partitioning procedure is guided by the diversity within partitions and randomness over iterations, and we present an interpretation showing why these two factors are essential. The final matching results are calculated over all subsets via an intersection graph. Extensive experimental results on synthetic and real image datasets show that our algorithm notably improves the efficiency without sacrificing the accuracy.  Springer Nature Switzerland AG 2018.
KW  - Topology
KW  - Computer vision
KW  - Graphic methods
KW  - Image enhancement
KW  - Pattern matching
KW  - Random processes
KW  - Structural optimization
U2  - Global consistency
U2  - Graph clustering
U2  - Graph matchings
U2  - Intersection graph
U2  - Point process
U2  - Real images
U2  - Real-world
U2  - Topological structure
DO  - 10.1007/978-3-030-01261-8_9
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_9
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Compositing-Aware Image Search
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhao, Hengshuang
A1  - Shen, Xiaohui
A1  - Lin, Zhe
A1  - Sunkavalli, Kalyan
A1  - Price, Brian
A1  - Jia, Jiaya
AD  - The Chinese University of Hong Kong, Shatin, Hong KongByteDance AI Lab, Menlo Park, United StatesAdobe Research, San Jose, United StatesTencent Youtu Lab, Shenzhen, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977820
SP  - 517
EP  - 532
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present a new image search technique that, given a background image, returns compatible foreground objects for image compositing tasks. The compatibility of a foreground object and a background scene depends on various aspects such as semantics, surrounding context, geometry, style and color. However, existing image search techniques measure the similarities on only a few aspects, and may return many results that are not suitable for compositing. Moreover, the importance of each factor may vary for different object categories and image content, making it difficult to manually define the matching criteria. In this paper, we propose to learn feature representations for foreground objects and background scenes respectively, where image content and object category information are jointly encoded during training. As a result, the learned features can adaptively encode the most important compatibility factors. We project the features to a common embedding space, so that the compatibility scores can be easily measured using the cosine similarity, enabling very efficient search. We collect an evaluation set consisting of eight object categories commonly used in compositing tasks, on which we demonstrate that our approach significantly outperforms other search techniques.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Information analysis
KW  - Semantics
U2  - Background image
U2  - Background scenes
U2  - Cosine similarity
U2  - Feature representation
U2  - Foreground objects
U2  - Image compositing
U2  - Matching criterion
U2  - Object categories
DO  - 10.1007/978-3-030-01219-9_31
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_31
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Less is more: Picking informative frames for video captioning
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Yangyu
A1  - Wang, Shuhui
A1  - Zhang, Weigang
A1  - Huang, Qingming
AD  - University of Chinese Academy of Science, Beijing; 100049, ChinaKey Laboratory of Intelligent Information Processing, Institute of Computing Technology, CAS, Beijing; 100190, ChinaHarbin Institute of Technology, Weihai; 264200, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008942
SP  - 367
EP  - 384
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In video captioning task, the best practice has been achieved by attention-based models which associate salient visual components with sentences in the video. However, existing study follows a common procedure which includes a frame-level appearance modeling and motion modeling on equal interval frame sampling, which may bring about redundant visual information, sensitivity to content noise and unnecessary computation cost. We propose a plug-and-play PickNet to perform informative frame picking in video captioning. Based on a standard encoder-decoder framework, we develop a reinforcement-learning-based procedure to train the network sequentially, where the reward of each frame picking action is designed by maximizing visual diversity and minimizing discrepancy between generated caption and the ground-truth. The rewarded candidate will be selected and the corresponding latent representation of encoder-decoder will be updated for future trials. This procedure goes on until the end of the video sequence. Consequently, a compact frame subset can be selected to represent the visual information and perform video captioning without performance degradation. Experiment results show that our model can achieve competitive performance across popular benchmarks while only 68 frames are used.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Benchmarking
KW  - Decoding
KW  - Reinforcement learning
KW  - Signal encoding
U2  - Appearance modeling
U2  - Competitive performance
U2  - Computation costs
U2  - Equal intervals
U2  - Performance degradation
U2  - Video sequences
U2  - Visual components
U2  - Visual information
DO  - 10.1007/978-3-030-01261-8_22
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_22
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Is robustness the cost of accuracy? – A comprehensive study on the robustness of 18 deep image classification models
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Su, Dong
A1  - Zhang, Huan
A1  - Chen, Hongge
A1  - Yi, Jinfeng
A1  - Chen, Pin-Yu
A1  - Gao, Yupeng
AD  - IBM Research, New York, United StatesUniversity of California, Davis, Davis, United StatesMassachusetts Institute of Technology, Cambridge, United StatesJD AI Research, Beijing, China
VL  - 11216 LNCS
PY  - 2018
U1  - 20184305977463
SP  - 644
EP  - 661
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The prediction accuracy has been the long-lasting and sole standard for comparing the performance of different image classification models, including the ImageNet competition. However, recent studies have highlighted the lack of robustness in well-trained deep neural networks to adversarial examples. Visually imperceptible perturbations to natural images can easily be crafted and mislead the image classifiers towards misclassification. To demystify the trade-offs between robustness and accuracy, in this paper we thoroughly benchmark 18 ImageNet models using multiple robustness metrics, including the distortion, success rate and transferability of adversarial examples between 306 pairs of models. Our extensive experimental results reveal several new insights: (1) linear scaling law - the empirical 2 and  distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to robustness than model size, and the disclosed accuracy-robustness Pareto frontier can be used as an evaluation criterion for ImageNet model designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in  distortion; (4) there exist models (in VGG family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family. Experiment code is publicly available at https://github.com/huanzhang12/Adversarial_Survey.  Springer Nature Switzerland AG 2018.
KW  - Image classification
KW  - Computer vision
KW  - Deep neural networks
KW  - Economic and social effects
KW  - Image enhancement
KW  - Network architecture
KW  - Robustness (control systems)
U2  - Adversarial attacks
U2  - Classification errors
U2  - Classification models
U2  - Distortion metrics
U2  - Evaluation criteria
U2  - Misclassifications
U2  - Model architecture
U2  - Prediction accuracy
DO  - 10.1007/978-3-030-01258-8_39
L2  - http://dx.doi.org/10.1007/978-3-030-01258-8_39
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Person Search in Videos with One Portrait Through Visual and Temporal Links
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Huang, Qingqiu
A1  - Liu, Wentao
A1  - Lin, Dahua
AD  - CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Shatin, Hong KongDepartment of Computer Science and Technology, Tsinghua University, Beijing, ChinaSenseTime Research, Beijing, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008946
SP  - 437
EP  - 454
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In real-world applications, e.g. law enforcement and video retrieval, one often needs to search a certain person in long videos with just one portrait. This is much more challenging than the conventional settings for person re-identification, as the search may need to be carried out in the environments different from where the portrait was taken. In this paper, we aim to tackle this challenge and propose a novel framework, which takes into account the identity invariance along a tracklet, thus allowing person identities to be propagated via both the visual and the temporal links. We also develop a novel scheme called Progressive Propagation via Competitive Consensus, which significantly improves the reliability of the propagation process. To promote the study of person search, we construct a large-scale benchmark, which contains 127K manually annotated tracklets from 192 movies. Experiments show that our approach remarkably outperforms mainstream person re-id methods, raising the mAP from 42.16 % to 62.27 % (Code at https://github.com/hqqasw/person-search-PPCC ).  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Artificial intelligence
KW  - Computer science
KW  - Computers
U2  - Competitive Consensus
U2  - Person re identifications
U2  - Person search
U2  - Portrait
U2  - Propagation process
U2  - Real-world
U2  - Video retrieval
U2  - Visual and temporal
DO  - 10.1007/978-3-030-01261-8_26
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_26
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Video Object Segmentation with Joint Re-identification and Attention-Aware Mask Propagation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Xiaoxiao
A1  - Loy, Chen Change
AD  - Department of Information Engineering, The Chinese University of Hong Kong, Hong KongNanyang Technological University, Singapore, Singapore
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977842
SP  - 93
EP  - 110
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The problem of video object segmentation can become extremely challenging when multiple instances co-exist. While each instance may exhibit large scale and pose variations, the problem is compounded when instances occlude each other causing failures in tracking. In this study, we formulate a deep recurrent network that is capable of segmenting and tracking objects in video simultaneously by their temporal continuity, yet able to re-identify them when they re-appear after a prolonged occlusion. We combine temporal propagation and re-identification functionalities into a single framework that can be trained end-to-end. In particular, we present a re-identification module with template expansion to retrieve missing objects despite their large appearance changes. In addition, we contribute an attention-based recurrent mask propagation approach that is robust to distractors not belonging to the target segment. Our approach achieves a new state-of-the-art G-mean of 68.2 on the challenging DAVIS 2017 benchmark (test-dev set), outperforming the winning solution. Project Page: http://mmlab.ie.cuhk.edu.hk/projects/DyeNet/.  2018, Springer Nature Switzerland AG.
KW  - Image segmentation
KW  - Computer vision
KW  - Motion compensation
U2  - Multiple instances
U2  - Re identifications
U2  - Recurrent networks
U2  - State of the art
U2  - Temporal continuity
U2  - Temporal propagation
U2  - Tracking objects
U2  - Video-object segmentation
DO  - 10.1007/978-3-030-01219-9_6
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_6
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - stagNet: An attentive semantic RNN for group activity recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Qi, Mengshi
A1  - Qin, Jie
A1  - Li, Annan
A1  - Wang, Yunhong
A1  - Luo, Jiebo
A1  - Van Gool, Luc
AD  - Beijing Advanced Innovation Center for Big Data and Brain Computing, School of Computer Science and Engineering, Beihang University, Beijing, ChinaComputer Vision Laboratory, ETH Zurich, Zurich, SwitzerlandInception Institute of Artificial Intelligence, Abu Dhabi, United Arab EmiratesDepartment of Computer Science, University of Rochester, Rochester, United States
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978728
SP  - 104
EP  - 120
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Group activity recognition plays a fundamental role in a variety of applications, e.g. sports video analysis and intelligent surveillance. How to model the spatio-temporal contextual information in a scene still remains a crucial yet challenging issue. We propose a novel attentive semantic recurrent neural network (RNN), dubbed as stagNet, for understanding group activities in videos, based on the spatio-temporal attention and semantic graph. A semantic graph is explicitly modeled to describe the spatial context of the whole scene, which is further integrated with the temporal factor via structural-RNN. Benefiting from the factor sharing and message passing mechanisms, our model is capable of extracting discriminative spatio-temporal features and capturing inter-group relationships. Moreover, we adopt a spatio-temporal attention model to attend to key persons/frames for improved performance. Two widely-used datasets are employed for performance evaluation, and the extensive results demonstrate the superiority of our method.  Springer Nature Switzerland AG 2018.
KW  - Semantics
KW  - Computer vision
KW  - Graphic methods
KW  - Message passing
KW  - Recurrent neural networks
KW  - Security systems
U2  - Group activity recognition
U2  - Intelligent surveillance
U2  - Performance evaluations
U2  - Recurrent neural network (RNN)
U2  - Scene understanding
U2  - Semantic graphs
U2  - Spatio temporal
U2  - Spatio temporal features
DO  - 10.1007/978-3-030-01249-6_7
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_7
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep Boosting for Image Denoising
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Chang
A1  - Xiong, Zhiwei
A1  - Tian, Xinmei
A1  - Wu, Feng
AD  - University of Science and Technology of China, Hefei, China
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978864
SP  - 3
EP  - 19
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Boosting is a classic algorithm which has been successfully applied to diverse computer vision tasks. In the scenario of image denoising, however, the existing boosting algorithms are surpassed by the emerging learning-based models. In this paper, we propose a novel deep boosting framework (DBF) for denoising, which integrates several convolutional networks in a feed-forward fashion. Along with the integrated networks, however, the depth of the boosting framework is substantially increased, which brings difficulty to training. To solve this problem, we introduce the concept of dense connection that overcomes the vanishing of gradients during training. Furthermore, we propose a path-widening fusion scheme cooperated with the dilated convolution to derive a lightweight yet efficient convolutional network as the boosting unit, named Dilated Dense Fusion Network (DDFN). Comprehensive experiments demonstrate that our DBF outperforms existing methods on widely used benchmarks, in terms of different denoising tasks.  2018, Springer Nature Switzerland AG.
KW  - Image denoising
KW  - Computer vision
KW  - Convolution
U2  - Boosting algorithm
U2  - Classic algorithm
U2  - Convolutional networks
U2  - De-noising
U2  - Feed forward
U2  - Integrated networks
U2  - Learning Based Models
DO  - 10.1007/978-3-030-01252-6_1
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_1
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Instance-Level Human Parsing via Part Grouping Network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Gong, Ke
A1  - Liang, Xiaodan
A1  - Li, Yicheng
A1  - Chen, Yimin
A1  - Yang, Ming
A1  - Lin, Liang
AD  - Sun Yat-sen University, Guangzhou, ChinaSenseTime Group (Limited), Beijing, ChinaCVTE Research, Guangzhou, China
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004599
SP  - 805
EP  - 822
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Instance-level human parsing towards real-world human analysis scenarios is still under-explored due to the absence of sufficient data resources and technical difficulty in parsing multiple instances in a single pass. Several related works all follow the parsing-by-detection pipeline that heavily relies on separately trained detection models to localize instances and then performs human parsing for each instance sequentially. Nonetheless, two discrepant optimization targets of detection and parsing lead to suboptimal representation learning and error accumulation for final results. In this work, we make the first attempt to explore a detection-free Part Grouping Network (PGN) for efficiently parsing multiple people in an image in a single pass. Our PGN reformulates instance-level human parsing as two twinned sub-tasks that can be jointly learned and mutually refined via a unified network: (1) semantic part segmentation for assigning each pixel as a human part (e.g., face, arms); (2) instance-aware edge detection to group semantic parts into distinct person instances. Thus the shared intermediate representation would be endowed with capabilities in both characterizing fine-grained parts and inferring instance belongings of each part. Finally, a simple instance partition process is employed to get final results during inference. We conducted experiments on PASCAL-Person-Part dataset and our PGN outperforms all state-of-the-art methods. Furthermore, we show its superiority on a newly collected multi-person parsing dataset (CIHP) including 38,280 diverse images, which is the largest dataset so far and can facilitate more advanced human analysis. The CIHP benchmark and our source code are available at http://sysu-hcp.net/lip/.  2018, Springer Nature Switzerland AG.
KW  - Chemical detection
KW  - Computer vision
KW  - Edge detection
KW  - Semantics
U2  - Error accumulation
U2  - Instance-level human parsing
U2  - Intermediate representations
U2  - Multiple instances
U2  - Part grouping
U2  - Semantic parts
U2  - State-of-the-art methods
U2  - Technical difficulties
DO  - 10.1007/978-3-030-01225-0_47
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_47
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Transductive Centroid Projection for Semi-supervised Large-Scale Recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Yu
A1  - Song, Guanglu
A1  - Shao, Jing
A1  - Jin, Xiao
A1  - Wang, Xiaogang
AD  - The Chinese University of Hong Kong, Shatin, Hong KongSensetime Group Limited, Beijing; 100084, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976887
SP  - 72
EP  - 89
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Conventional deep semi-supervised learning methods, such as recursive clustering and training process, suffer from cumulative error and high computational complexity when collaborating with Convolutional Neural Networks. To this end, we design a simple but effective learning mechanism that merely substitutes the last fully-connected layer with the proposed Transductive Centroid Projection (TCP) module. It is inspired by the observation of the weights in the final classification layer (called anchors) converge to the central direction of each class in hyperspace. Specifically, we design the TCP module by dynamically adding an ad hoc anchor for each cluster in one mini-batch. It essentially reduces the probability of the inter-class conflict and enables the unlabelled data functioning as labelled data. We inspect its effectiveness with elaborate ablation study on seven public face/person classification benchmarks. Without any bells and whistles, TCP can achieve significant performance gains over most state-of-the-art methods in both fully-supervised and semi-supervised manners.  2018, Springer Nature Switzerland AG.
KW  - Supervised learning
KW  - Computer vision
KW  - Deep learning
KW  - Face recognition
KW  - Neural networks
KW  - Signaling
KW  - Transmission control protocol
U2  - Convolutional neural network
U2  - Cumulative errors
U2  - Effective learning
U2  - Fully-connected layers
U2  - Person Re-ID
U2  - Semi- supervised learning
U2  - Semi-supervised learning methods
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01228-1_5
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_5
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Domain Transfer Through Deep Activation Matching
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Huang, Haoshuo
A1  - Huang, Qixing
A1  - Krahenbuhl, Philipp
AD  - Tsinghua University, Beijing, ChinaUniversity of Texas at Austin, Austin, United States
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978943
SP  - 611
EP  - 626
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We introduce a layer-wise unsupervised domain adaptation approach for semantic segmentation. Instead of merely matching the output distributions of the source and target domains, our approach aligns the distributions of activations of intermediate layers. This scheme exhibits two key advantages. First, matching across intermediate layers introduces more constraints for training the network in the target domain, making the optimization problem better conditioned. Second, the matched activations at each layer provide similar inputs to the next layer for both training and adaptation, and thus alleviate covariate shift. We use a Generative Adversarial Network (or GAN) to align activation distributions. Experimental results show that our approach achieves state-of-the-art results on a variety of popular domain adaptation tasks, including (1) from GTA to Cityscapes for semantic segmentation, (2) from SYNTHIA to Cityscapes for semantic segmentation, and (3) adaptations on USPS and MNIST for image classification (The website of this paper is https://rsents.github.io/dam.html ).  2018, Springer Nature Switzerland AG.
KW  - Chemical activation
KW  - Computer vision
KW  - Image classification
KW  - Image segmentation
KW  - Semantics
U2  - Cityscapes
U2  - Domain adaptation
U2  - Semantic segmentation
U2  - SYNTHIA
U2  - USPS and MNIST
DO  - 10.1007/978-3-030-01270-0_36
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_36
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Dynamic filtering with large sampling field for ConvNets
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wu, Jialin
A1  - Li, Dai
A1  - Yang, Yu
A1  - Bajaj, Chandrajit
A1  - Ji, Xiangyang
AD  - The Department of Automation, Tsinghua University, Beijing; 100084, ChinaThe University of Texas at Austin, Austin; TX; 78712, United States
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978767
SP  - 188
EP  - 203
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We propose a dynamic filtering strategy with large sampling field for ConvNets (LS-DFN), where the position-specific kernels learn from not only the identical position but also multiple sampled neighbour regions. During sampling, residual learning is introduced to ease training and an attention mechanism is applied to fuse features from different samples. Such multiple samples enlarge the kernels receptive fields significantly without requiring more parameters. While LS-DFN inherits the advantages of DFN [5], namely avoiding feature map blurring by positionwise kernels while keeping translation invariance, it also efficiently alleviates the overfitting issue caused by much more parameters than normal CNNs. Our model is efficient and can be trained end-to-end via standard back-propagation. We demonstrate the merits of our LS-DFN on both sparse and dense prediction tasks involving object detection, semantic segmentation and flow estimation. Our results show LS-DFN enjoys stronger recognition abilities in object detection and semantic segmentation tasks on VOC benchmark [8] and sharper responses in flow estimation on FlyingChairs dataset [6] compared to strong baselines.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Backpropagation
KW  - Object detection
KW  - Object recognition
KW  - Semantics
U2  - Attention mechanisms
U2  - Dynamic filtering
U2  - Flow estimation
U2  - Large sampling field
U2  - Multiple samples
U2  - Recognition abilities
U2  - Semantic segmentation
U2  - Translation invariance
DO  - 10.1007/978-3-030-01249-6_12
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_12
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Part-aligned bilinear representations for person re-identification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Suh, Yumin
A1  - Wang, Jingdong
A1  - Tang, Siyu
A1  - Mei, Tao
A1  - Lee, Kyoung Mu
AD  - ASRI, Seoul National University, Seoul, Korea, Republic ofMicrosoft Research Asia, Beijing, ChinaMax Planck Institute for Intelligent Systems, Tubingen, GermanyUniversity of Tubingen, Tubingen, GermanyJD AI Research, Beijing, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020312
SP  - 418
EP  - 437
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Comparing the appearance of corresponding body parts is essential for person re-identification. As body parts are frequently misaligned between the detected human boxes, an image representation that can handle this misalignment is required. In this paper, we propose a network that learns a part-aligned representation for person re-identification. Our model consists of a two-stream network, which generates appearance and body part feature maps respectively, and a bilinear-pooling layer that fuses two feature maps to an image descriptor. We show that it results in a compact descriptor, where the image matching similarity is equivalent to an aggregation of the local appearance similarities of the corresponding body parts. Since the image similarity does not depend on the relative positions of parts, our approach significantly reduces the part misalignment problem. Training the network does not require any part annotation on the person re-identification dataset. Instead, we simply initialize the part sub-stream using a pre-trained sub-network of an existing pose estimation network and train the whole network to minimize the re-identification loss. We validate the effectiveness of our approach by demonstrating its superiority over the state-of-the-art methods on the standard benchmark datasets including Market-1501, CUHK03, CUHK01 and DukeMTMC, and standard video dataset MARS.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Alignment
U2  - Appearance similarities
U2  - Bilinear pooling
U2  - Compact descriptor
U2  - Image representations
U2  - Person re identifications
U2  - Re identifications
U2  - Relative positions
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01264-9_25
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_25
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep Regionlets for Object Detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Xu, Hongyu
A1  - Lv, Xutao
A1  - Wang, Xiaoyu
A1  - Ren, Zhou
A1  - Bodla, Navaneeth
A1  - Chellappa, Rama
AD  - University of Maryland, College Park; MD, United StatesIntellifusion, Shenzhen, ChinaSnap Inc., Venice, Los Angeles, United States
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978907
SP  - 827
EP  - 844
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a novel object detection framework named Deep Regionlets by establishing a bridge between deep neural networks and conventional detection schema for accurate generic object detection. Motivated by the abilities of regionlets for modeling object deformation and multiple aspect ratios, we incorporate regionlets into an end-to-end trainable deep learning framework. The deep regionlets framework consists of a region selection network and a deep regionlet learning module. Specifically, given a detection bounding box proposal, the region selection network provides guidance on where to select regions to learn the features from. The regionlet learning module focuses on local feature selection and transformation to alleviate local variations. To this end, we first realize non-rectangular region selection within the detection framework to accommodate variations in object appearance. Moreover, we design a gating network within the regionlet leaning module to enable soft regionlet selection and pooling. The Deep Regionlets framework is trained end-to-end without additional efforts. We perform ablation studies and conduct extensive experiments on the PASCAL VOC and Microsoft COCO datasets. The proposed framework outperforms state-of-the-art algorithms, such as RetinaNet and Mask R-CNN, even without additional segmentation labels.  2018, Springer Nature Switzerland AG.
KW  - Object detection
KW  - Aspect ratio
KW  - Computer vision
KW  - Deep learning
KW  - Deep neural networks
KW  - Feature extraction
KW  - Image segmentation
KW  - Object recognition
U2  - Conventional detection
U2  - Deep Regionlets
U2  - Detection framework
U2  - Learning frameworks
U2  - Local feature selections
U2  - Rectangular regions
U2  - Spatial transformation
U2  - State-of-the-art algorithms
DO  - 10.1007/978-3-030-01252-6_49
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_49
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Grassmann Pooling as Compact Homogeneous Bilinear Pooling for Fine-Grained Visual Classification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wei, Xing
A1  - Zhang, Yue
A1  - Gong, Yihong
A1  - Zhang, Jiawei
A1  - Zheng, Nanning
AD  - Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xian, ChinaSenseTime Research, Shenzhen, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977810
SP  - 365
EP  - 380
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Designing discriminative and invariant features is the key to visual recognition. Recently, the bilinear pooled feature matrix of Convolutional Neural Network (CNN) has shown to achieve state-of-the-art performance on a range of fine-grained visual recognition tasks. The bilinear feature matrix collects second-order statistics and is closely related to the covariance matrix descriptor. However, the bilinear feature could suffer from the visual burstiness phenomenon similar to other visual representations such as VLAD and Fisher Vector. The reason is that the bilinear feature matrix is sensitive to the magnitudes and correlations of local CNN feature elements which can be measured by its singular values. On the other hand, the singular vectors are more invariant and reasonable to be adopted as the feature representation. Motivated by this point, we advocate an alternative pooling method which transforms the CNN feature matrix to an orthonormal matrix consists of its principal singular vectors. Geometrically, such orthonormal matrix lies on the Grassmann manifold, a Riemannian manifold whose points represent subspaces of the Euclidean space. Similarity measurement of images reduces to comparing the principal angles between these homogeneous subspaces and thus is independent of the magnitudes and correlations of local CNN activations. In particular, we demonstrate that the projection distance on the Grassmann manifold deduces a bilinear feature mapping without explicitly computing the bilinear feature matrix, which enables a very compact feature and classifier representation. Experimental results show that our method achieves an excellent balance of model complexity and accuracy on a variety of fine-grained image classification datasets.  2018, Springer Nature Switzerland AG.
KW  - Covariance matrix
KW  - Classification (of information)
KW  - Computer vision
KW  - Neural networks
KW  - Singular value decomposition
KW  - Vectors
U2  - Bilinear pooling
U2  - Burstiness
U2  - Convolutional Neural Networks (CNN)
U2  - Grassmann manifold
U2  - Second order statistics
U2  - Similarity measurements
U2  - State-of-the-art performance
U2  - Visual classification
DO  - 10.1007/978-3-030-01219-9_22
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_22
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep reinforcement learning with iterative shift for visual tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Ren, Liangliang
A1  - Yuan, Xin
A1  - Lu, Jiwen
A1  - Yang, Ming
A1  - Zhou, Jie
AD  - Tsinghua University, Beijing, ChinaHorizon Robotics, Inc., Beijing, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977516
SP  - 697
EP  - 713
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Visual tracking is confronted by the dilemma to locate a target both accurately and efficiently, and make decisions online whether and how to adapt the appearance model or even restart tracking. In this paper, we propose a deep reinforcement learning with iterative shift (DRL-IS) method for single object tracking, where an actor-critic network is introduced to predict the iterative shifts of object bounding boxes, and evaluate the shifts to take actions on whether to update object models or re-initialize tracking. Since locating an object is achieved by an iterative shift process, rather than online classification on many sampled locations, the proposed method is robust to cope with large deformations and abrupt motion, and computationally efficient since finding a target takes up to 10 shifts. In offline training, the critic network guides to learn how to make decisions jointly on motion estimation and tracking status in an end-to-end manner. Experimental results on the OTB benchmarks with large deformation improve the tracking precision by 1.7% and runs about 5 times faster than the competing state-of-the-art methods.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Computer vision
KW  - Deformation
KW  - Iterative methods
KW  - Motion estimation
KW  - Reinforcement learning
KW  - Tracking (position)
U2  - Actor-critic algorithm
U2  - Appearance modeling
U2  - Computationally efficient
U2  - Estimation and tracking
U2  - On-line classification
U2  - State-of-the-art methods
U2  - Tracking precision
U2  - Visual object tracking
DO  - 10.1007/978-3-030-01240-3_42
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_42
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep Video Quality Assessor: From Spatio-Temporal Visual Sensitivity to a Convolutional Neural Aggregation Network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Kim, Woojae
A1  - Kim, Jongyoo
A1  - Ahn, Sewoong
A1  - Kim, Jinwoo
A1  - Lee, Sanghoon
AD  - Department of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea, Republic ofMicrosoft Research, Beijing, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977691
SP  - 224
EP  - 241
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Incorporating spatio-temporal human visual perception into video quality assessment (VQA) remains a formidable issue. Previous statistical or computational models of spatio-temporal perception have limitations to be applied to the general VQA algorithms. In this paper, we propose a novel full-reference (FR) VQA framework named Deep Video Quality Assessor (DeepVQA) to quantify the spatio-temporal visual perception via a convolutional neural network (CNN) and a convolutional neural aggregation network (CNAN). Our framework enables to figure out the spatio-temporal sensitivity behavior through learning in accordance with the subjective score. In addition, to manipulate the temporal variation of distortions, we propose a novel temporal pooling method using an attention model. In the experiment, we show DeepVQA remarkably achieves the state-of-the-art prediction accuracy of more than 0.9 correlation, which is \sim 5% higher than those of conventional methods on the LIVE and CSIQ video databases.  2018, Springer Nature Switzerland AG.
KW  - Convolution
KW  - Computer vision
KW  - Neural networks
KW  - Vision
U2  - Attention mechanisms
U2  - Convolutional neural network
U2  - Temporal pooling
U2  - Video quality assessment
U2  - Visual sensitivity
DO  - 10.1007/978-3-030-01246-5_14
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_14
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Question-Guided Hybrid Convolution for Visual Question Answering
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Gao, Peng
A1  - Li, Hongsheng
A1  - Li, Shuang
A1  - Lu, Pan
A1  - Li, Yikang
A1  - Hoi, Steven C. H.
A1  - Wang, Xiaogang
AD  - CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Sha Tin, Hong KongSchool of Information Systems, Singapore Management Univeristy, Singapore, Singapore
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977707
SP  - 485
EP  - 501
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC) network for Visual Question Answering (VQA). Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural network and abandon the visual spatial information when learning multi-modal features. To address these problems, question-guided kernels generated from the input question are designed to convolute with visual features for capturing the textual and visual relationship in the early stage. The question-guided convolution can tightly couple the textual and visual information but also introduce more parameters when learning kernels. We apply the group convolution, which consists of question-independent kernels and question-dependent kernels, to reduce the parameter size and alleviate over-fitting. The hybrid convolution can generate discriminative multi-modal features with fewer parameters. The proposed approach is also complementary to existing bilinear pooling fusion and attention based VQA methods. By integrating with them, our method could further boost the performance. Experiments on VQA datasets validate the effectiveness of QGHC.  2018, Springer Nature Switzerland AG.
KW  - Convolution
KW  - Computer vision
U2  - Dynamic parameters
U2  - Learning kernels
U2  - Overfitting
U2  - Question Answering
U2  - State of the art
U2  - Visual feature
U2  - Visual information
U2  - Visual spatial
DO  - 10.1007/978-3-030-01246-5_29
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_29
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - BiSeNet: Bilateral segmentation network for real-time semantic segmentation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yu, Changqian
A1  - Wang, Jingbo
A1  - Peng, Chao
A1  - Gao, Changxin
A1  - Yu, Gang
A1  - Sang, Nong
AD  - National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, ChinaKey Laboratory of Machine Perception, Peking University, Beijing, ChinaMegvii Inc. (Face++), Beijing, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008940
SP  - 334
EP  - 349
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048  1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.  Springer Nature Switzerland AG 2018.
KW  - Semantic Web
KW  - Computer vision
KW  - Semantics
KW  - Statistical tests
U2  - Poor performance
U2  - Proposed architectures
U2  - Real-time inference
U2  - Real-time semantics
U2  - Segmentation performance
U2  - Semantic segmentation
U2  - Spatial informations
U2  - Spatial resolution
DO  - 10.1007/978-3-030-01261-8_20
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_20
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning Compression from Limited Unlabeled Data
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - He, Xiangyu
A1  - Cheng, Jian
AD  - National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, ChinaUniversity of Chinese Academy of Sciences, Beijing, ChinaCenter for Excellence in Brain Science and Intelligence Technology, Beijing, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977726
SP  - 778
EP  - 795
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Convolutional neural networks (CNNs) have dramatically advanced the state-of-art in a number of domains. However, most models are both computation and memory intensive, which arouse the interest of network compression. While existing compression methods achieve good performance, they suffer from three limitations: (1) the inevitable retraining with enormous labeled data; (2) the massive GPU hours for retraining; (3) the training tricks for model compression. Especially the requirement of retraining on original datasets makes it difficult to apply in many real-world scenarios, where training data is not publicly available. In this paper, we reveal that re-normalization is the practical and effective way to alleviate the above limitations. Through quantization or pruning, most methods may compress a large number of parameters but ignore the core role in performance degradation, which is the Gaussian conjugate prior induced by batch normalization. By employing the re-estimated statistics in batch normalization, we significantly improve the accuracy of compressed CNNs. Extensive experiments on ImageNet show it outperforms baselines by a large margin and is comparable to label-based methods. Besides, the fine-tuning process takes less than 5 min on CPU, using 1000 unlabeled images.  2018, Springer Nature Switzerland AG.
KW  - Deep neural networks
KW  - Computer vision
KW  - Neural networks
U2  - Compression methods
U2  - Conjugate prior
U2  - Convolutional neural network
U2  - Model compression
U2  - Network compression
U2  - Performance degradation
U2  - Real-world scenario
U2  - Unlabeled data
DO  - 10.1007/978-3-030-01246-5_46
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_46
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - How local is the local diversity? Reinforcing sequential determinantal point processes with dynamic ground sets for supervised video summarization
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Yandong
A1  - Wang, Liqiang
A1  - Yang, Tianbao
A1  - Gong, Boqing
AD  - University of Central Florida, Orlando; FL, United StatesJiulong Lake Campus, Southeast University, Nanjing; Jiangsu Province; 211189, ChinaUniversity of Iowa, Iowa City; IA, United StatesTencent AI Lab, Seattle; WA, United States
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006044
SP  - 156
EP  - 174
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The large volume of video content and high viewing frequency demand automatic video summarization algorithms, of which a key property is the capability of modeling diversity. If videos are lengthy like hours-long egocentric videos, it is necessary to track the temporal structures of the videos and enforce local diversity. The local diversity refers to that the shots selected from a short time duration are diverse but visually similar shots are allowed to co-exist in the summary if they appear far apart in the video. In this paper, we propose a novel probabilistic model, built upon SeqDPP, to dynamically control the time span of a video segment upon which the local diversity is imposed. In particular, we enable SeqDPP to learn to automatically infer how local the local diversity is supposed to be from the input video. The resulting model is extremely involved to train by the hallmark maximum likelihood estimation (MLE), which further suffers from the exposure bias and non-differentiable evaluation metrics. To tackle these problems, we instead devise a reinforcement learning algorithm for training the proposed model. Extensive experiments verify the advantages of our model and the new learning algorithm over MLE-based methods.  Springer Nature Switzerland AG 2018.
KW  - Learning algorithms
KW  - Computer vision
KW  - Maximum likelihood estimation
KW  - Reinforcement learning
KW  - Video recording
U2  - Evaluation metrics
U2  - Local diversities
U2  - Non-differentiable
U2  - Probabilistic modeling
U2  - Temporal structures
U2  - Video contents
U2  - Video segments
U2  - Video summarization
DO  - 10.1007/978-3-030-01237-3_10
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_10
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Efficient Uncertainty Estimation for Semantic Segmentation in Videos
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Huang, Po-Yu
A1  - Hsu, Wan-Ting
A1  - Chiu, Chun-Yueh
A1  - Wu, Ting-Fan
A1  - Sun, Min
AD  - National Tsing Hua University, Taipei, TaiwanUmbo Computer Vision, Taipei, Taiwan
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977711
SP  - 536
EP  - 552
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Uncertainty estimation in deep learning becomes more important recently. A deep learning model cant be applied in real applications if we dont know whether the model is certain about the decision or not. Some literature proposes the Bayesian neural network which can estimate the uncertainty by Monte Carlo Dropout (MC dropout). However, MC dropout needs to forward the model N times which results in N times slower. For real-time applications such as a self-driving car system, which needs to obtain the prediction and the uncertainty as fast as possible, so that MC dropout becomes impractical. In this work, we propose the region-based temporal aggregation (RTA) method which leverages the temporal information in videos to simulate the sampling procedure. Our RTA method with Tiramisu backbone is 10x faster than the MC dropout with Tiramisu backbone (N=5). Furthermore, the uncertainty estimation obtained by our RTA method is comparable to MC dropouts uncertainty estimation on pixel-level and frame-level metrics.  2018, Springer Nature Switzerland AG.
KW  - Uncertainty analysis
KW  - Computer vision
KW  - Deep learning
KW  - Image segmentation
KW  - Neural networks
KW  - Semantics
U2  - Bayesian neural networks
U2  - Efficient
U2  - Real-time application
U2  - Semantic segmentation
U2  - Temporal aggregation
U2  - Uncertainty
U2  - Uncertainty estimation
U2  - Video
DO  - 10.1007/978-3-030-01246-5_32
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_32
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - A Modulation Module for Multi-task Learning with Applications in Image Retrieval
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhao, Xiangyun
A1  - Li, Haoxiang
A1  - Shen, Xiaohui
A1  - Liang, Xiaodan
A1  - Wu, Ying
AD  - EECS Department, Northwestern University, Evanston, United StatesAIBee, Palo Alto, United StatesBytedance AI Lab, Beijing, ChinaCarnegie Mellon University, Pittsburgh, United States
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977703
SP  - 415
EP  - 432
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Multi-task learning has been widely adopted in many computer vision tasks to improve overall computation efficiency or boost the performance of individual tasks, under the assumption that those tasks are correlated and complementary to each other. However, the relationships between the tasks are complicated in practice, especially when the number of involved tasks scales up. When two tasks are of weak relevance, they may compete or even distract each other during joint training of shared parameters, and as a consequence undermine the learning of all the tasks. This will raise destructive interference which decreases learning efficiency of shared parameters and lead to low quality loss local optimum w.r.t. shared parameters. To address the this problem, we propose a general modulation module, which can be inserted into any convolutional neural network architecture, to encourage the coupling and feature sharing of relevant tasks while disentangling the learning of irrelevant tasks with minor parameters addition. Equipped with this module, gradient directions from different tasks can be enforced to be consistent for those shared parameters, which benefits multi-task joint training. The module is end-to-end learnable without ad-hoc design for specific tasks, and can naturally handle many tasks at the same time. We apply our approach on two retrieval tasks, face retrieval on the CelebA dataset [12] and product retrieval on the UT-Zappos50K dataset [34, 35], and demonstrate its advantage over other multi-task learning methods in both accuracy and storage efficiency.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Efficiency
KW  - Learning systems
KW  - Modulation
KW  - Network architecture
KW  - Neural networks
U2  - Computation efficiency
U2  - Convolutional neural network
U2  - Destructive interference
U2  - Feature Sharing
U2  - Gradient direction
U2  - Learning efficiency
U2  - Multitask learning
U2  - Storage efficiency
DO  - 10.1007/978-3-030-01246-5_25
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_25
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Hsu, Kuang-Jui
A1  - Tsai, Chung-Chi
A1  - Lin, Yen-Yu
A1  - Qian, Xiaoning
A1  - Chuang, Yung-Yu
AD  - Research Center for Information Technology Innovation, Academia Sinica, Taipei, TaiwanComputer Science and Information Engineering, National Taiwan University, Taipei, TaiwanElectrical and Computer Engineering, Texas AM University, College Station, United States
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976867
SP  - 502
EP  - 518
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we address co-saliency detection in a set of images jointly covering objects of a specific class by an unsupervised convolutional neural network (CNN). Our method does not require any additional training data in the form of object masks. We decompose co-saliency detection into two sub-tasks, single-image saliency detection and cross-image co-occurrence region discovery corresponding to two novel unsupervised losses, the single-image saliency (SIS) loss and the co-occurrence (COOC) loss. The two losses are modeled on a graphical model where the former and the latter act as the unary and pairwise terms, respectively. These two tasks can be jointly optimized for generating co-saliency maps of high quality. Furthermore, the quality of the generated co-saliency maps can be enhanced via two extensions: map sharpening by self-paced learning and boundary preserving by fully connected conditional random fields. Experiments show that our method achieves superior results, even outperforming many supervised methods.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Computer vision
KW  - Convolution
KW  - Graphic methods
KW  - Image segmentation
KW  - Neural networks
KW  - Unsupervised learning
U2  - Co saliencies
U2  - Conditional random field
U2  - Convolutional neural network
U2  - Convolutional Neural Networks (CNN)
U2  - GraphicaL model
U2  - Self-paced learning
U2  - Specific class
U2  - Supervised methods
DO  - 10.1007/978-3-030-01228-1_30
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_30
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Unsupervised image-to-image translation with stacked cycle-consistent adversarial networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Minjun
A1  - Huang, Haozhi
A1  - Ma, Lin
A1  - Liu, Wei
A1  - Zhang, Tong
A1  - Jiang, Yugang
AD  - Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, ChinaTencent AI Lab, Bellevue, United States
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977483
SP  - 186
EP  - 201
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recent studies on unsupervised image-to-image translation have made remarkable progress by training a pair of generative adversarial networks with a cycle-consistent loss. However, such unsupervised methods may generate inferior results when the image resolution is high or the two image domains are of significant appearance differences, such as the translations between semantic layouts and natural images in the Cityscapes dataset. In this paper, we propose novel Stacked Cycle-Consistent Adversarial Networks (SCANs) by decomposing a single translation into multi-stage transformations, which not only boost the image translation quality but also enable higher resolution image-to-image translation in a coarse-to-fine fashion. Moreover, to properly exploit the information from the previous stage, an adaptive fusion block is devised to learn a dynamic integration of the current stages output and the previous stages output. Experiments on multiple datasets demonstrate that our proposed approach can improve the translation quality compared with previous single-stage unsupervised methods.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Image resolution
KW  - Semantics
KW  - Unsupervised learning
U2  - Adversarial networks
U2  - Genearative adverserial network (GAN)
U2  - Higher resolution images
U2  - Image translation
U2  - Multiple data sets
U2  - Multistage transformation
U2  - Translation quality
U2  - Unsupervised method
DO  - 10.1007/978-3-030-01240-3_12
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_12
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Si, Chenyang
A1  - Jing, Ya
A1  - Wang, Wei
A1  - Wang, Liang
A1  - Tan, Tieniu
AD  - Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Beijing, ChinaCenter for Excellence in Brain Science and Intelligence Technology (CEBSIT), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, ChinaUniversity of Chinese Academy of Sciences (UCAS), Beijing, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977732
SP  - 106
EP  - 121
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Skeleton-based action recognition has made great progress recently, but many problems still remain unsolved. For example, the representations of skeleton sequences captured by most of the previous methods lack spatial structure information and detailed temporal dynamics features. In this paper, we propose a novel model with spatial reasoning and temporal stack learning (SR-TSL) for skeleton-based action recognition, which consists of a spatial reasoning network (SRN) and a temporal stack learning network (TSLN). The SRN can capture the high-level spatial structural information within each frame by a residual graph neural network, while the TSLN can model the detailed temporal dynamics of skeleton sequences by a composition of multiple skip-clip LSTMs. During training, we propose a clip-based incremental loss to optimize the model. We perform extensive experiments on the SYSU 3D Human-Object Interaction dataset and NTU RGB+D dataset and verify the effectiveness of each network of our model. The comparison results illustrate that our approach achieves much better results than the state-of-the-art methods.  2018, Springer Nature Switzerland AG.
KW  - Musculoskeletal system
KW  - Computer vision
U2  - Action recognition
U2  - Graph neural networks
U2  - Human-object interaction
U2  - Spatial reasoning
U2  - Spatial structural information
U2  - Spatial structure information
U2  - State-of-the-art methods
U2  - Temporal stack learning
DO  - 10.1007/978-3-030-01246-5_7
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_7
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning efficient single-stage pedestrian detectors by asymptotic localization fitting
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Wei
A1  - Liao, Shengcai
A1  - Hu, Weidong
A1  - Liang, Xuezhi
A1  - Chen, Xiao
AD  - Center for Biometrics and Security Research and National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, ChinaUniversity of Chinese Academy of Sciences, Beijing, ChinaNational University of Defense Technology, Changsha, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020326
SP  - 643
EP  - 659
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Though Faster R-CNN based two-stage detectors have witnessed significant boost in pedestrian detection accuracy, it is still slow for practical applications. One solution is to simplify this working flow as a single-stage detector. However, current single-stage detectors (e.g. SSD) have not presented competitive accuracy on common pedestrian detection benchmarks. This paper is towards a successful pedestrian detector enjoying the speed of SSD while maintaining the accuracy of Faster R-CNN. Specifically, a structurally simple but effective module called Asymptotic Localization Fitting (ALF) is proposed, which stacks a series of predictors to directly evolve the default anchor boxes of SSD step by step into improving detection results. As a result, during training the latter predictors enjoy more and better-quality positive samples, meanwhile harder negatives could be mined with increasing IoU thresholds. On top of this, an efficient single-stage pedestrian detection architecture (denoted as ALFNet) is designed, achieving state-of-the-art performance on CityPersons and Caltech, two of the largest pedestrian detection benchmarks, and hence resulting in an attractive pedestrian detector in both accuracy and speed. Code is available at https://github.com/VideoObjectSearch/ALFNet.  2018, Springer Nature Switzerland AG.
KW  - Benchmarking
KW  - Computer vision
KW  - Neural networks
U2  - Anchor-box
U2  - Asymptotic localization fitting
U2  - Caltech
U2  - Convolutional neural network
U2  - Pedestrian detection
U2  - Single stage
U2  - State-of-the-art performance
U2  - Two-stage
DO  - 10.1007/978-3-030-01264-9_38
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_38
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Stereo Vision-Based Semantic 3D Object and Ego-Motion Tracking for Autonomous Driving
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Peiliang
A1  - Qin, Tong
A1  - Shen, Shaojie
AD  - Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005989
SP  - 664
EP  - 679
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We propose a stereo vision-based approach for tracking the camera ego-motion and 3D semantic objects in dynamic autonomous driving scenarios. Instead of directly regressing the 3D bounding box using end-to-end approaches, we propose to use the easy-to-labeled 2D detection and discrete viewpoint classification together with a light-weight semantic inference method to obtain rough 3D object measurements. Based on the object-aware-aided camera pose tracking which is robust in dynamic environments, in combination with our novel dynamic object bundle adjustment (BA) approach to fuse temporal sparse feature correspondences and the semantic 3D measurement model, we obtain 3D object pose, velocity and anchored dynamic point cloud estimation with instance accuracy and temporal consistency. The performance of our proposed method is demonstrated in diverse scenarios. Both the ego-motion estimation and object localization are compared with the state-of-of-the-art solutions.  2018, Springer Nature Switzerland AG.
KW  - Stereo vision
KW  - Cameras
KW  - Computer vision
KW  - Motion estimation
KW  - Object recognition
KW  - Semantics
KW  - Stereo image processing
U2  - 3D object
U2  - Dynamic environments
U2  - Ego-motion estimation
U2  - Object localization
U2  - Temporal consistency
U2  - Vision-based approaches
U2  - Vision-based semantics
U2  - Visual odometry
DO  - 10.1007/978-3-030-01216-8_40
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_40
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep Cross-Modal Projection Learning for Image-Text Matching
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Ying
A1  - Lu, Huchuan
AD  - Dalian University of Technology, Dalian, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977722
SP  - 707
EP  - 723
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The key point of image-text matching is how to accurately measure the similarity between visual and textual inputs. Despite the great progress of associating the deep cross-modal embeddings with the bi-directional ranking loss, developing the strategies for mining useful triplets and selecting appropriate margins remains a challenge in real applications. In this paper, we propose a cross-modal projection matching (CMPM) loss and a cross-modal projection classification (CMPC) loss for learning discriminative image-text embeddings. The CMPM loss minimizes the KL divergence between the projection compatibility distributions and the normalized matching distributions defined with all the positive and negative samples in a mini-batch. The CMPC loss attempts to categorize the vector projection of representations from one modality onto another with the improved norm-softmax loss, for further enhancing the feature compactness of each class. Extensive analysis and experiments on multiple datasets demonstrate the superiority of the proposed approach.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Computer vision
KW  - Text processing
U2  - Bi-directional
U2  - Cross-modal
U2  - Image texts
U2  - Multiple data sets
U2  - Negative samples
U2  - Projection matching
U2  - Real applications
U2  - Vector projection
DO  - 10.1007/978-3-030-01246-5_42
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_42
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Contour knowledge transfer for salient object detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Xin
A1  - Yang, Fan
A1  - Cheng, Hong
A1  - Liu, Wei
A1  - Shen, Dinggang
AD  - University of Electronic Science and Technology of China, Chengdu; 611731, ChinaDepartment of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill; NC; 27599, United States
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006171
SP  - 370
EP  - 385
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In recent years, deep Convolutional Neural Networks (CNNs) have broken all records in salient object detection. However, training such a deep model requires a large amount of manual annotations. Our goal is to overcome this limitation by automatically converting an existing deep contour detection model into a salient object detection model without using any manual salient object masks. For this purpose, we have created a deep network architecture, namely Contour-to-Saliency Network (C2S-Net), by grafting a new branch onto a well-trained contour detection network. Therefore, our C2S-Net has two branches for performing two different tasks: (1) predicting contours with the original contour branch, and (2) estimating per-pixel saliency score of each image with the newly-added saliency branch. To bridge the gap between these two tasks, we further propose a contour-to-saliency transferring method to automatically generate salient object masks which can be used to train the saliency branch from outputs of the contour branch. Finally, we introduce a novel alternating training pipeline to gradually update the network parameters. In this scheme, the contour branch generates saliency masks for training the saliency branch, while the saliency branch, in turn, feeds back saliency knowledge in the form of saliency-aware contour labels, for fine-tuning the contour branch. The proposed method achieves state-of-the-art performance on five well-known benchmarks, outperforming existing fully supervised methods while also maintaining high efficiency.  Springer Nature Switzerland AG 2018.
KW  - Object detection
KW  - Benchmarking
KW  - Computer vision
KW  - Deep learning
KW  - Deep neural networks
KW  - Knowledge management
KW  - Network architecture
KW  - Neural networks
KW  - Object recognition
U2  - Deep convolutional neural networks
U2  - Knowledge transfer
U2  - Network parameters
U2  - Saliency detection
U2  - Salient object detection
U2  - State-of-the-art performance
U2  - Transfer learning
U2  - Transferring method
DO  - 10.1007/978-3-030-01267-0_22
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_22
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Modeling varying camera-imu time offset in optimization-based visual-inertial odometry
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Ling, Yonggen
A1  - Bao, Linchao
A1  - Jie, Zequn
A1  - Zhu, Fengming
A1  - Li, Ziyang
A1  - Tang, Shanmin
A1  - Liu, Yongsheng
A1  - Liu, Wei
A1  - Zhang, Tong
AD  - Tencent AI Lab, Shenzhen, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977503
SP  - 491
EP  - 507
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Combining cameras and inertial measurement units (IMUs) has been proven effective in motion tracking, as these two sensing modalities offer complementary characteristics that are suitable for fusion. While most works focus on global-shutter cameras and synchronized sensor measurements, consumer-grade devices are mostly equipped with rolling-shutter cameras and suffer from imperfect sensor synchronization. In this work, we propose a nonlinear optimization-based monocular visual inertial odometry (VIO) with varying camera-IMU time offset modeled as an unknown variable. Our approach is able to handle the rolling-shutter effects and imperfect sensor synchronization in a unified way. Additionally, we introduce an efficient algorithm based on dynamic programming and red-black tree to speed up IMU integration over variable-length time intervals during the optimization. An uncertainty-aware initialization is also presented to launch the VIO robustly. Comparisons with state-of-the-art methods on the Euroc dataset and mobile phone data are shown to validate the effectiveness of our approach.  Springer Nature Switzerland AG 2018.
KW  - Dynamic programming
KW  - Cameras
KW  - Computer vision
KW  - Nonlinear programming
KW  - Synchronization
KW  - Trees (mathematics)
U2  - Complementary characteristics
U2  - Inertial measurement unit
U2  - Mobile phone datum
U2  - Non-linear optimization
U2  - Odometry
U2  - Rolling shutter cameras
U2  - Sensor measurements
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01240-3_30
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_30
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deblurring Natural Image Using Super-Gaussian Fields
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Yuhang
A1  - Dong, Wenyong
A1  - Gong, Dong
A1  - Zhang, Lei
A1  - Shi, Qinfeng
AD  - Computer School, Wuhan University, Hubei, ChinaSchool of Computer Science, The University of Adelaide, Adelaide, Australia
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977706
SP  - 467
EP  - 484
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Blind image deblurring is a challenging problem due to its ill-posed nature, of which the success is closely related to a proper image prior. Although a large number of sparsity-based priors, such as the sparse gradient prior, have been successfully applied for blind image deblurring, they inherently suffer from several drawbacks, limiting their applications. Existing sparsity-based priors are usually rooted in modeling the response of images to some specific filters (e.g., image gradients), which are insufficient to capture the complicated image structures. Moreover, the traditional sparse priors or regularizations model the filter response (e.g., image gradients) independently and thus fail to depict the long-range correlation among them. To address the above issues, we present a novel image prior for image deblurring based on a Super-Gaussian field model with adaptive structures. Instead of modeling the response of the fixed short-term filters, the proposed Super-Gaussian fields capture the complicated structures in natural images by integrating potentials on all cliques (e.g., centring at each pixel) into a joint probabilistic distribution. Considering that the fixed filters in different scales are impractical for the coarse-to-fine framework of image deblurring, we define each potential function as a super-Gaussian distribution. Through this definition, the partition function, the curse for traditional MRFs, can be theoretically ignored, and all model parameters of the proposed Super-Gaussian fields can be data-adaptively learned and inferred from the blurred observation with a variational framework. Extensive experiments on both blind deblurring and non-blind deblurring demonstrate the effectiveness of the proposed method.  2018, Springer Nature Switzerland AG.
KW  - Image enhancement
KW  - Computer vision
KW  - Discrete cosine transforms
KW  - Gaussian distribution
KW  - Structural frames
U2  - Adaptive structure
U2  - Complicated structures
U2  - Joint probabilistic
U2  - Long range correlations
U2  - Partition functions
U2  - Potential function
U2  - Super-Gaussian distributions
U2  - Variational framework
DO  - 10.1007/978-3-030-01246-5_28
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_28
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Image Generation from Sketch Constraint Using Contextual GAN
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Lu, Yongyi
A1  - Wu, Shangzhe
A1  - Tai, Yu-Wing
A1  - Tang, Chi-Keung
AD  - The Hong Kong University of Science and Technology, Kowloon, Hong KongTencent Youtu, Shanghai, ChinaUniversity of Oxford, Oxford, United Kingdom
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978918
SP  - 213
EP  - 228
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper we investigate image generation guided by hand sketch. When the input sketch is badly drawn, the output of common image-to-image translation follows the input edges due to the hard condition imposed by the translation process. Instead, we propose to use sketch as weak constraint, where the output edges do not necessarily follow the input edges. We address this problem using a novel joint image completion approach, where the sketch provides the image context for completing, or generating the output image. We train a generated adversarial network, i.e, contextual GAN to learn the joint distribution of sketch and the corresponding image by using joint images. Our contextual GAN has several advantages. First, the simple joint image representation allows for simple and effective learning of joint distribution in the same image-sketch space, which avoids complicated issues in cross-domain learning. Second, while the output is related to its input overall, the generated features exhibit more freedom in appearance and do not strictly align with the input features as previous conditional GANs do. Third, from the joint images point of view, image and sketch are of no difference, thus exactly the same deep joint image completion network can be used for image-to-sketch generation. Experiments evaluated on three different datasets show that our contextual GAN can generate more realistic images than state-of-the-art conditional GANs on challenging inputs and generalize well on common categories.  2018, Springer Nature Switzerland AG.
KW  - Drawing (graphics)
KW  - Computer vision
KW  - Image reconstruction
U2  - Adversarial networks
U2  - Cross-domain learning
U2  - Effective learning
U2  - Image generations
U2  - Image representations
U2  - Image translation
U2  - Joint distributions
U2  - Translation process
DO  - 10.1007/978-3-030-01270-0_13
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_13
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Acquisition of localization confidence for accurate object detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Jiang, Borui
A1  - Luo, Ruixuan
A1  - Mao, Jiayuan
A1  - Xiao, Tete
A1  - Jiang, Yuning
AD  - School of Electronics Engineering and Computer Science, Peking University, Beijing, ChinaITCS, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, ChinaMegvii Inc. (Face++), Beijing, ChinaToutiao AI Lab, Beijing, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020337
SP  - 816
EP  - 832
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Modern CNN-based object detectors rely on bounding box regression and non-maximum suppression to localize objects. While the probabilities for class labels naturally reflect classification confidence, localization confidence is absent. This makes properly localized bounding boxes degenerate during iterative regression or even suppressed during NMS. In the paper we propose IoU-Net learning to predict the IoU between each detected bounding box and the matched ground-truth. The network acquires this confidence of localization, which improves the NMS procedure by preserving accurately localized bounding boxes. Furthermore, an optimization-based bounding box refinement method is proposed, where the predicted IoU is formulated as the objective. Extensive experiments on the MS-COCO dataset show the effectiveness of IoU-Net, as well as its compatibility with and adaptivity to several state-of-the-art object detectors.  2018, Springer Nature Switzerland AG.
KW  - Object detection
KW  - Computer vision
KW  - Iterative methods
KW  - Object recognition
KW  - Regression analysis
U2  - Bounding box
U2  - Classification confidence
U2  - Iterative regression
U2  - Non-maximum suppression
U2  - Object detectors
U2  - Object localization
U2  - Refinement methods
U2  - State of the art
DO  - 10.1007/978-3-030-01264-9_48
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_48
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Multi-scale Context Intertwining for Semantic Segmentation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Lin, Di
A1  - Ji, Yuanfeng
A1  - Lischinski, Dani
A1  - Cohen-Or, Daniel
A1  - Huang, Hui
AD  - Shenzhen University, Shenzhen, ChinaThe Hebrew University of Jerusalem, Jerusalem, IsraelTel Aviv University, Tel Aviv, Israel
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977826
SP  - 622
EP  - 638
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Accurate semantic image segmentation requires the joint consideration of local appearance, semantic information, and global scene context. In todays age of pre-trained deep networks and their powerful convolutional features, state-of-the-art semantic segmentation approaches differ mostly in how they choose to combine together these different kinds of information. In this work, we propose a novel scheme for aggregating features from different scales, which we refer to as Multi-Scale Context Intertwining (MSCI). In contrast to previous approaches, which typically propagate information between scales in a one-directional manner, we merge pairs of feature maps in a bidirectional and recurrent fashion, via connections between two LSTM chains. By training the parameters of the LSTM units on the segmentation task, the above approach learns how to extract powerful and effective features for pixel-level semantic segmentation, which are then combined hierarchically. Furthermore, rather than using fixed information propagation routes, we subdivide images into super-pixels, and use the spatial relationship between them in order to perform image-adapted context aggregation. Our extensive evaluation on public benchmarks indicates that all of the aforementioned components of our approach increase the effectiveness of information propagation throughout the network, and significantly improve its eventual segmentation accuracy.  2018, Springer Nature Switzerland AG.
KW  - Long short-term memory
KW  - Computer vision
KW  - Convolution
KW  - Deep learning
KW  - Image segmentation
KW  - Information dissemination
KW  - Pixels
KW  - Semantics
U2  - Convolutional neural network
U2  - Information propagation
U2  - Segmentation accuracy
U2  - Semantic image segmentations
U2  - Semantic information
U2  - Semantic segmentation
U2  - Spatial relationships
U2  - State of the art
DO  - 10.1007/978-3-030-01219-9_37
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_37
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning Data Terms for Non-blind Deblurring
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Dong, Jiangxin
A1  - Pan, Jinshan
A1  - Sun, Deqing
A1  - Su, Zhixun
A1  - Yang, Ming-Hsuan
AD  - Dalian University of Technology, Dalian, ChinaNanjing University of Science and Technology, Nanjing, ChinaNVIDIA, Westford, United StatesGuilin University of Electronic Technology, Guilin, ChinaUniversity of California at Merced, Merced, United States
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978904
SP  - 777
EP  - 792
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Existing deblurring methods mainly focus on developing effective image priors and assume that blurred images contain insignificant amounts of noise. However, state-of-the-art deblurring methods do not perform well on real-world images degraded with significant noise or outliers. To address these issues, we show that it is critical to learn data fitting terms beyond the commonly used 1 or 2 norm. We propose a simple and effective discriminative framework to learn data terms that can adaptively handle blurred images in the presence of severe noise and outliers. Instead of learning the distribution of the data fitting errors, we directly learn the associated shrinkage function for the data term using a cascaded architecture, which is more flexible and efficient. Our analysis shows that the shrinkage functions learned at the intermediate stages can effectively suppress noise and preserve image structures. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods.  2018, Springer Nature Switzerland AG.
KW  - Image enhancement
KW  - Computer vision
KW  - Data handling
KW  - Shrinkage
KW  - Statistics
U2  - Image deblurring
U2  - Image Structures
U2  - Intermediate stage
U2  - Learning data
U2  - Noise and outliers
U2  - Shrinkage functions
U2  - State of the art
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01252-6_46
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_46
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Generative Domain-Migration Hashing for Sketch-to-Image Retrieval
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Jingyi
A1  - Shen, Fumin
A1  - Liu, Li
A1  - Zhu, Fan
A1  - Yu, Mengyang
A1  - Shao, Ling
A1  - Shen, Heng Tao
A1  - Van Gool, Luc
AD  - Center for Future Media and School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, ChinaInception Institute of Artificial Intelligence, Abu Dhabi, United Arab EmiratesComputer Vision Lab, ETH Zurich, Zurich, Switzerland
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005965
SP  - 304
EP  - 321
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Due to the succinct nature of free-hand sketch drawings, sketch-based image retrieval (SBIR) has abundant practical use cases in consumer electronics. However, SBIR remains a long-standing unsolved problem mainly because of the significant discrepancy between the sketch domain and the image domain. In this work, we propose a Generative Domain-migration Hashing (GDH) approach, which for the first time generates hashing codes from synthetic natural images that are migrated from sketches. The generative model learns a mapping that the distributions of sketches can be indistinguishable from the distribution of natural images using an adversarial loss, and simultaneously learns an inverse mapping based on the cycle consistency loss in order to enhance the indistinguishability. With the robust mapping learned from the generative model, GDH can migrate sketches to their indistinguishable image counterparts while preserving the domain-invariant information of sketches. With an end-to-end multi-task learning framework, the generative model and binarized hashing codes can be jointly optimized. Comprehensive experiments of both category-level and fine-grained SBIR on multiple large-scale datasets demonstrate the consistently balanced superiority of GDH in terms of efficiency, memory costs and effectiveness (Models and code at https://github.com/YCJGG/GDH ).  2018, Springer Nature Switzerland AG.
KW  - Image retrieval
KW  - Codes (symbols)
KW  - Computer vision
KW  - Hash functions
KW  - Image enhancement
KW  - Mapping
U2  - Domain-migration
U2  - Generative model
U2  - Indistinguishability
U2  - Large-scale datasets
U2  - Multitask learning
U2  - SBIR
U2  - Sketch-based image retrievals
U2  - Unsolved problems
DO  - 10.1007/978-3-030-01216-8_19
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_19
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - RCAA: Relational context-aware agents for person search
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chang, Xiaojun
A1  - Huang, Po-Yao
A1  - Shen, Yi-Dong
A1  - Liang, Xiaodan
A1  - Yang, Yi
A1  - Hauptmann, Alexander G.
AD  - School of Computer Science, Carnegie Mellon University, Pittsburgh, United StatesInstitute of Software, Chinese Academy of Sciences, Beijing, ChinaCentre for Artificial Intelligence, University of Technology Sydney, Ultimo, Australia
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977525
SP  - 86
EP  - 102
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We aim to search for a target person from a gallery of whole scene images for which the annotations of pedestrian bounding boxes are unavailable. Previous approaches to this problem have relied on a pedestrian proposal net, which may generate redundant proposals and increase the computational burden. In this paper, we address this problem by training relational context-aware agents which learn the actions to localize the target person from the gallery of whole scene images. We incorporate the relational spatial and temporal contexts into the framework. Specifically, we propose to use the target person as the query in the query-dependent relational network. The agent determines the best action to take at each time step by simultaneously considering the local visual information, the relational and temporal contexts, together with the target person. To validate the performance of our approach, we conduct extensive experiments on the large-scale Person Search benchmark dataset and achieve significant improvements over the compared approaches. It is also worth noting that the proposed model even performs better than traditional methods with perfect pedestrian detectors.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Benchmarking
U2  - Benchmark datasets
U2  - Bounding box
U2  - Computational burden
U2  - Context-aware agent
U2  - Person search
U2  - Relational network
U2  - Scene image
U2  - Visual information
DO  - 10.1007/978-3-030-01240-3_6
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_6
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep Generative Models for Weakly-Supervised Multi-Label Classification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chu, Hong-Min
A1  - Yeh, Chih-Kuan
A1  - Wang, Yu-Chiang Frank
AD  - College of EECS, National Taiwan University, Taipei, TaiwanMachine Learning Department, Carnegie Mellon University, Pittsburgh, United States
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005972
SP  - 409
EP  - 425
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In order to train learning models for multi-label classification (MLC), it is typically desirable to have a large amount of fully annotated multi-label data. Since such annotation process is in general costly, we focus on the learning task of weakly-supervised multi-label classification (WS-MLC). In this paper, we tackle WS-MLC by learning deep generative models for describing the collected data. In particular, we introduce a sequential network architecture for constructing our generative model with the ability to approximate observed data posterior distributions. We show that how information of training data with missing labels or unlabeled ones can be exploited, which allows us to learn multi-label classifiers via scalable variational inferences. Empirical studies on various scales of datasets demonstrate the effectiveness of our proposed model, which performs favorably against state-of-the-art MLC algorithms.  2018, Springer Nature Switzerland AG.
KW  - Classification (of information)
KW  - Computer vision
KW  - Network architecture
KW  - Supervised learning
U2  - Empirical studies
U2  - Generative model
U2  - Multi label classification
U2  - Posterior distributions
U2  - Semi- supervised learning
U2  - State of the art
U2  - Variational inference
U2  - Weakly supervised learning
DO  - 10.1007/978-3-030-01216-8_25
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_25
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Rendering portraitures from monocular camera and beyond
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Xu, Xiangyu
A1  - Sun, Deqing
A1  - Liu, Sifei
A1  - Ren, Wenqi
A1  - Zhang, Yu-Jin
A1  - Yang, Ming-Hsuan
A1  - Sun, Jian
AD  - Tsinghua University, Beijing, ChinaSenseTime, Beijing, ChinaNvidia, Santa Clara, United StatesTencent AI Lab, Bellevue, United StatesUC Merced, Merced, United StatesGoogle, Menlo Park, United StatesFace++, Beijing, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977502
SP  - 36
EP  - 51
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Shallow Depth-of-Field (DoF) is a desirable effect in photography which renders artistic photos. Usually, it requires single-lens reflex cameras and certain photography skills to generate such effects. Recently, dual-lens on cellphones is used to estimate scene depth and simulate DoF effects for portrait shots. However, this technique cannot be applied to photos already taken and does not work well for whole-body scenes where the subject is at a distance from the cameras. In this work, we introduce an automatic system that achieves portrait DoF rendering for monocular cameras. Specifically, we first exploit Convolutional Neural Networks to estimate the relative depth and portrait segmentation maps from a single input image. Since these initial estimates from a single input are usually coarse and lack fine details, we further learn pixel affinities to refine the coarse estimation maps. With the refined estimation, we conduct depth and segmentation-aware blur rendering to the input image with a Conditional Random Field and image matting. In addition, we train a spatially-variant Recursive Neural Network to learn and accelerate this rendering process. We show that the proposed algorithm can effectively generate portraitures with realistic DoF effects using one single input. Experimental results also demonstrate that our depth and segmentation estimation modules perform favorably against the state-of-the-art methods both quantitatively and qualitatively.  Springer Nature Switzerland AG 2018.
KW  - Rendering (computer graphics)
KW  - Cameras
KW  - Computer vision
KW  - Image segmentation
KW  - Neural networks
KW  - Photography
U2  - Conditional random field
U2  - Convolutional neural network
U2  - Estimation module
U2  - Portrait segmentation
U2  - Recursive neural networks
U2  - Rendering process
U2  - Single lens reflex cameras
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01240-3_3
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_3
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning region features for object detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Gu, Jiayuan
A1  - Hu, Han
A1  - Wang, Liwei
A1  - Wei, Yichen
A1  - Dai, Jifeng
AD  - Key Laboratory of Machine Perception, MOE, School of EECS, Peking University, Beijing, ChinaMicrosoft Research Asia, Beijing, ChinaCenter for Data Science, Beijing Institute of Big Data Research, Peking University, Beijing, China
VL  - 11216 LNCS
PY  - 2018
U1  - 20184305977447
SP  - 392
EP  - 406
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - While most steps in the modern object detection methods are learnable, the region feature extraction step remains largely hand-crafted, featured by RoI pooling methods. This work proposes a general viewpoint that unifies existing region feature extraction methods and a novel method that is end-to-end learnable. The proposed method removes most heuristic choices and outperforms its RoI pooling counterparts. It moves further towards fully learnable object detection.  Springer Nature Switzerland AG 2018.
KW  - Heuristic methods
KW  - Computer vision
KW  - Extraction
KW  - Feature extraction
KW  - Object detection
KW  - Object recognition
U2  - End to end
U2  - Object detection method
U2  - Region feature
DO  - 10.1007/978-3-030-01258-8_24
L2  - http://dx.doi.org/10.1007/978-3-030-01258-8_24
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - GAL: Geometric adversarial loss for single-view 3D-object reconstruction
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Jiang, Li
A1  - Shi, Shaoshuai
A1  - Qi, Xiaojuan
A1  - Jia, Jiaya
AD  - The Chinese University of Hong Kong, Hong KongTencent YouTu Lab, Shenzhen, China
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006086
SP  - 820
EP  - 834
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we present a framework for reconstructing a point-based 3D model of an object from a single-view image. We found distance metrics, like Chamfer distance, were used in previous work to measure the difference of two point sets and serve as the loss function in point-based reconstruction. However, such point-point loss does not constrain the 3D model from a global perspective. We propose adding geometric adversarial loss (GAL). It is composed of two terms where the geometric loss ensures consistent shape of reconstructed 3D models close to ground-truth from different viewpoints, and the conditional adversarial loss generates a semantically-meaningful point cloud. GAL benefits predicting the obscured part of objects and maintaining geometric structure of the predicted 3D model. Both the qualitative results and quantitative analysis manifest the generality and suitability of our method.  Springer Nature Switzerland AG 2018.
KW  - Three dimensional computer graphics
KW  - Computer vision
KW  - Geometry
KW  - Image reconstruction
U2  - 3-D object reconstruction
U2  - 3D reconstruction
U2  - Distance metrics
U2  - Geometric consistency
U2  - Geometric structure
U2  - Global perspective
U2  - Loss functions
U2  - Point cloud
DO  - 10.1007/978-3-030-01237-3_49
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_49
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Exploiting Vector Fields for Geometric Rectification of Distorted Document Images
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Meng, Gaofeng
A1  - Su, Yuanqi
A1  - Wu, Ying
A1  - Xiang, Shiming
A1  - Pan, Chunhong
AD  - National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, ChinaXian Jiaotong University, Xian; Shaanxi, ChinaNorthwestern University, Evanston, United States
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978916
SP  - 180
EP  - 195
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper proposes a segment-free method for geometric rectification of a distorted document image captured by a hand-held camera. The method can recover the 3D page shape by exploiting the intrinsic vector fields of the image. Based on the assumption that the curled page shape is a general cylindrical surface, we estimate the parameters related to the camera and the 3D shape model through weighted majority voting on the vector fields. Then the spatial directrix of the surface is recovered by solving an ordinary differential equation (ODE) through the Euler method. Finally, the geometric distortions in images can be rectified by flattening the estimated 3D page surface onto a plane. Our method can exploit diverse types of visual cues available in a distorted document image to estimate its vector fields for 3D page shape recovery. In comparison to the state-of-the-art methods, the great advantage is that it is a segment-free method and does not have to extract curved text lines or textual blocks, which is still a very challenging problem especially for a distorted document image. Our method can therefore be freely applied to document images with extremely complicated page layouts and severe image quality degradation. Extensive experiments are implemented to demonstrate the effectiveness of the proposed method.  2018, Springer Nature Switzerland AG.
KW  - Image segmentation
KW  - Cameras
KW  - Computer vision
KW  - Geometry
KW  - Optical character recognition
KW  - Ordinary differential equations
KW  - Recovery
KW  - Shape optimization
KW  - Vectors
U2  - 3DShape recovery
U2  - Cylindrical surface
U2  - Document image processing
U2  - Geometric distortion
U2  - Geometric rectification
U2  - Ordinary differential equation (ODE)
U2  - State-of-the-art methods
U2  - Vector fields
DO  - 10.1007/978-3-030-01270-0_11
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_11
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Eigendecomposition-Free Training of Deep Networks with Zero Eigenvalue-Based Losses
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Dang, Zheng
A1  - Yi, Kwang Moo
A1  - Hu, Yinlin
A1  - Wang, Fei
A1  - Fua, Pascal
A1  - Salzmann, Mathieu
AD  - National Engineering Laboratory for Visual Information Processing and Application, Xian Jiaotong University, 99 Yanxiang Road, Xian; Shaanxi; 710054, ChinaSchool of Electronic and Information Engineering, Xian Jiaotong University, 28 West Xianning Road, Xian; Shaanxi; 710049, ChinaVisual Computing Group, University of Victoria, VIC, CanadaCVLab, EPFL, Lausanne, Switzerland
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976885
SP  - 792
EP  - 807
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Many classical Computer Vision problems, such as essential matrix computation and pose estimation from 3D to 2D correspondences, can be solved by finding the eigenvector corresponding to the smallest, or zero, eigenvalue of a matrix representing a linear system. Incorporating this in deep learning frameworks would allow us to explicitly encode known notions of geometry, instead of having the network implicitly learn them from data. However, performing eigendecomposition within a network requires the ability to differentiate this operation. While theoretically doable, this introduces numerical instability in the optimization process in practice. In this paper, we introduce an eigendecomposition-free approach to training a deep network whose loss depends on the eigenvector corresponding to a zero eigenvalue of a matrix predicted by the network. We demonstrate on several tasks, including keypoint matching and 3D pose estimation, that our approach is much more robust than explicit differentiation of the eigendecomposition. It has better convergence properties and yields state-of-the-art results on both tasks.  2018, Springer Nature Switzerland AG.
KW  - Eigenvalues and eigenfunctions
KW  - Computer vision
KW  - Deep learning
KW  - Linear systems
KW  - Singular value decomposition
U2  - 3D pose estimation
U2  - Computer vision problems
U2  - Convergence properties
U2  - Eigen decomposition
U2  - End to end
U2  - Key point matching
U2  - Learning frameworks
U2  - Numerical instability
DO  - 10.1007/978-3-030-01228-1_47
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_47
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Dense Semantic and Topological Correspondence of 3D Faces without Landmarks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Fan, Zhenfeng
A1  - Hu, Xiyuan
A1  - Chen, Chen
A1  - Peng, Silong
AD  - Institute of Automation, Chinese Academy of Sciences, Beijing; 100190, ChinaUniversity of Chinese Academy of Sciences, Huairou, ChinaBeijing Visytem Co. Ltd, Haidian, China
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978939
SP  - 541
EP  - 558
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Many previous literatures use landmarks to guide the correspondence of 3D faces. However, these landmarks, either manually or automatically annotated, are hard to define consistently across different faces in many circumstances. We propose a general framework for dense correspondence of 3D faces without landmarks in this paper. The dense correspondence goal is revisited in two perspectives: semantic and topological correspondence. Starting from a template facial mesh, we sequentially perform global alignment, primary correspondence by template warping, and contextual mesh refinement, to reach the final correspondence result. The semantic correspondence is achieved by a local iterative closest point (ICP) algorithm of kernelized version, allowing accurate matching of local features. Then, robust deformation from the template to the target face is formulated as a minimization problem. Furthermore, this problem leads to a well-posed sparse linear system such that the solution is unique and efficient. Finally, a contextual mesh refining algorithm is applied to ensure topological correspondence. In the experiment, the proposed method is evaluated both qualitatively and quantitatively on two datasets including a publicly available FRGC v2.0 dataset, demonstrating reasonable and reliable correspondence results.  2018, Springer Nature Switzerland AG.
KW  - Computer graphics
KW  - Computer vision
KW  - Iterative methods
KW  - Linear systems
KW  - Mesh generation
KW  - Semantics
KW  - Topology
U2  - 3D faces
U2  - Dense correspondences
U2  - Iterative closest point algorithm
U2  - Minimization problems
U2  - Point-set registrations
U2  - Refining algorithms
U2  - Semantic correspondence
U2  - Sparse linear systems
DO  - 10.1007/978-3-030-01270-0_32
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_32
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Small-scale pedestrian detection based on topological line localization and temporal feature aggregation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Song, Tao
A1  - Sun, Leiyu
A1  - Xie, Di
A1  - Sun, Haiming
A1  - Pu, Shiliang
AD  - Hikvision Research Institute, Hangzhou, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977610
SP  - 554
EP  - 569
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - A critical issue in pedestrian detection is to detect small-scale objects that will introduce feeble contrast and motion blur in images and videos, which in our opinion should partially resort to deep-rooted annotation bias. Motivated by this, we propose a novel method integrated with somatic topological line localization (TLL) and temporal feature aggregation for detecting multi-scale pedestrians, which works particularly well with small-scale pedestrians that are relatively far from the camera. Moreover, a post-processing scheme based on Markov Random Field (MRF) is introduced to eliminate ambiguities in occlusion cases. Applying with these methodologies comprehensively, we achieve best detection performance on Caltech benchmark and improve performance of small-scale objects significantly (miss rate decreases from 74.53% to 60.79%). Beyond this, we also achieve competitive performance on CityPersons dataset and show the existence of annotation bias in KITTI dataset.  Springer Nature Switzerland AG 2018.
KW  - Feature extraction
KW  - Benchmarking
KW  - Computer vision
KW  - Deep learning
KW  - Magnetorheological fluids
KW  - Markov processes
KW  - Object detection
KW  - Topology
U2  - Competitive performance
U2  - Detection performance
U2  - Improve performance
U2  - Markov Random Fields
U2  - Multi-scale
U2  - Pedestrian detection
U2  - Post-processing scheme
U2  - Temporal features
DO  - 10.1007/978-3-030-01234-2_33
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_33
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - HybridFusion: Real-time performance capture using a single depth sensor and sparse IMUs
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zheng, Zerong
A1  - Yu, Tao
A1  - Li, Hao
A1  - Guo, Kaiwen
A1  - Dai, Qionghai
A1  - Fang, Lu
A1  - Liu, Yebin
AD  - Tsinghua University, Beijing, ChinaBeihang University, Beijing, ChinaUniversity of Southern California, Los Angeles; CA, United StatesGoogle Inc., Mountain View; CA, United StatesTsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977496
SP  - 389
EP  - 406
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We propose a light-weight yet highly robust method for real-time human performance capture based on a single depth camera and sparse inertial measurement units (IMUs). Our method combines non-rigid surface tracking and volumetric fusion to simultaneously reconstruct challenging motions, detailed geometries and the inner human body of a clothed subject. The proposed hybrid motion tracking algorithm and efficient per-frame sensor calibration technique enable non-rigid surface reconstruction for fast motions and challenging poses with severe occlusions. Significant fusion artifacts are reduced using a new confidence measurement for our adaptive TSDF-based fusion. The above contributions are mutually beneficial in our reconstruction system, which enable practical human performance capture that is real-time, robust, low-cost and easy to deploy. Experiments show that extremely challenging performances and loop closure problems can be handled successfully.  Springer Nature Switzerland AG 2018.
KW  - Surface reconstruction
KW  - Computer vision
U2  - Inertial measurement unit
U2  - Motion tracking algorithms
U2  - Performance capture
U2  - Real time
U2  - Real time performance
U2  - Reconstruction systems
U2  - Sensor calibration
U2  - Single-view
DO  - 10.1007/978-3-030-01240-3_24
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_24
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Decouple learning for parameterized image operators
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Fan, Qingnan
A1  - Chen, Dongdong
A1  - Yuan, Lu
A1  - Hua, Gang
A1  - Yu, Nenghai
A1  - Chen, Baoquan
AD  - Shandong University, Jinan, ChinaUniversity of Science and Technology of China, Hefei, ChinaBeijing Film Academy, Beijing, ChinaMicrosoft Research, Beijing, ChinaPeking University, Beijing, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008947
SP  - 455
EP  - 471
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Many different deep networks have been used to approximate, accelerate or improve traditional image operators, such as image smoothing, super-resolution and denoising. Among these traditional operators, many contain parameters which need to be tweaked to obtain the satisfactory results, which we refer to as parameterized image operators. However, most existing deep networks trained for these operators are only designed for one specific parameter configuration, which does not meet the needs of real scenarios that usually require flexible parameters settings. To overcome this limitation, we propose a new decouple learning algorithm to learn from the operator parameters to dynamically adjust the weights of a deep network for image operators, denoted as the base network. The learned algorithm is formed as another network, namely the weight learning network, which can be end-to-end jointly trained with the base network. Experiments demonstrate that the proposed framework can be successfully applied to many traditional parameterized image operators. We provide more analysis to better understand the proposed framework, which may inspire more promising research in this direction. Our codes and models have been released in https://github.com/fqnchina/DecoupleLearning.  Springer Nature Switzerland AG 2018.
KW  - Learning algorithms
KW  - Computer vision
KW  - Image enhancement
KW  - Parameterization
U2  - De-noising
U2  - Deep networks
U2  - Image operators
U2  - Image smoothing
U2  - Learning network
U2  - Parameterized
U2  - Parameters setting
U2  - Super resolution
DO  - 10.1007/978-3-030-01261-8_27
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_27
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Linear Span Network for Object Skeleton Detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Chang
A1  - Ke, Wei
A1  - Qin, Fei
A1  - Ye, Qixiang
AD  - University of Chinese Academy of Sciences, Beijing, China
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406006000
SP  - 136
EP  - 151
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Robust object skeleton detection requires to explore rich representative visual features and effective feature fusion strategies. In this paper, we first re-visit the implementation of HED, the essential principle of which can be ideally described with a linear reconstruction model. Hinted by this, we formalize a Linear Span framework, and propose Linear Span Network (LSN) which introduces Linear Span Units (LSUs) to minimizes the reconstruction error. LSN further utilizes subspace linear span besides the feature linear span to increase the independence of convolutional features and the efficiency of feature integration, which enhances the capability of fitting complex ground-truth. As a result, LSN can effectively suppress the cluttered backgrounds and reconstruct object skeletons. Experimental results validate the state-of-the-art performance of the proposed LSN.  2018, Springer Nature Switzerland AG.
KW  - Linear networks
KW  - Computer vision
KW  - Musculoskeletal system
KW  - Object detection
U2  - Cluttered backgrounds
U2  - Feature fusion
U2  - Feature integration
U2  - Linear reconstruction
U2  - Linear span
U2  - Reconstruction error
U2  - State-of-the-art performance
U2  - Visual feature
DO  - 10.1007/978-3-030-01216-8_9
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_9
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Unified Perceptual Parsing for Scene Understanding
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Xiao, Tete
A1  - Liu, Yingcheng
A1  - Zhou, Bolei
A1  - Jiang, Yuning
A1  - Sun, Jian
AD  - Peking University, Beijing, ChinaMIT CSAIL, Cambridge, United StatesBytedance Inc., Beijing, ChinaMegvii Inc., Beijing, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976862
SP  - 432
EP  - 448
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes (Models are available at https://github.com/CSAILVision/unifiedparsing ).  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Deep neural networks
KW  - Image segmentation
KW  - Object detection
KW  - Semantics
U2  - Machine vision systems
U2  - Multiple levels
U2  - Natural scenes
U2  - Scene understanding
U2  - Semantic segmentation
U2  - Training strategy
U2  - Visual concept
U2  - Visual knowledge
DO  - 10.1007/978-3-030-01228-1_26
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_26
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Remote Photoplethysmography Correspondence Feature for 3D Mask Face Presentation Attack Detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Si-Qi
A1  - Lan, Xiangyuan
A1  - Yuen, Pong C.
AD  - Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978941
SP  - 577
EP  - 594
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - 3D mask face presentation attack, as a new challenge in face recognition, has been attracting increasing attention. Recently, remote Photoplethysmography (rPPG) is employed as an intrinsic liveness cue which is independent of the mask appearance. Although existing rPPG-based methods achieve promising results on both intra and cross dataset scenarios, they may not be robust enough when rPPG signals are contaminated by noise. In this paper, we propose a new liveness feature, called rPPG correspondence feature (CFrPPG) to precisely identify the heartbeat vestige from the observed noisy rPPG signals. To further overcome the global interferences, we propose a novel learning strategy which incorporates the global noise within the CFrPPG feature. Extensive experiments indicate that the proposed feature not only outperforms the state-of-the-art rPPG based methods on 3D mask attacks but also be able to handle the practical scenarios with dim light and camera motion.  2018, Springer Nature Switzerland AG.
KW  - Face recognition
KW  - Computer vision
KW  - Photoplethysmography
U2  - 3D masks
U2  - Attack detection
U2  - Camera motions
U2  - Dim light
U2  - Global noise
U2  - Learning strategy
U2  - Liveness
U2  - State of the art
DO  - 10.1007/978-3-030-01270-0_34
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_34
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Generalizing a person retrieval model hetero- and homogeneously
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhong, Zhun
A1  - Zheng, Liang
A1  - Li, Shaozi
A1  - Yang, Yi
AD  - Cognitive Science Department, Xiamen University, Xiamen, ChinaCentre for Artificial Intelligence, University of Technology Sydney, Ultimo, AustraliaResearch School of Computer Science, Australian National University, Canberra, Australia
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008930
SP  - 176
EP  - 192
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Person re-identification (re-ID) poses unique challenges for unsupervised domain adaptation (UDA) in that classes in the source and target sets (domains) are entirely different and that image variations are largely caused by cameras. Given a labeled source training set and an unlabeled target training set, we aim to improve the generalization ability of re-ID models on the target testing set. To this end, we introduce a Hetero-Homogeneous Learning (HHL) method. Our method enforces two properties simultaneously: (1) camera invariance, learned via positive pairs formed by unlabeled target images and their camera style transferred counterparts; (2) domain connectedness, by regarding source/target images as negative matching pairs to the target/source images. The first property is implemented by homogeneous learning because training pairs are collected from the same domain. The second property is achieved by heterogeneous learning because we sample training pairs from both the source and target domains. On Market-1501, DukeMTMC-reID and CUHK03, we show that the two properties contribute indispensably and that very competitive re-ID UDA accuracy is achieved. Code is available at: https://github.com/zhunzhong07/HHL.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Ability testing
KW  - Cameras
U2  - Domain adaptation
U2  - Generalization ability
U2  - Image variations
U2  - Person re identifications
U2  - Retrieval models
U2  - Sample training
U2  - Target domain
U2  - Target images
DO  - 10.1007/978-3-030-01261-8_11
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_11
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - SegStereo: Exploiting semantic information for disparity estimation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yang, Guorun
A1  - Zhao, Hengshuang
A1  - Shi, Jianping
A1  - Deng, Zhidong
A1  - Jia, Jiaya
AD  - Department of Computer Science, State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, ChinaThe Chinese University of Hong Kong, Shatin, Hong KongSenseTime Research, Beijing, ChinaTencent YouTu Lab, Shenzhen, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977616
SP  - 660
EP  - 676
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Disparity estimation for binocular stereo images finds a wide range of applications. Traditional algorithms may fail on featureless regions, which could be handled by high-level clues such as semantic segments. In this paper, we suggest that appropriate incorporation of semantic cues can greatly rectify prediction in commonly-used disparity estimation frameworks. Our method conducts semantic feature embedding and regularizes semantic cues as the loss term to improve learning disparity. Our unified model SegStereo employs semantic features from segmentation and introduces semantic softmax loss, which helps improve the prediction accuracy of disparity maps. The semantic cues work well in both unsupervised and supervised manners. SegStereo achieves state-of-the-art results on KITTI Stereo benchmark and produces decent prediction on both CityScapes and FlyingThings3D datasets.  Springer Nature Switzerland AG 2018.
KW  - Semantics
KW  - Computer vision
KW  - Forecasting
KW  - Stereo image processing
U2  - Binocular stereo
U2  - Disparity estimations
U2  - Prediction accuracy
U2  - Semantic cues
U2  - Semantic features
U2  - Semantic information
U2  - Softmax loss regularization
U2  - Unified Modeling
DO  - 10.1007/978-3-030-01234-2_39
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_39
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Deng, Haowen
A1  - Birdal, Tolga
A1  - Ilic, Slobodan
AD  - Technische Universitat Munchen, Munich, GermanySiemens AG, Munich, GermanyNational University of Defense Technology, Changsha, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976874
SP  - 620
EP  - 638
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present PPF-FoldNet for unsupervised learning of 3D local descriptors on pure point cloud geometry. Based on the folding-based auto-encoding of well known point pair features, PPF-FoldNet offers many desirable properties: it necessitates neither supervision, nor a sensitive local reference frame, benefits from point-set sparsity, is end-to-end, fast, and can extract powerful rotation invariant descriptors. Thanks to a novel feature visualization, its evolution can be monitored to provide interpretable insights. Our extensive experiments demonstrate that despite having six degree-of-freedom invariance and lack of training labels, our network achieves state of the art results in standard benchmark datasets and outperforms its competitors when rotations and varying point densities are present. PPF-FoldNet achieves 9% higher recall on standard benchmarks, 23% higher recall when rotations are introduced into the same datasets and finally, a margin of 35% is attained when point density is significantly decreased.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Computer vision
KW  - Degrees of freedom (mechanics)
KW  - Rotation
KW  - Unsupervised learning
U2  - Benchmark datasets
U2  - Descriptors
U2  - Local descriptors
U2  - Local feature
U2  - Rotation invariance
U2  - Rotation invariant
U2  - Six degree-of-freedom
U2  - State of the art
DO  - 10.1007/978-3-030-01228-1_37
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_37
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Pan, Xingang
A1  - Luo, Ping
A1  - Shi, Jianping
A1  - Tang, Xiaoou
AD  - CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Shatin, Hong KongSenseTime Group Limited, Beijing, China
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004579
SP  - 484
EP  - 500
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Convolutional neural networks (CNNs) have achieved great successes in many computer vision problems. Unlike existing works that designed CNN architectures to improve performance on a single task of a single domain and not generalizable, we present IBN-Net, a novel convolutional architecture, which remarkably enhances a CNNs modeling ability on one domain (e.g. Cityscapes) as well as its generalization capacity on another domain (e.g. GTA5) without finetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch Normalization (BN) as building blocks, and can be wrapped into many advanced deep networks to improve their performances. This work has three key contributions. (1) By delving into IN and BN, we disclose that IN learns features that are invariant to appearance changes, such as colors, styles, and virtuality/reality, while BN is essential for preserving content related information. (2) IBN-Net can be applied to many advanced deep architectures, such as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their performance without increasing computational cost. (3) When applying the trained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves comparable improvements as domain adaptation methods, even without using data from the target domain. With IBN-Net, we won the 1st place on the WAD 2018 Challenge Drivable Area track, with an mIoU of 86.18%.  2018, Springer Nature Switzerland AG.
KW  - Network architecture
KW  - Computer vision
KW  - Convolution
KW  - Invariance
KW  - Neural networks
U2  - CNNs
U2  - Computer vision problems
U2  - Convolutional neural network
U2  - Generalization
U2  - Generalization capacity
U2  - Improve performance
U2  - Instance normalization
U2  - Without fine-tuning
DO  - 10.1007/978-3-030-01225-0_29
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_29
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Move Forward and Tell: A Progressive Generator of Video Descriptions
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Xiong, Yilei
A1  - Dai, Bo
A1  - Lin, Dahua
AD  - CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Hong Kong
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978885
SP  - 489
EP  - 505
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present an efficient framework that can generate a coherent paragraph to describe a given video. Previous works on video captioning usually focus on video clips. They typically treat an entire video as a whole and generate the caption conditioned on a single embedding. On the contrary, we consider videos with rich temporal structures and aim to generate paragraph descriptions that can preserve the story flow while being coherent and concise. Towards this goal, we propose a new approach, which produces a descriptive paragraph by assembling temporally localized descriptions. Given a video, it selects a sequence of distinctive clips and generates sentences thereon in a coherent manner. Particularly, the selection of clips and the production of sentences are done jointly and progressively driven by a recurrent network  what to describe next depends on what have been said before. Here, the recurrent network is learned via self-critical sequence training with both sentence-level and paragraph-level rewards. On the ActivityNet Captions dataset, our method demonstrated the capability of generating high-quality paragraph descriptions for videos. Compared to those by other methods, the descriptions produced by our method are often more relevant, more coherent, and more concise.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Reinforcement learning
U2  - Critical sequence
U2  - Move forward and tell
U2  - New approaches
U2  - Recurrent networks
U2  - Repetition evaluation
U2  - Sentence level
U2  - Temporal structures
U2  - Video captioning
DO  - 10.1007/978-3-030-01252-6_29
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_29
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Specular-to-Diffuse Translation for Multi-view Reconstruction
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wu, Shihao
A1  - Huang, Hui
A1  - Portenier, Tiziano
A1  - Sela, Matan
A1  - Cohen-Or, Daniel
A1  - Kimmel, Ron
A1  - Zwicker, Matthias
AD  - University of Bern, Bern, SwitzerlandShenzhen University, Shenzhen, ChinaTechnion - Israel Institute of Technology, Haifa, IsraelTel-Aviv University, Tel Aviv, IsraelUniversity of Maryland, College Park, United States
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004561
SP  - 193
EP  - 211
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Most multi-view 3D reconstruction algorithms, especially when shape-from-shading cues are used, assume that object appearance is predominantly diffuse. To alleviate this restriction, we introduce S2Dnet, a generative adversarial network for transferring multiple views of objects with specular reflection into diffuse ones, so that multi-view reconstruction methods can be applied more effectively. Our network extends unsupervised image-to-image translation to multi-view specular to diffuse translation. To preserve object appearance across multiple views, we introduce a Multi-View Coherence loss (MVC) that evaluates the similarity and faithfulness of local patches after the view-transformation. In addition, we carefully design and generate a large synthetic training data set using physically-based rendering. During testing, our network takes only the raw glossy images as input, without extra information such as segmentation masks or lighting estimation. Results demonstrate that multi-view reconstruction can be significantly improved using the images filtered by our network.  2018, Springer Nature Switzerland AG.
KW  - Image reconstruction
KW  - Computer vision
KW  - Image enhancement
KW  - Image segmentation
U2  - Adversarial networks
U2  - Image translation
U2  - Multi-view reconstruction
U2  - Multi-views
U2  - Specular-to-diffuse
DO  - 10.1007/978-3-030-01225-0_12
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_12
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Correcting the triplet selection bias for triplet loss
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yu, Baosheng
A1  - Liu, Tongliang
A1  - Gong, Mingming
A1  - Ding, Changxing
A1  - Tao, Dacheng
AD  - UBTECH Sydney AI Centre and SIT, FEIT, The University of Sydney, Sydney, AustraliaDepartment of Biomedical Informatics, University of Pittsburgh, Pittsburgh, United StatesDepartment of Philosophy, Carnegie Mellon University, Pittsburgh, United StatesSchool of Electronic and Information Engineering, South China University of Technology, Guangzhou, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977423
SP  - 71
EP  - 86
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Triplet loss, popular for metric learning, has made a great success in many computer vision tasks, such as fine-grained image classification, image retrieval, and face recognition. Considering that the number of triplets grows cubically with the size of training data, triplet selection is thus indispensable for efficiently training with triplet loss. However, in practice, the training is usually very sensitive to the selection of triplets, e.g., it almost does not converge with randomly selected triplets and selecting the hardest triplets also leads to bad local minima. We argue that the bias in the selection of triplets degrades the performance of learning with triplet loss. In this paper, we propose a new variant of triplet loss, which tries to reduce the bias in triplet selection by adaptively correcting the distribution shift on the selected triplets. We refer to this new triplet loss as adapted triplet loss. We conduct a number of experiments on MNIST and Fashion-MNIST for image classification, and on CARS196, CUB200-2011, and Stanford Online Products for image retrieval. The experimental results demonstrate the effectiveness of the proposed method.  Springer Nature Switzerland AG 2018.
KW  - Image retrieval
KW  - Computer vision
KW  - Face recognition
KW  - Image classification
U2  - Domain adaptation
U2  - Fine grained
U2  - Local minimums
U2  - Metric learning
U2  - Online products
U2  - Selection bias
U2  - Stanford
U2  - Training data
DO  - 10.1007/978-3-030-01231-1_5
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_5
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Weakly Supervised Region Proposal Network and Object Detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Tang, Peng
A1  - Wang, Xinggang
A1  - Wang, Angtian
A1  - Yan, Yongluan
A1  - Liu, Wenyu
A1  - Huang, Junzhou
A1  - Yuille, Alan
AD  - School of EIC, Huazhong University of Science and Technology, Wuhan, ChinaTencent AI lab, Shenzhen, ChinaDepartment of CSE, University of Texas at Arlington, Arlington, United StatesDepartment of Computer Science, The Johns Hopkins University, Baltimore, United States
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978878
SP  - 370
EP  - 386
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The Convolutional Neural Network (CNN) based region proposal generation method (i.e. region proposal network), trained using bounding box annotations, is an essential component in modern fully supervised object detectors. However, Weakly Supervised Object Detection (WSOD) has not benefited from CNN-based proposal generation due to the absence of bounding box annotations, and is relying on standard proposal generation methods such as selective search. In this paper, we propose a weakly supervised region proposal network which is trained using only image-level annotations. The weakly supervised region proposal network consists of two stages. The first stage evaluates the objectness scores of sliding window boxes by exploiting the low-level information in CNN and the second stage refines the proposals from the first stage using a region-based CNN classifier. Our proposed region proposal network is suitable for WSOD, can be plugged into a WSOD network easily, and can share its convolutional computations with the WSOD network. Experiments on the PASCAL VOC and ImageNet detection datasets show that our method achieves the state-of-the-art performance for WSOD with performance gain of about 3 % on average.  2018, Springer Nature Switzerland AG.
KW  - Object detection
KW  - Classification (of information)
KW  - Computer vision
KW  - Convolution
KW  - Image segmentation
KW  - Neural networks
KW  - Object recognition
U2  - Convolutional neural network
U2  - Convolutional Neural Networks (CNN)
U2  - Generation method
U2  - Object detectors
U2  - Performance Gain
U2  - Region proposals
U2  - State-of-the-art performance
U2  - Weakly supervised learning
DO  - 10.1007/978-3-030-01252-6_22
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_22
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - GeoDesc: Learning local descriptors by integrating geometry constraints
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Luo, Zixin
A1  - Shen, Tianwei
A1  - Zhou, Lei
A1  - Zhu, Siyu
A1  - Zhang, Runze
A1  - Yao, Yao
A1  - Fang, Tian
A1  - Quan, Long
AD  - Hong Kong University of Science and Technology, Clear Water Bay, Hong KongShenzhen Zhuke Innovation Technology (Altizure), Shenzhen, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977482
SP  - 170
EP  - 185
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Learned local descriptors based on Convolutional Neural Networks (CNNs) have achieved significant improvements on patch-based benchmarks, whereas not having demonstrated strong generalization ability on recent benchmarks of image-based 3D reconstruction. In this paper, we mitigate this limitation by proposing a novel local descriptor learning approach that integrates geometry constraints from multi-view reconstructions, which benefits the learning process in terms of data generation, data sampling and loss computation. We refer to the proposed descriptor as GeoDesc, and demonstrate its superior performance on various large-scale benchmarks, and in particular show its great success on challenging reconstruction tasks. Moreover, we provide guidelines towards practical integration of learned descriptors in Structure-from-Motion (SfM) pipelines, showing the good trade-off that GeoDesc delivers to 3D reconstruction tasks between accuracy and efficiency.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Benchmarking
KW  - Computer vision
KW  - Economic and social effects
KW  - Image enhancement
KW  - Image reconstruction
KW  - Neural networks
U2  - Convolutional neural network
U2  - Feature descriptors
U2  - Generalization ability
U2  - Geometry constraints
U2  - Learning approach
U2  - Local feature
U2  - Multi-view reconstruction
U2  - Structure from motion
DO  - 10.1007/978-3-030-01240-3_11
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_11
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Adaptively Transforming Graph Matching
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Fudong
A1  - Xue, Nan
A1  - Zhang, Yipeng
A1  - Bai, Xiang
A1  - Xia, Gui-Song
AD  - State Key Laboratory LIESMARS, Wuhan University, Wuhan, ChinaSchool of Computer Science, Wuhan University, Wuhan, ChinaEIS, Huazhong University of Science and Technology, Wuhan, China
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978945
SP  - 646
EP  - 662
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recently, many graph matching methods that incorporate pairwise constraint and that can be formulated as a quadratic assignment problem (QAP) have been proposed. Although these methods demonstrate promising results for the graph matching problem, they have high complexity in space or time. In this paper, we introduce an adaptively transforming graph matching (ATGM) method from the perspective of functional representation. More precisely, under a transformation formulation, we aim to match two graphs by minimizing the discrepancy between the original graph and the transformed graph. With a linear representation map of the transformation, the pairwise edge attributes of graphs are explicitly represented by unary node attributes, which enables us to reduce the space and time complexity significantly. Due to an efficient Frank-Wolfe method-based optimization strategy, we can handle graphs with hundreds and thousands of nodes within an acceptable amount of time. Meanwhile, because transformation map can preserve graph structures, a domain adaptation-based strategy is proposed to remove the outliers. The experimental results demonstrate that our proposed method outperforms the state-of-the-art graph matching algorithms.  2018, Springer Nature Switzerland AG.
KW  - Graph theory
KW  - Combinatorial optimization
KW  - Computer vision
KW  - Linear transformations
KW  - Mathematical transformations
KW  - Pattern matching
U2  - Frank-wolfe methods
U2  - Functional representation
U2  - Graph matching problems
U2  - Graph matchings
U2  - Graph-matching algorithms
U2  - Quadratic assignment problems
U2  - Space and time complexity
U2  - Transformation representation
DO  - 10.1007/978-3-030-01270-0_38
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_38
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deepvs: A deep learning based video saliency prediction approach
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Jiang, Lai
A1  - Xu, Mai
A1  - Liu, Tie
A1  - Qiao, Minglang
A1  - Wang, Zulin
AD  - Beihang University, Beijing, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020325
SP  - 625
EP  - 642
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a novel deep learning based video saliency prediction method, named DeepVS. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which includes 32 subjects fixations on 538 videos. We find from LEDOV that human attention is more likely to be attracted by objects, particularly the moving objects or the moving parts of objects. Hence, an object-to-motion convolutional neural network (OM-CNN) is developed to predict the intra-frame saliency for DeepVS, which is composed of the objectness and motion subnets. In OM-CNN, cross-net mask and hierarchical feature normalization are proposed to combine the spatial features of the objectness subnet and the temporal features of the motion subnet. We further find from our database that there exists a temporal correlation of human attention with a smooth saliency transition across video frames. We thus propose saliency-structured convolutional long short-term memory (SS-ConvLSTM) network, using the extracted features from OM-CNN as the input. Consequently, the inter-frame saliency maps of a video can be generated, which consider both structured output with center-bias and cross-frame transitions of human attention maps. Finally, the experimental results show that DeepVS advances the state-of-the-art in video saliency prediction.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Computer vision
KW  - Convolution
KW  - Database systems
KW  - Eye movements
KW  - Eye tracking
KW  - Forecasting
KW  - Long short-term memory
U2  - Convolutional LSTM
U2  - Convolutional neural network
U2  - Hierarchical features
U2  - Prediction methods
U2  - Spatial features
U2  - Temporal correlations
U2  - Temporal features
U2  - Video saliencies
DO  - 10.1007/978-3-030-01264-9_37
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_37
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Nanyang
A1  - Zhang, Yinda
A1  - Li, Zhuwen
A1  - Fu, Yanwei
A1  - Liu, Wei
A1  - Jiang, Yu-Gang
AD  - Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, ChinaPrinceton University, Princeton, United StatesIntel Labs, Santa Clara, United StatesSchool of Data Science, Fudan University, Shanghai, ChinaTencent AI Lab, Bellevue, United States
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978897
SP  - 55
EP  - 71
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.  2018, Springer Nature Switzerland AG.
KW  - Mesh generation
KW  - Computer graphics
KW  - Computer vision
KW  - Convolution
KW  - Deep neural networks
KW  - Deformation
KW  - Graphic methods
KW  - Neural networks
U2  - 3-D shape
U2  - Coarse to fine
U2  - Convolutional neural network
U2  - End to end
U2  - Mesh reconstruction
DO  - 10.1007/978-3-030-01252-6_4
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_4
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Towards end-to-end license plate detection and recognition: A large dataset and baseline
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Xu, Zhenbo
A1  - Yang, Wei
A1  - Meng, Ajin
A1  - Lu, Nanxue
A1  - Huang, Huan
A1  - Ying, Changchun
A1  - Huang, Liusheng
AD  - School of Computer Science and Technology, University of Science and Technology of China, Hefei, ChinaXingtai Financial Holdings Group Co., Ltd., Hefei; Anhui, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008935
SP  - 261
EP  - 277
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Most current license plate (LP) detection and recognition approaches are evaluated on a small and usually unrepresentative dataset since there are no publicly available large diverse datasets. In this paper, we introduce CCPD, a large and comprehensive LP dataset. All images are taken manually by workers of a roadside parking management company and are annotated carefully. To our best knowledge, CCPD is the largest publicly available LP dataset to date with over 250k unique car images, and the only one provides vertices location annotations. With CCPD, we present a novel network model which can predict the bounding box and recognize the corresponding LP number simultaneously with high speed and accuracy. Through comparative experiments, we demonstrate our model outperforms current object detection and recognition approaches in both accuracy and speed. In real-world applications, our model recognizes LP numbers directly from relatively high-resolution images at over 61 fps and 98.5% accuracy.  Springer Nature Switzerland AG 2018.
KW  - Object recognition
KW  - Computer vision
KW  - Human resource management
KW  - Image segmentation
KW  - License plates (automobile)
KW  - Neural networks
KW  - Object detection
KW  - Optical character recognition
U2  - Comparative experiments
U2  - Convolutional neural network
U2  - High resolution image
U2  - License plate detection
U2  - Network modeling
U2  - Object detection and recognition
U2  - Object segmentation
U2  - Parking management
DO  - 10.1007/978-3-030-01261-8_16
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_16
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Look deeper into depth: monocular depth estimation with semantic booster and attention-driven loss
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Jiao, Jianbo
A1  - Cao, Ying
A1  - Song, Yibing
A1  - Lau, Rynson
AD  - City University of Hong Kong, Kowloon, Hong KongUniversity of Illinois at Urbana-Champaign, Urbana, United StatesTencent AI Lab, Shenzhen, China
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006190
SP  - 55
EP  - 71
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Monocular depth estimation benefits greatly from learning based techniques. By studying the training data, we observe that the per-pixel depth values in existing datasets typically exhibit a long-tailed distribution. However, most previous approaches treat all the regions in the training data equally regardless of the imbalanced depth distribution, which restricts the model performance particularly on distant depth regions. In this paper, we investigate the long tail property and delve deeper into the distant depth regions (i.e. the tail part) to propose an attention-driven loss for the network supervision. In addition, to better leverage the semantic information for monocular depth estimation, we propose a synergy network to automatically learn the information sharing strategies between the two tasks. With the proposed attention-driven loss and synergy network, the depth estimation and semantic labeling tasks can be mutually improved. Experiments on the challenging indoor dataset show that the proposed approach achieves state-of-the-art performance on both monocular depth estimation and semantic labeling tasks.  Springer Nature Switzerland AG 2018.
KW  - Semantics
KW  - Computer vision
U2  - Depth distribution
U2  - Information sharing strategies
U2  - Long-tailed distributions
U2  - Model performance
U2  - Monocular depth
U2  - Semantic information
U2  - Semantic labeling
U2  - State-of-the-art performance
DO  - 10.1007/978-3-030-01267-0_4
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_4
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - AGIL: Learning Attention from Human for Visuomotor Tasks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Ruohan
A1  - Liu, Zhuode
A1  - Zhang, Luxin
A1  - Whritner, Jake A.
A1  - Muller, Karl S.
A1  - Hayhoe, Mary M.
A1  - Ballard, Dana H.
AD  - Department of Computer Science, University of Texas at Austin, Austin, United StatesGoogle Inc., Mountain View, United StatesDepartment of Intelligence Science, Peking University, Beijing, ChinaCenter for Perceptual Systems, University of Texas at Austin, Austin, United States
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978899
SP  - 692
EP  - 707
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - When intelligent agents learn visuomotor behaviors from human demonstrations, they may benefit from knowing where the human is allocating visual attention, which can be inferred from their gaze. A wealth of information regarding intelligent decision making is conveyed by human gaze allocation; hence, exploiting such information has the potential to improve the agents performance. With this motivation, we propose the AGIL (Attention Guided Imitation Learning) framework. We collect high-quality human action and gaze data while playing Atari games in a carefully controlled experimental setting. Using these data, we first train a deep neural network that can predict human gaze positions and visual attention with high accuracy (the gaze network) and then train another network to predict human actions (the policy network). Incorporating the learned attention model from the gaze network into the policy network significantly improves the action prediction accuracy and task performance.  2018, Springer Nature Switzerland AG.
KW  - Behavioral research
KW  - Computer vision
KW  - Decision making
KW  - Deep neural networks
KW  - Eye tracking
KW  - Forecasting
KW  - Intelligent agents
U2  - Action prediction
U2  - Experimental settings
U2  - Human demonstrations
U2  - Imitation learning
U2  - Intelligent decision making
U2  - Task performance
U2  - Visual Attention
U2  - Wealth of information
DO  - 10.1007/978-3-030-01252-6_41
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_41
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Consensus-driven propagation in massive unlabeled data for face recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhan, Xiaohang
A1  - Liu, Ziwei
A1  - Yan, Junjie
A1  - Lin, Dahua
A1  - Loy, Chen Change
AD  - CUHK - SenseTime Joint Lab, The Chinese University of Hong Kong, Shatin, Hong KongSenseTime Group Limited, Beijing, ChinaNanyang Technological University, Singapore, Singapore
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977508
SP  - 576
EP  - 592
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Face recognition has witnessed great progress in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking the real-world scenario, where the unlabeled data are collected from unconstrained environments and their identities are exclusive from the labeled ones. Our main insight is that although the class information is not available, we can still faithfully approximate these semantic relationships by constructing a relational graph in a bottom-up manner. We propose Consensus-Driven Propagation (CDP) to tackle this challenging problem with two modules, the committee and the mediator, which select positive face pairs robustly by carefully aggregating multi-view information. Extensive experiments validate the effectiveness of both modules to discard outliers and mine hard positives. With CDP, we achieve a compelling accuracy of 78.18% on MegaFace identification challenge by using only 9% of the labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all labels are employed.  Springer Nature Switzerland AG 2018.
KW  - Face recognition
KW  - Computer vision
KW  - Semantics
U2  - Bottom-up manner
U2  - Class information
U2  - High capacity
U2  - Real-world scenario
U2  - Relational graph
U2  - Semantic relationships
U2  - Unconstrained environments
U2  - Unlabeled data
DO  - 10.1007/978-3-030-01240-3_35
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_35
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - VQA-E: Explaining, elaborating, and enhancing your answers for visual questions
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Qing
A1  - Tao, Qingyi
A1  - Joty, Shafiq
A1  - Cai, Jianfei
A1  - Luo, Jiebo
AD  - University of Science and Technology of China, Hefei, ChinaNanyang Technological University, Singapore, SingaporeNVIDIA AI Technology Center, Westford, United StatesUniversity of Rochester, Rochester, United States
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977611
SP  - 570
EP  - 586
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Most existing works in visual question answering (VQA) are dedicated to improving the accuracy of predicted answers, while disregarding the explanations. We argue that the explanation for an answer is of the same or even more importance compared with the answer itself, since it makes the question answering process more understandable and traceable. To this end, we propose a new task of VQA-E (VQA with Explanation), where the models are required to generate an explanation with the predicted answer. We first construct a new dataset, and then frame the VQA-E problem in a multi-task learning architecture. Our VQA-E dataset is automatically derived from the VQA v2 dataset by intelligently exploiting the available captions. We also conduct a user study to validate the quality of the synthesized explanations. We quantitatively show that the additional supervision from explanations can not only produce insightful textual sentences to justify the answers, but also improve the performance of answer prediction. Our model outperforms the state-of-the-art methods by a clear margin on the VQA v2 dataset.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Artificial intelligence
KW  - Computer science
KW  - Computers
U2  - Multitask learning
U2  - Question Answering
U2  - State-of-the-art methods
U2  - User study
DO  - 10.1007/978-3-030-01234-2_34
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_34
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Simple baselines for human pose estimation and tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Xiao, Bin
A1  - Wu, Haiping
A1  - Wei, Yichen
AD  - Microsoft Research Asia, Beijing, ChinaUniversity of Electronic Science and Technology of China, Chengdu, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977406
SP  - 472
EP  - 487
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github.com/leoxiaobin/pose.pytorch.  Springer Nature Switzerland AG 2018.
KW  - Gesture recognition
KW  - Computational complexity
KW  - Computer vision
U2  - Algorithm analysis
U2  - Baseline methods
U2  - Human pose estimations
U2  - Human pose tracking
U2  - Pose estimation
U2  - Pose tracking
U2  - State of the art
U2  - System complexity
DO  - 10.1007/978-3-030-01231-1_29
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_29
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - GridFace: Face Rectification via Learning Local Homography Transformations
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhou, Erjin
A1  - Cao, Zhimin
A1  - Sun, Jian
AD  - Face++, Megvii Inc., Beijing, China
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978914
SP  - 3
EP  - 20
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a method, called GridFace, to reduce facial geometric variations and improve the recognition performance. Our method rectifies the face by local homography transformations, which are estimated by a face rectification network. To encourage the image generation with canonical views, we apply a regularization based on the natural face distribution. We learn the rectification network and recognition network in an end-to-end manner. Extensive experiments show our method greatly reduces geometric variations, and gains significant improvements in unconstrained face recognition scenarios.  2018, Springer Nature Switzerland AG.
KW  - Face recognition
KW  - Computer vision
U2  - End to end
U2  - Face rectification
U2  - Geometric variations
U2  - Homography transformation
U2  - Image generations
DO  - 10.1007/978-3-030-01270-0_1
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_1
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Efficient semantic scene completion network with spatial group convolution
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Jiahui
A1  - Zhao, Hao
A1  - Yao, Anbang
A1  - Chen, Yurong
A1  - Zhang, Li
A1  - Liao, Hongen
AD  - Department of Biomedical Engineering, Tsinghua University, Beijing, ChinaDepartment of Electronic Engineering, Tsinghua University, Beijing, ChinaIntel Labs China, Beijing, China
VL  - 11216 LNCS
PY  - 2018
U1  - 20184305977470
SP  - 749
EP  - 765
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We introduce Spatial Group Convolution (SGC) for accelerating the computation of 3D dense prediction tasks. SGC is orthogonal to group convolution, which works on spatial dimensions rather than feature channel dimension. It divides input voxels into different groups, then conducts 3D sparse convolution on these separated groups. As only valid voxels are considered when performing convolution, computation can be significantly reduced with a slight loss of accuracy. The proposed operations are validated on semantic scene completion task, which aims to predict a complete 3D volume with semantic labels from a single depth image. With SGC, we further present an efficient 3D sparse convolutional network, which harnesses a multiscale architecture and a coarse-to-fine prediction strategy. Evaluations are conducted on the SUNCG dataset, achieving state-of-the-art performance and fast speed.  Springer Nature Switzerland AG 2018.
KW  - Convolution
KW  - Computer vision
KW  - Forecasting
KW  - Semantic Web
KW  - Semantics
U2  - Channel dimension
U2  - Convolutional networks
U2  - Loss of accuracy
U2  - Prediction tasks
U2  - Semantic labels
U2  - Spatial dimension
U2  - Spatial groups
U2  - State-of-the-art performance
DO  - 10.1007/978-3-030-01258-8_45
L2  - http://dx.doi.org/10.1007/978-3-030-01258-8_45
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - PM-GANs: Discriminative representation learning for action recognition using partial-modalities
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Lan
A1  - Gao, Chenqiang
A1  - Yang, Luyu
A1  - Zhao, Yue
A1  - Zuo, Wangmeng
A1  - Meng, Deyu
AD  - School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing; 400065, ChinaChongqing Key Laboratory of Signal and Information Processing, Chongqing; 400065, ChinaUniversity of Maryland College Park, College Park; MD; 20742, United StatesHarbin Institute of Technology, Harbin; 150001, ChinaXian Jiaotong University, Xian; 710049, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977401
SP  - 389
EP  - 406
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Data of different modalities generally convey complimentary but heterogeneous information, and a more discriminative representation is often preferred by combining multiple data modalities like the RGB and infrared features. However in reality, obtaining both data channels is challenging due to many limitations. For example, the RGB surveillance cameras are often restricted from private spaces, which is in conflict with the need of abnormal activity detection for personal security. As a result, using partial data channels to build a full representation of multi-modalities is clearly desired. In this paper, we propose a novel Partial-modal Generative Adversarial Networks (PM-GANs) that learns a full-modal representation using data from only partial modalities. The full representation is achieved by a generated representation in place of the missing data channel. Extensive experiments are conducted to verify the performance of our proposed method on action recognition, compared with four state-of-the-art methods. Meanwhile, a new Infrared-Visible Dataset for action recognition is introduced, and will be the first publicly available action dataset that contains paired infrared and visible spectrum. (The dataset will be available at http://www.escience.cn/people/gaochenqiang/Publications.html).  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Security systems
U2  - Abnormal activity detection
U2  - Action recognition
U2  - Adversarial networks
U2  - Cross-modal representations
U2  - Heterogeneous information
U2  - Modal representation
U2  - State-of-the-art methods
U2  - Surveillance cameras
DO  - 10.1007/978-3-030-01231-1_24
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_24
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning to reconstruct high-quality 3D shapes with cascaded fully convolutional networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Cao, Yan-Pei
A1  - Liu, Zheng-Ning
A1  - Kuang, Zheng-Fei
A1  - Kobbelt, Leif
A1  - Hu, Shi-Min
AD  - Tsinghua University, Beijing, ChinaOwlii Inc., Beijing, ChinaRWTH Aachen University, Aachen, Germany
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977511
SP  - 626
EP  - 643
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present a data-driven approach to reconstructing high-resolution and detailed volumetric representations of 3D shapes. Although well studied, algorithms for volumetric fusion from multi-view depth scans are still prone to scanning noise and occlusions, making it hard to obtain high-fidelity 3D reconstructions. In this paper, inspired by recent advances in efficient 3D deep learning techniques, we introduce a novel cascaded 3D convolutional network architecture, which learns to reconstruct implicit surface representations from noisy and incomplete depth maps in a progressive, coarse-to-fine manner. To this end, we also develop an algorithm for end-to-end training of the proposed cascaded structure. Qualitative and quantitative experimental results on both simulated and real-world datasets demonstrate that the presented approach outperforms existing state-of-the-art work in terms of quality and fidelity of reconstructed models.  Springer Nature Switzerland AG 2018.
KW  - Image reconstruction
KW  - Cascade control systems
KW  - Computer vision
KW  - Convolution
KW  - Deep learning
KW  - Network architecture
U2  - 3D reconstruction
U2  - Cascaded structure
U2  - Convolutional networks
U2  - Data-driven approach
U2  - Implicit surface representation
U2  - Learning techniques
U2  - Real-world datasets
U2  - Volumetric representation
DO  - 10.1007/978-3-030-01240-3_38
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_38
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - EC-Net: An edge-aware point set consolidation network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yu, Lequan
A1  - Li, Xianzhi
A1  - Fu, Chi-Wing
A1  - Cohen-Or, Daniel
A1  - Heng, Pheng-Ann
AD  - The Chinese University of Hong Kong, Shatin, Hong KongTel Aviv University, Tel Aviv, IsraelShenzhen Key Laboratory of Virtual Reality and Human Interaction Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977600
SP  - 398
EP  - 414
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Point clouds obtained from 3D scans are typically sparse, irregular, and noisy, and required to be consolidated. In this paper, we present the first deep learning based edge-aware technique to facilitate the consolidation of point clouds. We design our network to process points grouped in local patches, and train it to learn and help consolidate points, deliberately for edges. To achieve this, we formulate a regression component to simultaneously recover 3D point coordinates and point-to-edge distances from upsampled features, and an edge-aware joint loss function to directly minimize distances from output points to 3D meshes and to edges. Compared with previous neural network based works, our consolidation is edge-aware. During the synthesis, our network can attend to the detected sharp edges and enable more accurate 3D reconstructions. Also, we trained our network on virtual scanned point clouds, demonstrated the performance of our method on both synthetic and real point clouds, presented various surface reconstruction results, and showed how our method outperforms the state-of-the-arts.  Springer Nature Switzerland AG 2018.
KW  - Arts computing
KW  - Computer vision
KW  - Deep learning
KW  - Neural networks
KW  - Surface reconstruction
U2  - 3D reconstruction
U2  - Edge aware
U2  - Edge distance
U2  - Learning
U2  - Loss functions
U2  - Point cloud
U2  - Point coordinates
U2  - State of the art
DO  - 10.1007/978-3-030-01234-2_24
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_24
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Cross-Modal Hamming Hashing
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Cao, Yue
A1  - Liu, Bin
A1  - Long, Mingsheng
A1  - Wang, Jianmin
AD  - School of Software, Tsinghua University, Beijing, ChinaNational Engineering Laboratory for Big Data Software, Beijing National Research Center for Information Science and Technology, Beijing, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977690
SP  - 207
EP  - 223
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Cross-modal hashing enables similarity retrieval across different content modalities, such as searching relevant images in response to text queries. It provide with the advantages of computation efficiency and retrieval quality for multimedia retrieval. Hamming space retrieval enables efficient constant-time search that returns data items within a given Hamming radius to each query, by hash lookups instead of linear scan. However, Hamming space retrieval is ineffective in existing cross-modal hashing methods, subject to their weak capability of concentrating the relevant items to be within a small Hamming ball, while worse still, the Hamming distances between hash codes from different modalities are inevitably large due to the large heterogeneity across different modalities. This work presents Cross-Modal Hamming Hashing (CMHH), a novel deep cross-modal hashing approach that generates compact and highly concentrated hash codes to enable efficient and effective Hamming space retrieval. The main idea is to penalize significantly on similar cross-modal pairs with Hamming distance larger than the Hamming radius threshold, by designing a pairwise focal loss based on the exponential distribution. Extensive experiments demonstrate that CMHH can generate highly concentrated hash codes and achieve state-of-the-art cross-modal retrieval performance for both hash lookups and linear scan scenarios on three benchmark datasets, NUS-WIDE, MIRFlickr-25K, and IAPR TC-12.  2018, Springer Nature Switzerland AG.
KW  - Hamming distance
KW  - Benchmarking
KW  - Codes (symbols)
KW  - Computer vision
KW  - Hash functions
KW  - Information retrieval
U2  - Computation efficiency
U2  - Cross-modal
U2  - Deep hashing
U2  - Exponential distributions
U2  - Hamming space
U2  - Multimedia Retrieval
U2  - Retrieval performance
U2  - Similarity retrieval
DO  - 10.1007/978-3-030-01246-5_13
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_13
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Adding attentiveness to the neurons in recurrent neural networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Pengfei
A1  - Xue, Jianru
A1  - Lan, Cuiling
A1  - Zeng, Wenjun
A1  - Gao, Zhanning
A1  - Zheng, Nanning
AD  - Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xian, ChinaMicrosoft Reserach Asia, Beijing, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977319
SP  - 136
EP  - 152
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recurrent neural networks (RNNs) are capable of modeling the temporal dynamics of complex sequential information. However, the structures of existing RNN neurons mainly focus on controlling the contributions of current and historical information but do not explore the different importance levels of different elements in an input vector of a time slot. We propose adding a simple yet effective Element-wise-Attention Gate (EleAttG) to an RNN block (e.g., all RNN neurons in a network layer) that empowers the RNN neurons to have the attentiveness capability. For an RNN block, an EleAttG is added to adaptively modulate the input by assigning different levels of importance, i.e., attention, to each element/dimension of the input. We refer to an RNN block equipped with an EleAttG as an EleAtt-RNN block. Specifically, the modulation of the input is content adaptive and is performed at fine granularity, being element-wise rather than input-wise. The proposed EleAttG, as an additional fundamental unit, is general and can be applied to any RNN structures, e.g., standard RNN, Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU). We demonstrate the effectiveness of the proposed EleAtt-RNN by applying it to the action recognition tasks on both 3D human skeleton data and RGB videos. Experiments show that adding attentiveness through EleAttGs to RNN blocks significantly boosts the power of RNNs.  Springer Nature Switzerland AG 2018.
KW  - Long short-term memory
KW  - Computer vision
KW  - Musculoskeletal system
KW  - Network layers
KW  - Neurons
KW  - Recurrent neural networks
U2  - Action recognition
U2  - Element-wise-Attention Gate (EleAttG)
U2  - Historical information
U2  - Recurrent neural network (RNNs)
U2  - RGB video
U2  - Sequential information
U2  - Skeleton
U2  - Temporal dynamics
DO  - 10.1007/978-3-030-01240-3_9
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_9
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Reverse attention for salient object detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Shuhan
A1  - Tan, Xiuli
A1  - Wang, Ben
A1  - Hu, Xuelong
AD  - School of Information Engineering, Yangzhou University, Yangzhou, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977486
SP  - 236
EP  - 252
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Benefit from the quick development of deep learning techniques, salient object detection has achieved remarkable progresses recently. However, there still exists following two major challenges that hinder its application in embedded devices, low resolution output and heavy model weight. To this end, this paper presents an accurate yet compact deep network for efficient salient object detection. More specifically, given a coarse saliency prediction in the deepest layer, we first employ residual learning to learn side-output residual features for saliency refinement, which can be achieved with very limited convolutional parameters while keep accuracy. Secondly, we further propose reverse attention to guide such side-output residual learning in a top-down manner. By erasing the current predicted salient regions from side-output features, the network can eventually explore the missing object parts and details which results in high resolution and accuracy. Experiments on six benchmark datasets demonstrate that the proposed approach compares favorably against state-of-the-art methods, and with advantages in terms of simplicity, efficiency (45 FPS) and model size (81 MB).  Springer Nature Switzerland AG 2018.
KW  - Object detection
KW  - Computer vision
KW  - Deep learning
KW  - Object recognition
U2  - Benchmark datasets
U2  - ITS applications
U2  - Learning techniques
U2  - Learning to learn
U2  - Reverse attention
U2  - Salient object detection
U2  - Side-output residual learning
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01240-3_15
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_15
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Accelerating dynamic programs via nested benders decomposition with application to multi-person pose estimation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Shaofei
A1  - Ihler, Alexander
A1  - Kording, Konrad
A1  - Yarkony, Julian
AD  - Baidu Inc., Beijing, ChinaUC Irvine, Irvine, United StatesUniversity of Pennsylvania, Philadelphia, United StatesExperian Data Lab, San Diego, United States
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020329
SP  - 677
EP  - 692
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present a novel approach to solve dynamic programs (DP), which are frequent in computer vision, on tree-structured graphs with exponential node state space. Typical DP approaches have to enumerate the joint state space of two adjacent nodes on every edge of the tree to compute the optimal messages. Here we propose an algorithm based on Nested Benders Decomposition (NBD) that iteratively lower-bounds the message on every edge and promises to be far more efficient. We apply our NBD algorithm along with a novel Minimum Weight Set Packing (MWSP) formulation to a multi-person pose estimation problem. While our algorithm is provably optimal at termination it operates in linear time for practical DP problems, gaining upato 500  speed up over traditional DP algorithm which have polynomial complexity.  2018, Springer Nature Switzerland AG.
KW  - Trees (mathematics)
KW  - Application programs
KW  - Computer vision
KW  - Forestry
KW  - Iterative methods
KW  - Linear programming
KW  - Stochastic programming
U2  - Adjacent nodes
U2  - Benders decomposition
U2  - Column generation
U2  - Dynamic programs
U2  - Minimum weight
U2  - Polynomial complexity
U2  - Pose estimation
U2  - Tree-structured
DO  - 10.1007/978-3-030-01264-9_40
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_40
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Key-word-aware network for referring expression image segmentation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Shi, Hengcan
A1  - Li, Hongliang
A1  - Meng, Fanman
A1  - Wu, Qingbo
AD  - School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977407
SP  - 38
EP  - 54
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Referring expression image segmentation aims to segment out the object referred by a natural language query expression. Without considering the specific properties of visual and textual information, existing works usually deal with this task by directly feeding a foreground/background classifier with cascaded image and text features, which are extracted from each image region and the whole query, respectively. On the one hand, they ignore that each word in a query expression makes different contributions to identify the desired object, which requires a differential treatment in extracting text feature. On the other hand, the relationships of different image regions are not considered as well, even though they are greatly important to eliminate the undesired foreground object in accordance with specific query. To address aforementioned issues, in this paper, we propose a key-word-aware network, which contains a query attention model and a key-word-aware visual context model. In extracting text features, the query attention model attends to assign higher weights for the words which are more important for identifying object. Meanwhile, the key-word-aware visual context model describes the relationships among different image regions, according to corresponding query. Our proposed method outperforms state-of-the-art methods on two referring expression image segmentation databases.  Springer Nature Switzerland AG 2018.
KW  - Image segmentation
KW  - Classification (of information)
KW  - Computer vision
U2  - Differential treatment
U2  - Foreground/background
U2  - Key words
U2  - Natural language queries
U2  - Query attention
U2  - Referring expressions
U2  - State-of-the-art methods
U2  - Visual context
DO  - 10.1007/978-3-030-01231-1_3
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_3
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning 3D keypoint descriptors for non-rigid shape matching
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Hanyu
A1  - Guo, Jianwei
A1  - Yan, Dong-Ming
A1  - Quan, Weize
A1  - Zhang, Xiaopeng
AD  - NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006043
SP  - 3
EP  - 20
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we present a novel deep learning framework that derives discriminative local descriptors for 3D surface shapes. In contrast to previous convolutional neural networks (CNNs) that rely on rendering multi-view images or extracting intrinsic shape properties, we parameterize the multi-scale localized neighborhoods of a keypoint into regular 2D grids, which are termed as geometry images. The benefits of such geometry images include retaining sufficient geometric information, as well as allowing the usage of standard CNNs. Specifically, we leverage a triplet network to perform deep metric learning, which takes a set of triplets as input, and a newly designed triplet loss function is minimized to distinguish between similar and dissimilar pairs of keypoints. At the testing stage, given a geometry image of a point of interest, our network outputs a discriminative local descriptor for it. Experimental results for non-rigid shape matching on several benchmarks demonstrate the superior performance of our learned descriptors over traditional descriptors and the state-of-the-art learning-based alternatives.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Benchmarking
KW  - Computer vision
KW  - Geometry
KW  - Neural networks
U2  - Convolutional neural network
U2  - Geometric information
U2  - Learning frameworks
U2  - Local descriptors
U2  - Local feature descriptor
U2  - Non-rigid shapes
U2  - Point of interest
U2  - Triplet CNNs
DO  - 10.1007/978-3-030-01237-3_1
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_1
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Dual-agent deep reinforcement learning for deformable face tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Guo, Minghao
A1  - Lu, Jiwen
A1  - Zhou, Jie
AD  - Tsinghua University, Beijing, China
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978805
SP  - 783
EP  - 799
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a dual-agent deep reinforcement learning (DADRL) method for deformable face tracking, which generates bounding boxes and detects facial landmarks interactively from face videos. Most existing deformable face tracking methods learn models for these two tasks individually, and perform these two procedures subsequently during the testing phase, which ignore the intrinsic connections of these two tasks. Motivated by the fact that the performance of facial landmark detection depends heavily on the accuracy of the generated bounding boxes, we exploit the interactions of these two tasks in probabilistic manner by following a Bayesian model and propose a unified framework for simultaneous bounding box tracking and landmark detection. By formulating it as a Markov decision process, we define two agents to exploit the relationships and pass messages via an adaptive sequence of actions under a deep reinforcement learning framework to iteratively adjust the positions of the bounding boxes and facial landmarks. Our proposed DADRL achieves performance improvements over the state-of-the-art deformable face tracking methods on the most challenging category of the 300-VW dataset.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Bayesian networks
KW  - Computer vision
KW  - Deformation
KW  - Face recognition
KW  - Iterative methods
KW  - Markov processes
KW  - Reinforcement learning
U2  - Face Tracking
U2  - Facial landmark detection
U2  - Landmark detection
U2  - Markov Decision Processes
U2  - Performance improvements
U2  - Sequence of actions
U2  - State of the art
U2  - Unified framework
DO  - 10.1007/978-3-030-01249-6_47
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_47
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - PSANet: Point-wise spatial attention network for scene parsing
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhao, Hengshuang
A1  - Zhang, Yi
A1  - Liu, Shu
A1  - Shi, Jianping
A1  - Loy, Chen Change
A1  - Lin, Dahua
A1  - Jia, Jiaya
AD  - The Chinese University of Hong Kong, Shatin, Hong KongCUHK-Sensetime Joint Lab, The Chinese University of Hong Kong, Shatin, Hong KongSenseTime Research, Beijing, ChinaNanyang Technological University, Singapore, SingaporeTencent Youtu Lab, Shenzhen, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977488
SP  - 270
EP  - 286
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We notice information flow in convolutional neural networks is restricted inside local neighborhood regions due to the physical design of convolutional filters, which limits the overall understanding of complex scenes. In this paper, we propose the point-wise spatial attention network (PSANet) to relax the local neighborhood constraint. Each position on the feature map is connected to all the other ones through a self-adaptively learned attention mask. Moreover, information propagation in bi-direction for scene parsing is enabled. Information at other positions can be collected to help the prediction of the current position and vice versa, information at the current position can be distributed to assist the prediction of other ones. Our proposed approach achieves top performance on various competitive scene parsing datasets, including ADE20K, PASCAL VOC 2012 and Cityscapes, demonstrating its effectiveness and generality.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Convolution
KW  - Information dissemination
KW  - Neural networks
KW  - Semantics
U2  - Convolutional neural network
U2  - Information flows
U2  - Information propagation
U2  - Local neighborhoods
U2  - Physical design
U2  - Scene parsing
U2  - Semantic segmentation
U2  - Spatial attention
DO  - 10.1007/978-3-030-01240-3_17
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_17
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Recurrent Fusion Network for Image Captioning
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Jiang, Wenhao
A1  - Ma, Lin
A1  - Jiang, Yu-Gang
A1  - Liu, Wei
A1  - Zhang, Tong
AD  - Tencent AI Lab, Shenzhen, ChinaFudan University, Shanghai, China
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005979
SP  - 510
EP  - 526
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recently, much advance has been made in image captioning, and an encoder-decoder framework has been adopted by all the state-of-the-art models. Under this framework, an input image is encoded by a convolutional neural network (CNN) and then translated into natural language with a recurrent neural network (RNN). The existing models counting on this framework employ only one kind of CNNs, e.g., ResNet or Inception-X, which describes the image contents from only one specific view point. Thus, the semantic meaning of the input image cannot be comprehensively understood, which restricts improving the performance. In this paper, to exploit the complementary information from multiple encoders, we propose a novel recurrent fusion network (RFNet) for the image captioning task. The fusion process in our model can exploit the interactions among the outputs of the image encoders and generate new compact and informative representations for the decoder. Experiments on the MSCOCO dataset demonstrate the effectiveness of our proposed RFNet, which sets a new state-of-the-art for image captioning.  2018, Springer Nature Switzerland AG.
KW  - Image fusion
KW  - Computer vision
KW  - Decoding
KW  - Image enhancement
KW  - Recurrent neural networks
KW  - Semantics
KW  - Signal encoding
U2  - Convolutional Neural Networks (CNN)
U2  - Encoder-decoder
U2  - Fusion process
U2  - Image captioning
U2  - Natural languages
U2  - Recurrent fusion network (RFNet)
U2  - Recurrent neural network (RNN)
U2  - State of the art
DO  - 10.1007/978-3-030-01216-8_31
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_31
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep Feature Pyramid Reconfiguration for Object Detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Kong, Tao
A1  - Sun, Fuchun
A1  - Huang, Wenbing
A1  - Liu, Huaping
AD  - Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, ChinaTencent AI Lab, Shenzhen, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976846
SP  - 172
EP  - 188
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - State-of-the-art object detectors usually learn multi-scale representations to get better results by employing feature pyramids. However, the current designs for feature pyramids are still inefficient to integrate the semantic information over different scales. In this paper, we begin by investigating current feature pyramids solutions, and then reformulate the feature pyramid construction as the feature reconfiguration process. Finally, we propose a novel reconfiguration architecture to combine low-level representations with high-level semantic features in a highly-nonlinear yet efficient way. In particular, our architecture which consists of global attention and local reconfigurations, is able to gather task-oriented features across different spatial locations and scales, globally and locally. Both the global attention and local reconfiguration are lightweight, in-place, and end-to-end trainable. Using this method in the basic SSD system, our models achieve consistent and significant boosts compared with the original model and its other variations, without losing real-time processing speed.  2018, Springer Nature Switzerland AG.
KW  - Feature extraction
KW  - Computer vision
KW  - Object detection
KW  - Object recognition
KW  - Semantics
U2  - Feature pyramid
U2  - Global-local
U2  - High-level semantic features
U2  - Local reconfigurations
U2  - Low level representation
U2  - Multiscale representations
U2  - Real-time processing speed
U2  - Reconfiguration process
DO  - 10.1007/978-3-030-01228-1_11
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_11
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning to predict crisp boundaries
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Deng, Ruoxi
A1  - Shen, Chunhua
A1  - Liu, Shengjun
A1  - Wang, Huibing
A1  - Liu, Xinru
AD  - Central South University, Changsha, ChinaThe University of Adelaide, Adelaide, AustraliaDalian University of Technology, Dalian, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977413
SP  - 570
EP  - 586
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recent methods for boundary or edge detection built on Deep Convolutional Neural Networks (CNNs) typically suffer from the issue of predicted edges being thick and need post-processing to obtain crisp boundaries. Highly imbalanced categories of boundary versus background in training data is one of main reasons for the above problem. In this work, the aim is to make CNNs produce sharp boundaries without post-processing. We introduce a novel loss for boundary detection, which is very effective for classifying imbalanced data and allows CNNs to produce crisp boundaries. Moreover, we propose an end-to-end network which adopts the bottom-up/top-down architecture to tackle the task. The proposed network effectively leverages hierarchical features and produces pixel-accurate boundary mask, which is critical to reconstruct the edge map. Our experiments illustrate that directly making crisp prediction not only promotes the visual results of CNNs, but also achieves better results against the state-of-the-art on the BSDS500 dataset (ODS F-score of.815) and the NYU Depth dataset (ODS F-score of.762).  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Convolution
KW  - Deep neural networks
KW  - Edge detection
KW  - Image segmentation
KW  - Neural networks
U2  - Bottom-up/top-down
U2  - Boundary detection
U2  - Contour detection
U2  - Convolutional neural network
U2  - Deep convolutional neural networks
U2  - End-to-end network
U2  - Hierarchical features
U2  - Sharp boundaries
DO  - 10.1007/978-3-030-01231-1_35
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_35
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Scale Aggregation Network for Accurate and Efficient Crowd Counting
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Cao, Xinkun
A1  - Wang, Zhipeng
A1  - Zhao, Yanyun
A1  - Su, Fei
AD  - School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, ChinaBeijing Key Laboratory of Network System and Network Culture, Beijing University of Posts and Telecommunications, Beijing, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976883
SP  - 757
EP  - 773
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a novel encoder-decoder network, called Scale Aggregation Network (SANet), for accurate and efficient crowd counting. The encoder extracts multi-scale features with scale aggregation modules and the decoder generates high-resolution density maps by using a set of transposed convolutions. Moreover, we find that most existing works use only Euclidean loss which assumes independence among each pixel but ignores the local correlation in density maps. Therefore, we propose a novel training loss, combining of Euclidean loss and local pattern consistency loss, which improves the performance of the model in our experiments. In addition, we use normalization layers to ease the training process and apply a patch-based test scheme to reduce the impact of statistic shift problem. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on four major crowd counting datasets and our method achieves superior performance to state-of-the-art methods while with much less parameters.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Decoding
KW  - Signal encoding
U2  - Aggregation network
U2  - Crowd counting
U2  - Crowd density
U2  - Local correlations
U2  - Local patterns
U2  - Multi-scale features
U2  - State-of-the-art methods
U2  - Training process
DO  - 10.1007/978-3-030-01228-1_45
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_45
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Lyu, Pengyuan
A1  - Liao, Minghui
A1  - Yao, Cong
A1  - Wu, Wenhao
A1  - Bai, Xiang
AD  - Huazhong University of Science and Technology, Wuhan, ChinaMegvii (Face++) Technology Inc., Beijing, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020339
SP  - 71
EP  - 88
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recently, models based on deep neural networks have dominated the fields of scene text detection and recognition. In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images. An end-to-end trainable neural network model for scene text spotting is proposed. The proposed model, named as Mask TextSpotter, is inspired by the newly published work Mask R-CNN. Different from previous methods that also accomplish text spotting with end-to-end trainable deep neural networks, Mask TextSpotter takes advantage of simple and smooth end-to-end learning procedure, in which precise text detection and recognition are acquired via semantic segmentation. Moreover, it is superior to previous methods in handling text instances of irregular shapes, for example, curved text. Experiments on ICDAR2013, ICDAR2015 and Total-Text demonstrate that the proposed method achieves state-of-the-art results in both scene text detection and end-to-end text recognition tasks.  2018, Springer Nature Switzerland AG.
KW  - Character recognition
KW  - Computer vision
KW  - Deep neural networks
KW  - Image segmentation
KW  - Neural networks
KW  - Semantics
U2  - Arbitrary shape
U2  - Irregular shape
U2  - Learning procedures
U2  - Neural network model
U2  - Scene Text
U2  - Semantic segmentation
U2  - State of the art
U2  - Text recognition
DO  - 10.1007/978-3-030-01264-9_5
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_5
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Lifelong Learning via Progressive Distillation and Retrospection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Hou, Saihui
A1  - Pan, Xinyu
A1  - Loy, Chen Change
A1  - Wang, Zilei
A1  - Lin, Dahua
AD  - Department of Automation, University of Science and Technology of China, Hefei, ChinaDepartment of Information Engineering, The Chinese University of Hong Kong, Hong KongNanyang Technological University, Singapore, Singapore
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977815
SP  - 452
EP  - 467
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Lifelong learning aims at adapting a learned model to new tasks while retaining the knowledge gained earlier. A key challenge for lifelong learning is how to strike a balance between the preservation on old tasks and the adaptation to a new one within a given model. Approaches that combine both objectives in training have been explored in previous works. Yet the performance still suffers from considerable degradation in a long sequence of tasks. In this work, we propose a novel approach to lifelong learning, which tries to seek a better balance between preservation and adaptation via two techniques: Distillation and Retrospection. Specifically, the target model adapts to the new task by knowledge distillation from an intermediate expert, while the previous knowledge is more effectively preserved by caching a small subset of data for old tasks. The combination of Distillation and Retrospection leads to a more gentle learning curve for the target model, and extensive experiments demonstrate that our approach can bring consistent improvements on both old and new tasks (Project page: http://mmlab.ie.cuhk.edu.hk/projects/lifelong/).  2018, Springer Nature Switzerland AG.
KW  - Distillation
KW  - Computer vision
U2  - Learning curves
U2  - Life long learning
U2  - Long sequences
U2  - Retrospection
U2  - Target model
DO  - 10.1007/978-3-030-01219-9_27
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_27
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Cheng, Xinjing
A1  - Wang, Peng
A1  - Yang, Ruigang
AD  - Baidu Research, Baidu Inc., Beijing, China
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978755
SP  - 108
EP  - 125
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Depth estimation from a single image is a fundamental problem in computer vision. In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for depth prediction. Specifically, we adopt an efficient linear propagation model, where the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). We apply the designed CSPN to two depth estimation tasks given a single image: (1) Refine the depth output from existing state-of-the-art (SOTA) methods; (2) Convert sparse depth samples to a dense depth map by embedding the depth samples within the propagation procedure. The second task is inspired by the availability of LiDAR that provides sparse but accurate depth measurements. We experimented the proposed CSPN over the popular NYU v2 [1] and KITTI [2] datasets, where we show that our proposed approach improves not only quality (e.g., 30% more reduction in depth error), but also speed (e.g., 2 to 5 faster) of depth maps than previous SOTA methods. The codes of CSPN are available at: https://github.com/XinJCheng/CSPN.  2018, Springer Nature Switzerland AG.
KW  - Convolution
KW  - Computer vision
KW  - Deep neural networks
KW  - Neural networks
U2  - Affinity matrix
U2  - Deep convolutional neural networks
U2  - Dense depth map
U2  - Depth Estimation
U2  - Depth measurements
U2  - Linear propagation
U2  - Spatial propagation
U2  - State of the art
DO  - 10.1007/978-3-030-01270-0_7
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_7
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - SOD-MTGAN: Small object detection via multi-task generative adversarial network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Bai, Yancheng
A1  - Zhang, Yongqiang
A1  - Ding, Mingli
A1  - Ghanem, Bernard
AD  - Visual Computing Center, King Abdullah University of Science and Technology, Thuwal, Saudi ArabiaInstitute of Software, Chinese Academy of Sciences (CAS), Beijing, ChinaSchool of Electrical Engineering and Automation, Harbin Institute of Technology, Harbin, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008932
SP  - 210
EP  - 226
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Object detection is a fundamental and important problem in computer vision. Although impressive results have been achieved on large/medium sized objects in large-scale detection benchmarks (e.g. the COCO dataset), the performance on small objects is far from satisfactory. The reason is that small objects lack sufficient detailed appearance information, which can distinguish them from the background or similar objects. To deal with the small object detection problem, we propose an end-to-end multi-task generative adversarial network (MTGAN). In the MTGAN, the generator is a super-resolution network, which can up-sample small blurred images into fine-scale ones and recover detailed information for more accurate detection. The discriminator is a multi-task network, which describes each super-resolved image patch with a real/fake score, object category scores, and bounding box regression offsets. Furthermore, to make the generator recover more details for easier detection, the classification and regression losses in the discriminator are back-propagated into the generator during training. Extensive experiments on the challenging COCO dataset demonstrate the effectiveness of the proposed method in restoring a clear super-resolved image from a blurred small one, and show that the detection performance, especially for small sized objects, improves over state-of-the-art methods.  Springer Nature Switzerland AG 2018.
KW  - Object detection
KW  - Benchmarking
KW  - Computer vision
KW  - Image enhancement
KW  - Object recognition
KW  - Optical resolving power
U2  - Adversarial networks
U2  - COCO
U2  - Detection performance
U2  - Image patches
U2  - Object categories
U2  - Small object detection
U2  - State-of-the-art methods
U2  - Super resolution
DO  - 10.1007/978-3-030-01261-8_13
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_13
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - 3D face reconstruction from light field images: A model-free approach
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Feng, Mingtao
A1  - Gilani, Syed Zulqarnain
A1  - Wang, Yaonan
A1  - Mian, Ajmal
AD  - College of Electrical and Information Engineering, Hunan University, Changsha; 410006, ChinaComputer Science and Software Engineering, The University of Western Australia, Perth; 6009, Australia
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978788
SP  - 508
EP  - 526
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Reconstructing 3D facial geometry from a single RGB image has recently instigated wide research interest. However, it is still an ill-posed problem and most methods rely on prior models hence undermining the accuracy of the recovered 3D faces. In this paper, we exploit the Epipolar Plane Images (EPI) obtained from light field cameras and learn CNN models that recover horizontal and vertical 3D facial curves from the respective horizontal and vertical EPIs. Our 3D face reconstruction network (FaceLFnet) comprises a densely connected architecture to learn accurate 3D facial curves from low resolution EPIs. To train the proposed FaceLFnets from scratch, we synthesize photo-realistic light field images from 3D facial scans. The curve by curve 3D face estimation approach allows the networks to learn from only 14K images of 80 identities, which still comprises over 11 Million EPIs/curves. The estimated facial curves are merged into a single pointcloud to which a surface is fitted to get the final 3D face. Our method is model-free, requires only a few training samples to learn FaceLFnet and can reconstruct 3D faces with high accuracy from single light field images under varying poses, expressions and lighting conditions. Comparison on the BU-3DFE and BU-4DFE datasets show that our method reduces reconstruction errors by over 20% compared to recent state of the art.  Springer Nature Switzerland AG 2018.
KW  - Image reconstruction
KW  - Computer vision
KW  - Three dimensional computer graphics
U2  - 3D face reconstruction
U2  - Epipolar plane images
U2  - Estimation approaches
U2  - Ill posed problem
U2  - Lighting conditions
U2  - Photo-realistic
U2  - Reconstruction error
U2  - Research interests
DO  - 10.1007/978-3-030-01249-6_31
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_31
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep bilinear learning for RGB-D action recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Hu, Jian-Fang
A1  - Zheng, Wei-Shi
A1  - Pan, Jiahui
A1  - Lai, Jianhuang
A1  - Zhang, Jianguo
AD  - Sun Yat-sen University, Guangzhou, ChinaUniversity of Dundee, Dundee, United KingdomKey Laboratory of Machine Intelligence and Advanced Computing, MOE, Guangzhou, ChinaInception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977597
SP  - 346
EP  - 362
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we focus on exploring modality-temporal mutual information for RGB-D action recognition. In order to learn time-varying information and multi-modal features jointly, we propose a novel deep bilinear learning framework. In the framework, we propose bilinear blocks that consist of two linear pooling layers for pooling the input cube features from both modality and temporal directions, separately. To capture rich modality-temporal information and facilitate our deep bilinear learning, a new action feature called modality-temporal cube is presented in a tensor structure for characterizing RGB-D actions from a comprehensive perspective. Our method is extensively tested on two public datasets with four different evaluation settings, and the results show that the proposed method outperforms the state-of-the-art approaches.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Computer vision
KW  - Geometry
U2  - Cube
U2  - Deep bilinear
U2  - Feature learning
U2  - Learning frameworks
U2  - RGB-D action
U2  - State-of-the-art approach
U2  - Temporal information
U2  - Time-varying information
DO  - 10.1007/978-3-030-01234-2_21
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_21
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Liquid Pouring Monitoring via Rich Sensory Inputs
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wu, Tz-Ying
A1  - Lin, Juan-Ting
A1  - Wang, Tsun-Hsuang
A1  - Hu, Chan-Wei
A1  - Niebles, Juan Carlos
A1  - Sun, Min
AD  - Department of Electrical Engineering, National Tsing Hua University, Hsinchu, TaiwanDepartment of Computer Science, Stanford University, Stanford, United States
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978877
SP  - 352
EP  - 369
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Humans have the amazing ability to perform very subtle manipulation task using a closed-loop control system with imprecise mechanics (i.e., our body parts) but rich sensory information (e.g., vision, tactile, etc.). In the closed-loop system, the ability to monitor the state of the task via rich sensory information is important but often less studied. In this work, we take liquid pouring as a concrete example and aim at learning to continuously monitor whether liquid pouring is successful (e.g., no spilling) or not via rich sensory inputs. We mimic humans rich sensories using synchronized observation from a chest-mounted camera and a wrist-mounted IMU sensor. Given many success and failure demonstrations of liquid pouring, we train a hierarchical LSTM with late fusion for monitoring. To improve the robustness of the system, we propose two auxiliary tasks during training: inferring (1) the initial state of containers and (2) forecasting the one-step future 3D trajectory of the hand with an adversarial training procedure. These tasks encourage our method to learn representation sensitive to container states and how objects are manipulated in 3D. With these novel components, our method achieves  8% and  11% better monitoring accuracy than the baseline method without auxiliary tasks on unseen containers and unseen users respectively.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Closed loop control systems
KW  - Closed loop systems
KW  - Containers
KW  - Liquids
KW  - Long short-term memory
U2  - Auxiliary tasks
U2  - Baseline methods
U2  - Manipulation task
U2  - Monitoring accuracy
U2  - Multi-modal fusion
U2  - Novel component
U2  - Sensory information
U2  - Training procedures
DO  - 10.1007/978-3-030-01252-6_21
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_21
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Does haze removal help CNN-based image classification?
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Pei, Yanting
A1  - Huang, Yaping
A1  - Zou, Qi
A1  - Lu, Yuhang
A1  - Wang, Song
AD  - Beijing Key Laboratory of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, ChinaDepartment of Computer Science and Engineering, University of South Carolina, Columbia; SC, United StatesSchool of Computer Science and Technology, Tianjin University, Tianjin, China
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978800
SP  - 697
EP  - 712
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Hazy images are common in real scenarios and many dehazing methods have been developed to automatically remove the haze from images. Typically, the goal of image dehazing is to produce clearer images from which human vision can better identify the object and structural details present in the images. When the ground-truth haze-free image is available for a hazy image, quantitative evaluation of image dehazing is usually based on objective metrics, such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM). However, in many applications, large-scale images are collected not for visual examination by human. Instead, they are used for many high-level vision tasks, such as automatic classification, recognition and categorization. One fundamental problem here is whether various dehazing methods can produce clearer images that can help improve the performance of the high-level tasks. In this paper, we empirically study this problem in the important task of image classification by using both synthetic and real hazy image datasets. From the experimental results, we find that the existing image-dehazing methods cannot improve much the image-classification performance and sometimes even reduce the image-classification performance.  Springer Nature Switzerland AG 2018.
KW  - Image classification
KW  - Classification (of information)
KW  - Computer vision
KW  - Demulsification
KW  - Image enhancement
KW  - Signal to noise ratio
U2  - Automatic classification
U2  - Classification accuracy
U2  - Classification performance
U2  - Dehazing
U2  - Haze removal
U2  - Hazy images
U2  - Peak Signal to Noise Ratio (PSNR)
U2  - Quantitative evaluation
DO  - 10.1007/978-3-030-01249-6_42
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_42
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - DeepKSPD: Learning Kernel-Matrix-Based SPD Representation For Fine-Grained Image Recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Engin, Melih
A1  - Wang, Lei
A1  - Zhou, Luping
A1  - Liu, Xinwang
AD  - School of Computing and Information Technology, University of Wollongong, Wollongong; NSW; 2500, AustraliaSchool of Electrical and Information Engineering, University of Sydney, Sydney; NSW; 2006, AustraliaSchool of Computer, National University of Defense Technology Changsha, Hunan; 410073, China
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005986
SP  - 629
EP  - 645
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - As a second-order pooled representation, covariance matrix has attracted much attention in visual recognition, and some pioneering works have recently integrated it into deep learning. A recent study shows that kernel matrix works considerably better than covariance matrix for this kind of representation, by modeling the higher-order, nonlinear relationship among pooled visual descriptors. Nevertheless, in that study neither the descriptors nor the kernel matrix is deeply learned. Worse, they are considered separately, hindering the pursuit of an optimal representation. To improve this situation, this work designs a deep network that jointly learns local descriptors and kernel-matrix-based pooled representation in an end-to-end manner. The derivatives for the mapping from a local descriptor set to this representation are derived to carry out backpropagation. More importantly, we introduce the Dalecki-Kren formula from Operator theory to give a concise and unified result on differentiating general functions defined on symmetric positive-definite (SPD) matrix, which shows its better numerical stability in conducting backpropagation compared with the existing method when handling the Riemannian geometry of SPD matrix. Experiments on fine-grained image benchmark datasets not only show the superiority of kernel-matrix-based SPD representation with deep local descriptors, but also verify the advantage of the proposed deep network in pursuing better SPD representations. Also, ablation study is provided to explain why and from where these improvements are attained.  2018, Springer Nature Switzerland AG.
KW  - Covariance matrix
KW  - Backpropagation
KW  - Computer vision
KW  - Deep learning
KW  - Geometry
KW  - Image recognition
KW  - Job analysis
KW  - Numerical methods
U2  - Benchmark datasets
U2  - Fine grained
U2  - Kernel matrices
U2  - Non-linear relationships
U2  - Riemannian geometry
U2  - SPD representation
U2  - Symmetric positive definite
U2  - Visual recognition
DO  - 10.1007/978-3-030-01216-8_38
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_38
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning human-object interactions by graph parsing neural networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Qi, Siyuan
A1  - Wang, Wenguan
A1  - Jia, Baoxiong
A1  - Shen, Jianbing
A1  - Zhu, Song-Chun
AD  - University of California, Los Angeles, Los Angeles, United StatesInternational Center for AI and Robot Autonomy (CARA), Los Angeles, United StatesBeijing Institute of Technology, Beijing, ChinaPeking University, Beijing, ChinaInception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977497
SP  - 407
EP  - 423
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper addresses the task of detecting and recognizing human-object interactions (HOI) in images and videos. We introduce the Graph Parsing Neural Network (GPNN), a framework that incorporates structural knowledge while being differentiable end-to-end. For a given scene, GPNN infers a parse graph that includes (i) the HOI graph structure represented by an adjacency matrix, and (ii) the node labels. Within a message passing inference framework, GPNN iteratively computes the adjacency matrices and node labels. We extensively evaluate our model on three HOI detection benchmarks on images and videos: HICO-DET, V-COCO, and CAD-120 datasets. Our approach significantly outperforms state-of-art methods, verifying that GPNN is scalable to large datasets and applies to spatial-temporal settings.  Springer Nature Switzerland AG 2018.
KW  - Message passing
KW  - Computer aided design
KW  - Computer vision
KW  - Iterative methods
KW  - Neural networks
KW  - Object detection
KW  - Visual languages
U2  - Adjacency matrices
U2  - Graph parsing
U2  - Graph structures
U2  - Human-object interaction
U2  - Large datasets
U2  - Spatial temporals
U2  - State-of-art methods
U2  - Structural knowledge
DO  - 10.1007/978-3-030-01240-3_25
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_25
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Neural Stereoscopic Image Style Transfer
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Gong, Xinyu
A1  - Huang, Haozhi
A1  - Ma, Lin
A1  - Shen, Fumin
A1  - Liu, Wei
A1  - Zhang, Tong
AD  - Tencent AI Lab, Shenzhen, ChinaUniversity of Electronic Science and Technology of China, Chengdu, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976877
SP  - 56
EP  - 71
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Neural style transfer is an emerging technique which is able to endow daily-life images with attractive artistic styles. Previous work has succeeded in applying convolutional neural networks (CNNs) to style transfer for monocular images or videos. However, style transfer for stereoscopic images is still a missing piece. Different from processing a monocular image, the two views of a stylized stereoscopic pair are required to be consistent to provide observers a comfortable visual experience. In this paper, we propose a novel dual path network for view-consistent style transfer on stereoscopic images. While each view of the stereoscopic pair is processed in an individual path, a novel feature aggregation strategy is proposed to effectively share information between the two paths. Besides a traditional perceptual loss being used for controlling the style transfer quality in each view, a multi-layer view loss is leveraged to enforce the network to coordinate the learning of both the paths to generate view-consistent stylized results. Extensive experiments show that, compared against previous methods, our proposed model can produce stylized stereoscopic images which achieve decent view consistency.  2018, Springer Nature Switzerland AG.
KW  - Stereo image processing
KW  - Computer vision
KW  - Neural networks
U2  - Convolutional neural network
U2  - Feature aggregation
U2  - Individual paths
U2  - Neural style transfer
U2  - Stereoscopic image
U2  - Stereoscopic pair
U2  - View consistency
U2  - Visual experiences
DO  - 10.1007/978-3-030-01228-1_4
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_4
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - A trilateral weighted sparse coding scheme for real-world image denoising
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Xu, Jun
A1  - Zhang, Lei
A1  - Zhang, David
AD  - The Hong Kong Polytechnic University, Kowloon, Hong KongSchool of Science and Engineering, The Chinese University of Hong Kong (Shenzhen), Shenzhen, China
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006054
SP  - 21
EP  - 38
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Most of existing image denoising methods assume the corrupted noise to be additive white Gaussian noise (AWGN). However, the realistic noise in real-world noisy images is much more complex than AWGN, and is hard to be modeled by simple analytical distributions. As a result, many state-of-the-art denoising methods in literature become much less effective when applied to real-world noisy images captured by CCD or CMOS cameras. In this paper, we develop a trilateral weighted sparse coding (TWSC) scheme for robust real-world image denoising. Specifically, we introduce three weight matrices into the data and regularization terms of the sparse coding framework to characterize the statistics of realistic noise and image priors. TWSC can be reformulated as a linear equality-constrained problem and can be solved by the alternating direction method of multipliers. The existence and uniqueness of the solution and convergence of the proposed algorithm are analyzed. Extensive experiments demonstrate that the proposed TWSC scheme outperforms state-of-the-art denoising methods on removing realistic noise.  Springer Nature Switzerland AG 2018.
KW  - Image denoising
KW  - Codes (symbols)
KW  - Computer vision
KW  - Constraint theory
KW  - Gaussian noise (electronic)
KW  - Image coding
KW  - White noise
U2  - Additive White Gaussian noise
U2  - Alternating direction method of multipliers
U2  - De-noising
U2  - Denoising methods
U2  - Existence and uniqueness
U2  - Image denoising methods
U2  - Regularization terms
U2  - Sparse coding
DO  - 10.1007/978-3-030-01237-3_2
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_2
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Long, Shangbang
A1  - Ruan, Jiaqiang
A1  - Zhang, Wenjie
A1  - He, Xin
A1  - Wu, Wenhao
A1  - Yao, Cong
AD  - Peking University, Beijing, ChinaMegvii (Face++) Technology Inc., Beijing, China
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005966
SP  - 19
EP  - 35
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Driven by deep neural networks and large scale datasets, scene text detection methods have progressed substantially over the past years, continuously refreshing the performance records on various standard benchmarks. However, limited by the representations (axis-aligned rectangles, rotated rectangles or quadrangles) adopted to describe text, existing methods may fall short when dealing with much more free-form text instances, such as curved text, which are actually very common in real-world scenarios. To tackle this problem, we propose a more flexible representation for scene text, termed as TextSnake, which is able to effectively represent text instances in horizontal, oriented and curved forms. In TextSnake, a text instance is described as a sequence of ordered, overlapping disks centered at symmetric axes, each of which is associated with potentially variable radius and orientation. Such geometry attributes are estimated via a Fully Convolutional Network (FCN) model. In experiments, the text detector based on TextSnake achieves state-of-the-art or comparable performance on Total-Text and SCUT-CTW1500, the two newly published benchmarks with special emphasis on curved text in natural images, as well as the widely-used datasets ICDAR 2015 and MSRA-TD500. Specifically, TextSnake outperforms the baseline on Total-Text by more than 40% in F-measure.  2018, Springer Nature Switzerland AG.
KW  - Deep neural networks
KW  - Benchmarking
KW  - Computer vision
KW  - Geometry
U2  - Convolutional networks
U2  - Curved text
U2  - Geometry attribute
U2  - Large-scale datasets
U2  - Real-world scenario
U2  - Scene Text
U2  - State of the art
U2  - Variable radius
DO  - 10.1007/978-3-030-01216-8_2
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_2
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Show, tell and discriminate: Image captioning by self-retrieval with partially labeled data
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Xihui
A1  - Li, Hongsheng
A1  - Shao, Jing
A1  - Chen, Dapeng
A1  - Wang, Xiaogang
AD  - The Chinese University of Hong Kong, Hong KongSenseTime Research, Hong Kong
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006170
SP  - 353
EP  - 369
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The aim of image captioning is to generate captions by machine to describe image contents. Despite many efforts, generating discriminative captions for images remains non-trivial. Most traditional approaches imitate the language structure patterns, thus tend to fall into a stereotype of replicating frequent phrases or sentences and neglect unique aspects of each image. In this work, we propose an image captioning framework with a self-retrieval module as training guidance, which encourages generating discriminative captions. It brings unique advantages: (1) the self-retrieval guidance can act as a metric and an evaluator of caption discriminativeness to assure the quality of generated captions. (2) The correspondence between generated captions and images are naturally incorporated in the generation process without human annotations, and hence our approach could utilize a large amount of unlabeled images to boost captioning performance with no additional annotations. We demonstrate the effectiveness of the proposed retrieval-guided method on COCO and Flickr30k captioning datasets, and show its superior captioning performance with more discriminative captions.  Springer Nature Switzerland AG 2018.
KW  - Image retrieval
KW  - Computer vision
U2  - Generation process
U2  - Human annotations
U2  - Image captioning
U2  - Image content
U2  - Labeled data
U2  - Language structure
U2  - Large amounts
U2  - Traditional approaches
DO  - 10.1007/978-3-030-01267-0_21
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_21
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning and matching multi-view descriptors for registration of point clouds
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhou, Lei
A1  - Zhu, Siyu
A1  - Luo, Zixin
A1  - Shen, Tianwei
A1  - Zhang, Runze
A1  - Zhen, Mingmin
A1  - Fang, Tian
A1  - Quan, Long
AD  - Hong Kong University of Science and Technology, Hong KongShenzhen Zhuke Innovation Technology (Altizure), Shenzhen, China
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006181
SP  - 527
EP  - 544
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Critical to the registration of point clouds is the establishment of a set of accurate correspondences between points in 3D space. The correspondence problem is generally addressed by the design of discriminative 3D local descriptors on the one hand, and the development of robust matching strategies on the other hand. In this work, we first propose a multi-view local descriptor, which is learned from the images of multiple views, for the description of 3D keypoints. Then, we develop a robust matching approach, aiming at rejecting outlier matches based on the efficient inference via belief propagation on the defined graphical model. We have demonstrated the boost of our approaches to registration on the public scanning and multi-view stereo datasets. The superior performance has been verified by the intensive comparisons against a variety of descriptors and matching methods.  Springer Nature Switzerland AG 2018.
KW  - Stereo image processing
KW  - Computer vision
KW  - Image segmentation
U2  - 3d descriptor
U2  - Belief propagation
U2  - Correspondence problems
U2  - Local descriptors
U2  - Matching methods
U2  - Multi-view stereo
U2  - Point cloud registration
U2  - Robust matching
DO  - 10.1007/978-3-030-01267-0_31
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_31
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Fine-grained visual categorization using meta-learning optimization with sample selection of auxiliary data
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Yabin
A1  - Tang, Hui
A1  - Jia, Kui
AD  - School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006049
SP  - 241
EP  - 256
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Fine-grained visual categorization (FGVC) is challenging due in part to the fact that it is often difficult to acquire an enough number of training samples. To employ large models for FGVC without suffering from overfitting, existing methods usually adopt a strategy of pre-training the models using a rich set of auxiliary data, followed by fine-tuning on the target FGVC task. However, the objective of pre-training does not take the target task into account, and consequently such obtained models are suboptimal for fine-tuning. To address this issue, we propose in this paper a new deep FGVC model termed MetaFGNet. Training of MetaFGNet is based on a novel regularized meta-learning objective, which aims to guide the learning of network parameters so that they are optimal for adapting to the target FGVC task. Based on MetaFGNet, we also propose a simple yet effective scheme for selecting more useful samples from the auxiliary data. Experiments on benchmark FGVC datasets show the efficacy of our proposed method.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Salinity measurement
U2  - Auxiliary data
U2  - Fine grained
U2  - Metalearning
U2  - Network parameters
U2  - Pre-training
U2  - Sample selection
U2  - Training sample
U2  - Visual categorization
DO  - 10.1007/978-3-030-01237-3_15
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_15
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Person re-identification with deep similarity-guided graph neural network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Shen, Yantao
A1  - Li, Hongsheng
A1  - Yi, Shuai
A1  - Chen, Dapeng
A1  - Wang, Xiaogang
AD  - CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Hong KongSenseTime Research, Hong Kong
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006180
SP  - 508
EP  - 526
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The person re-identification task requires to robustly estimate visual similarities between person images. However, existing person re-identification models mostly estimate the similarities of different image pairs of probe and gallery images independently while ignores the relationship information between different probe-gallery pairs. As a result, the similarity estimation of some hard samples might not be accurate. In this paper, we propose a novel deep learning framework, named Similarity-Guided Graph Neural Network (SGGNN) to overcome such limitations. Given a probe image and several gallery images, SGGNN creates a graph to represent the pairwise relationships between probe-gallery pairs (nodes) and utilizes such relationships to update the probe-gallery relation features in an end-to-end manner. Accurate similarity estimation can be achieved by using such updated probe-gallery relation features for prediction. The input features for nodes on the graph are the relation features of different probe-gallery image pairs. The probe-gallery relation feature updating is then performed by the messages passing in SGGNN, which takes other nodes information into account for similarity estimation. Different from conventional GNN approaches, SGGNN learns the edge weights with rich labels of gallery instance pairs directly, which provides relation fusion more precise information. The effectiveness of our proposed method is validated on three public person re-identification datasets.  Springer Nature Switzerland AG 2018.
KW  - Graph theory
KW  - Computer vision
KW  - Deep learning
KW  - Probes
U2  - Edge weights
U2  - Feature updating
U2  - Graph neural networks
U2  - Input features
U2  - Learning frameworks
U2  - Person re identifications
U2  - Similarity estimation
U2  - Visual similarity
DO  - 10.1007/978-3-030-01267-0_30
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_30
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Chao
A1  - Zheng, Haiyong
A1  - Yu, Zhibin
A1  - Zheng, Ziqiang
A1  - Gu, Zhaorui
A1  - Zheng, Bing
AD  - Ocean University of China, Qingdao; 266100, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977727
SP  - 796
EP  - 812
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Image-to-image translation has been made much progress with embracing Generative Adversarial Networks (GANs). However, its still very challenging for translation tasks that require high quality, especially at high-resolution and photorealism. In this paper, we present Discriminative Region Proposal Adversarial Networks (DRPAN) for high-quality image-to-image translation. We decompose the procedure of image-to-image translation task into three iterated steps, first is to generate an image with global structure but some local artifacts (via GAN), second is using our DRPnet to propose the most fake region from the generated image, and third is to implement image inpainting on the most fake region for more realistic result through a reviser, so that the system (DRPAN) can be gradually optimized to synthesize images with more attention on the most artifact local part. Experiments on a variety of image-to-image translation tasks and datasets validate that our method outperforms state-of-the-arts for producing high-quality translation results in terms of both human perceptual studies and automatic quantitative measures.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Artificial intelligence
KW  - Computer science
KW  - Computers
U2  - Adversarial networks
U2  - DRPAN
U2  - Global structure
U2  - High quality images
U2  - Image Inpainting
U2  - Image translation
U2  - Quantitative measures
U2  - State of the art
DO  - 10.1007/978-3-030-01246-5_47
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_47
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Exploring visual relationship for image captioning
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yao, Ting
A1  - Pan, Yingwei
A1  - Li, Yehao
A1  - Mei, Tao
AD  - JD AI Research, Beijing, ChinaSun Yat-sen University, Guangzhou, China
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020331
SP  - 711
EP  - 727
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - It is always well believed that modeling relationships between objects would be helpful for representing and eventually describing an image. Nevertheless, there has not been evidence in support of the idea on image description generation. In this paper, we introduce a new design to explore the connections between objects for image captioning under the umbrella of attention-based encoder-decoder framework. Specifically, we present Graph Convolutional Networks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that novelly integrates both semantic and spatial object relationships into image encoder. Technically, we build graphs over the detected objects in an image based on their spatial and semantic connections. The representations of each region proposed on objects are then refined by leveraging graph structure through GCN. With the learnt region-level features, our GCN-LSTM capitalizes on LSTM-based captioning framework with attention mechanism for sentence generation. Extensive experiments are conducted on COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1% to 128.7% on COCO testing set.  2018, Springer Nature Switzerland AG.
KW  - Long short-term memory
KW  - Brain
KW  - Computer vision
KW  - Convolution
KW  - Object detection
KW  - Semantics
KW  - Signal encoding
U2  - Attention mechanisms
U2  - Convolutional networks
U2  - Graph structures
U2  - Image captioning
U2  - Image descriptions
U2  - Model relationships
U2  - State-of-the-art approach
U2  - Visual relationship
DO  - 10.1007/978-3-030-01264-9_42
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_42
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - DPP-Net: Device-Aware Progressive Search for Pareto-Optimal Neural Architectures
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Dong, Jin-Dong
A1  - Cheng, An-Chieh
A1  - Juan, Da-Cheng
A1  - Wei, Wei
A1  - Sun, Min
AD  - National Tsing -Hua University, Hsinchu, TaiwanGoogle, Mountain View; CA, United States
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978889
SP  - 540
EP  - 555
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recent breakthroughs in Neural Architectural Search (NAS) have achieved state-of-the-art performances in applications such as image classification and language modeling. However, these techniques typically ignore device-related objectives such as inference time, memory usage, and power consumption. Optimizing neural architecture for device-related objectives is immensely crucial for deploying deep networks on portable devices with limited computing resources. We propose DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures, optimizing for both device-related (e.g., inference time and memory usage) and device-agnostic (e.g., accuracy and model size) objectives. DPP-Net employs a compact search space inspired by current state-of-the-art mobile CNNs, and further improves search efficiency by adopting progressive search (Liu et al. 2017). Experimental results on CIFAR-10 are poised to demonstrate the effectiveness of Pareto-optimal networks found by DPP-Net, for three different devices: (1) a workstation with Titan X GPU, (2) NVIDIA Jetson TX1 embedded system, and (3) mobile phone with ARM Cortex-A53. Compared to CondenseNet and NASNet (Mobile), DPP-Net achieves better performances: higher accuracy  shorter inference time on various devices. Additional experimental results show that models found by DPP-Net also achieve considerably-good performance on ImageNet as well.  2018, Springer Nature Switzerland AG.
KW  - Pareto principle
KW  - Cellular telephone systems
KW  - Computer vision
KW  - Memory architecture
KW  - Modeling languages
KW  - Multiobjective optimization
KW  - Network architecture
KW  - Neural networks
U2  - Computing resource
U2  - Language model
U2  - Neural architectures
U2  - Pareto-optimal
U2  - Portable device
U2  - Search efficiency
U2  - State of the art
U2  - State-of-the-art performance
DO  - 10.1007/978-3-030-01252-6_32
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_32
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - ICNet for Real-Time Semantic Segmentation on High-Resolution Images
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhao, Hengshuang
A1  - Qi, Xiaojuan
A1  - Shen, Xiaoyong
A1  - Shi, Jianping
A1  - Jia, Jiaya
AD  - The Chinese University of Hong Kong, Shatin, Hong KongTencent Youtu Lab, Shenzhen, ChinaSenseTime Research, Beijing, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977813
SP  - 418
EP  - 434
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.  2018, Springer Nature Switzerland AG.
KW  - Image segmentation
KW  - Computer vision
KW  - Quality control
KW  - Semantics
U2  - High resolution
U2  - High resolution image
U2  - High-quality segmentation
U2  - In-depth analysis
U2  - Real time
U2  - Real-time inference
U2  - Real-time semantics
U2  - Semantic segmentation
DO  - 10.1007/978-3-030-01219-9_25
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_25
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Saliency detection in 360° Videos
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Ziheng
A1  - Xu, Yanyu
A1  - Yu, Jingyi
A1  - Gao, Shenghua
AD  - ShanghaiTech University, Shanghai, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977607
SP  - 504
EP  - 520
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper presents a novel spherical convolutional neural network based scheme for saliency detection for 360 videos. Specifically, in our spherical convolution neural network definition, kernel is defined on a spherical crown, and the convolution involves the rotation of the kernel along the sphere. Considering that the 360 videos are usually stored with equirectangular panorama, we propose to implement the spherical convolution on panorama by stretching and rotating the kernel based on the location of patch to be convolved. Compared with existing spherical convolution, our definition has the parameter sharing property, which would greatly reduce the parameters to be learned. We further take the temporal coherence of the viewing process into consideration, and propose a sequential saliency detection by leveraging a spherical U-Net. To validate our approach, we construct a large-scale 360 videos saliency detection benchmark that consists of 104 360 videos viewed by 20+ human subjects. Comprehensive experiments validate the effectiveness of our spherical U-net for 360 video saliency detection.  Springer Nature Switzerland AG 2018.
KW  - Spheres
KW  - Computer vision
KW  - Convolution
KW  - Neural networks
U2  - Convolution neural network
U2  - Convolutional neural network
U2  - Equirectangular panoramas
U2  - Parameter sharing
U2  - Saliency detection
U2  - Spherical crown
U2  - Temporal coherence
U2  - Video saliencies
DO  - 10.1007/978-3-030-01234-2_30
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_30
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - PS-FCN: A flexible learning framework for photometric stereo
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Guanying
A1  - Han, Kai
A1  - Wong, Kwan-Yee K.
AD  - The University of Hong Kong, Pokfulam, Hong KongUniversity of Oxford, Oxford, United Kingdom
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977480
SP  - 3
EP  - 19
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper addresses the problem of photometric stereo for non-Lambertian surfaces. Existing approaches often adopt simplified reflectance models to make the problem more tractable, but this greatly hinders their applications on real-world objects. In this paper, we propose a deep fully convolutional network, called PS-FCN, that takes an arbitrary number of images of a static object captured under different light directions with a fixed camera as input, and predicts a normal map of the object in a fast feed-forward pass. Unlike the recently proposed learning based method, PS-FCN does not require a pre-defined set of light directions during training and testing, and can handle multiple images and light directions in an order-agnostic manner. Although we train PS-FCN on synthetic data, it can generalize well on real datasets. We further show that PS-FCN can be easily extended to handle the problem of uncalibrated photometric stereo. Extensive experiments on public real datasets show that PS-FCN outperforms existing approaches in calibrated photometric stereo, and promising results are achieved in uncalibrated scenario, clearly demonstrating its effectiveness.  Springer Nature Switzerland AG 2018.
KW  - Stereo image processing
KW  - Computer vision
KW  - Convolution
KW  - Neural networks
KW  - Photometry
U2  - Convolutional networks
U2  - Convolutional neural network
U2  - Learning-based methods
U2  - Non-lambertian surfaces
U2  - Photometric stereo
U2  - Real-world objects
U2  - Reflectance model
U2  - Training and testing
DO  - 10.1007/978-3-030-01240-3_1
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_1
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Adversarial Open-World Person Re-Identification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Xiang
A1  - Wu, Ancong
A1  - Zheng, Wei-Shi
AD  - Sun Yat-sen University, Guangzhou, ChinaInception Institute of Artificial Intelligence, Abu Dhabi, United Arab EmiratesKey Laboratory of Machine Intelligence and Advanced Computing, MOE, Guangzhou, China
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005964
SP  - 287
EP  - 303
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In a typical real-world application of re-id, a watch-list (gallery set) of a handful of target people (e.g. suspects) to track around a large volume of non-target people are demanded across camera views, and this is called the open-world person re-id. Different from conventional (closed-world) person re-id, a large portion of probe samples are not from target people in the open-world setting. And, it always happens that a non-target person would look similar to a target one and therefore would seriously challenge a re-id system. In this work, we introduce a deep open-world group-based person re-id model based on adversarial learning to alleviate the attack problem caused by similar non-target people. The main idea is learning to attack feature extractor on the target people by using GAN to generate very target-like images (imposters), and in the meantime the model will make the feature extractor learn to tolerate the attack by discriminative learning so as to realize group-based verification. The framework we proposed is called the adversarial open-world person re-identification, and this is realized by our Adversarial PersonNet (APN) that jointly learns a generator, a person discriminator, a target discriminator and a feature extractor, where the feature extractor and target discriminator share the same weights so as to makes the feature extractor learn to tolerate the attack by imposters for better group-based verification. While open-world person re-id is challenging, we show for the first time that the adversarial-based approach helps stabilize person re-id system under imposter attack more effectively.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Artificial intelligence
KW  - Computer science
KW  - Computers
U2  - Adversarial learning
U2  - Camera view
U2  - Discriminative learning
U2  - Feature extractor
U2  - Group-based
U2  - Large volumes
U2  - Model-based OPC
U2  - Person re identifications
DO  - 10.1007/978-3-030-01216-8_18
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_18
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - TS 2 C: Tight Box Mining with Surrounding Segmentation Context for Weakly Supervised Object Detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wei, Yunchao
A1  - Shen, Zhiqiang
A1  - Cheng, Bowen
A1  - Shi, Honghui
A1  - Xiong, Jinjun
A1  - Feng, Jiashi
A1  - Huang, Thomas
AD  - University of Illinois at UrbanaChampaign, Urbana; IL, United StatesFudan University, Shanghai, ChinaIBM T.J. Watson Research Center, Yorktown Heights, United StatesNational University of Singapore, Singapore, Singapore
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978883
SP  - 454
EP  - 470
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This work provides a simple approach to discover tight object bounding boxes with only image-level supervision, called Tight box mining with Surrounding Segmentation Context (TS2C). We observe that object candidates mined through current multiple instance learning methods are usually trapped to discriminative object parts, rather than the entire object. TS2C leverages surrounding segmentation context derived from weakly-supervised segmentation to suppress such low-quality distracting candidates and boost the high-quality ones. Specifically, TS2C is developed based on two key properties of desirable bounding boxes: (1) high purity, meaning most pixels in the box are with high object response, and (2) high completeness, meaning the box covers high object response pixels comprehensively. With such novel and computable criteria, more tight candidates can be discovered for learning a better object detector. With TS2C, we obtain 48.0% and 44.4% mAP scores on VOC 2007 and 2012 benchmarks, which are the new state-of-the-arts.  2018, Springer Nature Switzerland AG.
KW  - C (programming language)
KW  - Computer vision
KW  - Image segmentation
KW  - Learning systems
KW  - Object detection
KW  - Object recognition
KW  - Pixels
KW  - Semantics
U2  - Multiple instance learning
U2  - Object detectors
U2  - Semantic segmentation
U2  - Simple approach
U2  - State of the art
U2  - Supervised segmentation
U2  - Through current
U2  - Weakly supervised learning
DO  - 10.1007/978-3-030-01252-6_27
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_27
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Transductive Semi-Supervised Deep Learning Using Min-Max Features
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Shi, Weiwei
A1  - Gong, Yihong
A1  - Ding, Chris
A1  - Ma, Zhiheng
A1  - Tao, Xiaoyu
A1  - Zheng, Nanning
AD  - Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xian, ChinaUniversity of Texas at Arlington, Arlington, United States
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976854
SP  - 311
EP  - 327
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose Transductive Semi-Supervised Deep Learning (TSSDL) method that is effective for training Deep Convolutional Neural Network (DCNN) models. The method applies transductive learning principle to DCNN training, introduces confidence levels on unlabeled image samples to overcome unreliable label estimates on outliers and uncertain samples, and develops the Min-Max Feature (MMF) regularization that encourages DCNN to learn feature descriptors with better between-class separability and within-class compactness. TSSDL method is independent of any DCNN architectures and complementary to the latest Semi-Supervised Learning (SSL) methods. Comprehensive experiments on the benchmark datasets CIFAR10 and SVHN have shown that the DCNN model trained by the proposed TSSDL method can produce image classification accuracies compatible to the state-of-the-art SSL methods, and that combining TSSDL with the Mean Teacher method can produce the best classification accuracies on the two benchmark datasets.  2018, Springer Nature Switzerland AG.
KW  - Deep neural networks
KW  - Classification (of information)
KW  - Computer vision
KW  - Convolution
KW  - Neural networks
KW  - Supervised learning
KW  - Teaching
U2  - Classification accuracy
U2  - Confidence levels
U2  - Deep convolutional neural networks
U2  - Feature descriptors
U2  - Min-max
U2  - Semi-supervised
U2  - Semi-supervised learning (SSL)
U2  - Transductive learning
DO  - 10.1007/978-3-030-01228-1_19
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_19
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Cross-modal ranking with soft consistency and noisy labels for robust RGB-T tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Chenglong
A1  - Zhu, Chengli
A1  - Huang, Yan
A1  - Tang, Jin
A1  - Wang, Liang
AD  - Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, ChinaSchool of Computer Science and Technology, Anhui Univeristy, Hefei, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008971
SP  - 831
EP  - 847
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Due to the complementary benefits of visible (RGB) and thermal infrared (T) data, RGB-T object tracking attracts more and more attention recently for boosting the performance under adverse illumination conditions. Existing RGB-T tracking methods usually localize a target object with a bounding box, in which the trackers or detectors is often affected by the inclusion of background clutter. To address this problem, this paper presents a novel approach to suppress background effects for RGB-T tracking. Our approach relies on a novel cross-modal manifold ranking algorithm. First, we integrate the soft cross-modality consistency into the ranking model which allows the sparse inconsistency to account for the different properties between these two modalities. Second, we propose an optimal query learning method to handle label noises of queries. In particular, we introduce an intermediate variable to represent the optimal labels, and formulate it as a l1 -optimization based sparse learning problem. Moreover, we propose a single unified optimization algorithm to solve the proposed model with stable and efficient convergence behavior. Finally, the ranking results are incorporated into the patch-based object features to address the background effects, and the structured SVM is then adopted to perform RGB-T tracking. Extensive experiments suggest that the proposed approach performs well against the state-of-the-art methods on large-scale benchmark datasets.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Artificial intelligence
KW  - Computer science
KW  - Computers
KW  - Information fusion
U2  - Background clutter
U2  - Convergence behaviors
U2  - Cross modality
U2  - Illumination conditions
U2  - Manifold ranking
U2  - State-of-the-art methods
U2  - Unified optimizations
U2  - Visual Tracking
DO  - 10.1007/978-3-030-01261-8_49
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_49
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Stacked Cross Attention for Image-Text Matching
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Lee, Kuang-Huei
A1  - Chen, Xi
A1  - Hua, Gang
A1  - Hu, Houdong
A1  - He, Xiaodong
AD  - Microsoft AI and Research, Redmond, United StatesJD AI Research, Beijing, China
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004562
SP  - 212
EP  - 228
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment between objects or other salient stuff (e.g. snow, sky, lawn) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language, and makes image-text matching more interpretable. Prior work either simply aggregates the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions, or uses a multi-step attentional process to capture limited number of semantic alignments which is less interpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignments using both image regions and words in a sentence as context and infer image-text similarity. Our approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K, our approach outperforms the current best methods by 22.1% relatively in text retrieval from image query, and 18.2% relatively in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1 using the 5K test set). Code has been made available at: (https://github.com/kuanghuei/SCAN ).  2018, Springer Nature Switzerland AG.
KW  - Image retrieval
KW  - Alignment
KW  - Computer vision
KW  - Image enhancement
KW  - Semantics
U2  - Attention
U2  - Image regions
U2  - Latent semantics
U2  - Multi-modal
U2  - Semantic alignments
U2  - State of the art
U2  - Text retrieval
U2  - Visual semantics
DO  - 10.1007/978-3-030-01225-0_13
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_13
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - ELEGANT: Exchanging latent encodings with GAN for transferring multiple face attributes
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Xiao, Taihong
A1  - Hong, Jiapeng
A1  - Ma, Jinwen
AD  - Department of Information Science, School of Mathematical Sciences and LMAM, Peking University, Beijing; 100871, China
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978766
SP  - 172
EP  - 187
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recent studies on face attribute transfer have achieved great success. A lot of models are able to transfer face attributes with an input image. However, they suffer from three limitations: (1) incapability of generating image by exemplars; (2) being unable to transfer multiple face attributes simultaneously; (3) low quality of generated images, such as low-resolution or artifacts. To address these limitations, we propose a novel model which receives two images of opposite attributes as inputs. Our model can transfer exactly the same type of attributes from one image to another by exchanging certain part of their encodings. All the attributes are encoded in a disentangled manner in the latent space, which enables us to manipulate several attributes simultaneously. Besides, our model learns the residual images so as to facilitate training on higher resolution images. With the help of multi-scale discriminators for adversarial training, it can even generate high-quality images with finer details and less artifacts. We demonstrate the effectiveness of our model on overcoming the above three limitations by comparing with other methods on the CelebA face database. A pytorch implementation is available at https://github.com/Prinsphield/ELEGANT.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Encoding (symbols)
U2  - Adversarial networks
U2  - Attributes disentanglement
U2  - Face attribute transfer
U2  - High quality images
U2  - Higher resolution images
U2  - Image generations
U2  - Low resolution
U2  - Residual images
DO  - 10.1007/978-3-030-01249-6_11
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_11
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Quantized Densely Connected U-Nets for Efficient Landmark Localization
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Tang, Zhiqiang
A1  - Peng, Xi
A1  - Geng, Shijie
A1  - Wu, Lingfei
A1  - Zhang, Shaoting
A1  - Metaxas, Dimitris
AD  - Rutgers University, NB, United StatesBinghamton University, Binghamton, United StatesIBM T. J. Watson, Yorktown Heights, United StatesSenseTime, Beijing, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977809
SP  - 348
EP  - 364
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose quantized densely connected U-Nets for efficient visual landmark localization. The idea is that features of the same semantic meanings are globally reused across the stacked U-Nets. This dense connectivity largely improves the information flow, yielding improved localization accuracy. However, a vanilla dense design would suffer from critical efficiency issue in both training and testing. To solve this problem, we first propose order-K dense connectivity to trim off long-distance shortcuts; then, we use a memory-efficient implementation to significantly boost the training efficiency and investigate an iterative refinement that may slice the model size in half. Finally, to reduce the memory consumption and high precision operations both in training and testing, we further quantize weights, inputs, and gradients of our localization network to low bit-width numbers. We validate our approach in two tasks: human pose estimation and face alignment. The results show that our approach achieves state-of-the-art localization accuracy, but using #x0024;#x0024;\sim #x0024;#x0024; 70% fewer parameters, #x0024;#x0024;\sim #x0024;#x0024; 98% less model size and saving #x0024;#x0024;\sim #x0024;#x0024; 32 #x0024;#x0024;\times #x0024;#x0024; training memory compared with other benchmark localizers.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Efficiency
KW  - Semantics
U2  - High-precision operation
U2  - Human pose estimations
U2  - Iterative refinement
U2  - Landmark localization
U2  - Localization accuracy
U2  - Memory consumption
U2  - Training and testing
U2  - Training efficiency
DO  - 10.1007/978-3-030-01219-9_21
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_21
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Joint Map and Symmetry Synchronization
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Sun, Yifan
A1  - Liang, Zhenxiao
A1  - Huang, Xiangru
A1  - Huang, Qixing
AD  - The University of Texas at Austin, Austin, United StatesTsinghua University, Beijing, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976851
SP  - 257
EP  - 275
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Most existing techniques in map computation (e.g., in the form of feature or dense correspondences) assume that the underlying map between an object pair is unique. This assumption, however, easily breaks when visual objects possess self-symmetries. In this paper, we study the problem of jointly optimizing symmetry groups and pair-wise maps among a collection of symmetric objects. We introduce a lifting map representation for encoding both symmetry groups and maps between symmetry groups. Based on this representation, we introduce a computational framework for joint symmetry and map synchronization. Experimental results show that this approach outperforms state-of-the-art approaches for symmetry detection from a single object as well as joint map optimization among an object collection.  2018, Springer Nature Switzerland AG.
KW  - Object detection
KW  - Computer vision
KW  - Optimization
KW  - Quantum theory
U2  - Computational framework
U2  - Correspondences
U2  - Cycle-consistency
U2  - Dense correspondences
U2  - Map representations
U2  - State-of-the-art approach
U2  - Symmetry detection
U2  - Symmetry groups
DO  - 10.1007/978-3-030-01228-1_16
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_16
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - ML-LocNet: Improving Object Localization with Multi-view Learning Network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Xiaopeng
A1  - Yang, Yang
A1  - Feng, Jiashi
AD  - National University of Singapore, Singapore, SingaporeUniversity of Electronic Science and Technology of China, Chengdu, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977802
SP  - 248
EP  - 263
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper addresses Weakly Supervised Object Localization (WSOL) with only image-level supervision. We propose a Multi-view Learning Localization Network (ML-LocNet) by incorporating multi-view learning into a two-phase WSOL model. The multi-view learning would benefit localization due to the complementary relationships among the learned features from different views and the consensus property among the mined instances from each view. In the first phase, the representation is augmented by integrating features learned from multiple views, and in the second phase, the model performs multi-view co-training to enhance localization performance of one view with the help of instances mined from other views, which thus effectively avoids early fitting. ML-LocNet can be easily combined with existing WSOL models to further improve the localization accuracy. Its effectiveness has been proved experimentally. Notably, it achieves #x0024;#x0024;68.6\%#x0024;#x0024; CorLoc and #x0024;#x0024;49.7\%#x0024;#x0024; mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.  2018, Springer Nature Switzerland AG.
KW  - Object recognition
KW  - Computer vision
U2  - Complementary relationship
U2  - Localization accuracy
U2  - Localization performance
U2  - Multi-view learning
U2  - Multiple views
U2  - Object localization
U2  - State of the art
U2  - Weakly supervised learning
DO  - 10.1007/978-3-030-01219-9_15
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_15
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Unsupervised Person Re-identification by Deep Learning Tracklet Association
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Minxian
A1  - Zhu, Xiatian
A1  - Gong, Shaogang
AD  - Nanjing University of Science and Technology, Nanjing, ChinaQueen Mary University of London, London, United KingdomVision Semantics Limited, London, United Kingdom
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004597
SP  - 772
EP  - 788
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Most existing person re-identification (re-id) methods rely on supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in practical re-id deployment due to the lack of exhaustive identity labelling of image positive and negative pairs for every camera pair. In this work, we address this problem by proposing an unsupervised re-id deep learning approach capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data from videos in an end-to-end model optimisation. We formulate a Tracklet Association Unsupervised Deep Learning (TAUDL) framework characterised by jointly learning per-camera (within-camera) tracklet association (labelling) and cross-camera tracklet correlation by maximising the discovery of most likely tracklet relationships across camera views. Extensive experiments demonstrate the superiority of the proposed TAUDL model over the state-of-the-art unsupervised and domain adaptation re-id methods using six person re-id benchmarking datasets.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Cameras
KW  - Computer vision
KW  - Security systems
KW  - Unsupervised learning
U2  - Automatically generated
U2  - Domain adaptation
U2  - End-to-end models
U2  - Learning approach
U2  - Person re identifications
U2  - Surveillance video
U2  - Tracklet
U2  - Tracklet associations
DO  - 10.1007/978-3-030-01225-0_45
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_45
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Urban zoning using higher-order markov random fields on multi-view imagery data
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Feng, Tian
A1  - Truong, Quang-Trung
A1  - Nguyen, Duc Thanh
A1  - Koh, Jing Yu
A1  - Yu, Lap-Fai
A1  - Binder, Alexander
A1  - Yeung, Sai-Kit
AD  - University of New South Wales, Kensington, AustraliaSingapore University of Technology and Design, Singapore, SingaporeDeakin University, Geelong, AustraliaUniversity of Massachusetts Boston, Boston, United StatesHong Kong University of Science and Technology, Clear Water Bay, Hong Kong
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006074
SP  - 627
EP  - 644
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Urban zoning enables various applications in land use analysis and urban planning. As cities evolve, it is important to constantly update the zoning maps of cities to reflect urban pattern changes. This paper proposes a method for automatic urban zoning using higher-order Markov random fields (HO-MRF) built on multi-view imagery data including street-view photos and top-view satellite images. In the proposed HO-MRF, top-view satellite data is segmented via a multi-scale deep convolutional neural network (MS-CNN) and used in lower-order potentials. Street-view data with geo-tagged information is augmented in higher-order potentials. Various feature types for classifying street-view images were also investigated in our work. We evaluated the proposed method on a number of famous metropolises and provided in-depth analysis on technical issues.  Springer Nature Switzerland AG 2018.
KW  - Zoning
KW  - Computer vision
KW  - Deep neural networks
KW  - Land use
KW  - Magnetorheological fluids
KW  - Maps
KW  - Markov processes
KW  - Neural networks
KW  - Satellite imagery
U2  - Deep convolutional neural networks
U2  - Higher Order Potentials
U2  - In-depth analysis
U2  - Land-use analysis
U2  - Markov Random Fields
U2  - Satellite data
U2  - Satellite images
U2  - Urban zoning
DO  - 10.1007/978-3-030-01237-3_38
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_38
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Distractor-aware siamese networks for visual object tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhu, Zheng
A1  - Wang, Qiang
A1  - Li, Bo
A1  - Wu, Wei
A1  - Yan, Junjie
A1  - Hu, Weiming
AD  - University of Chinese Academy of Sciences, Beijing, ChinaInstitute of Automation, Chinese Academy of Sciences, Beijing, ChinaSenseTime Group Limited, Beijing, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977526
SP  - 103
EP  - 119
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Recently, Siamese networks have drawn great attention in visual tracking community because of their balanced accuracy and speed. However, features used in most Siamese tracking approaches can only discriminate foreground from the non-semantic backgrounds. The semantic backgrounds are always considered as distractors, which hinders the robustness of Siamese trackers. In this paper, we focus on learning distractor-aware Siamese networks for accurate and long-term tracking. To this end, features used in traditional Siamese trackers are analyzed at first. We observe that the imbalanced distribution of training data makes the learned features less discriminative. During the off-line training phase, an effective sampling strategy is introduced to control this distribution and make the model focus on the semantic distractors. During inference, a novel distractor-aware module is designed to perform incremental learning, which can effectively transfer the general embedding to the current video domain. In addition, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search region strategy. Extensive experiments on benchmarks show that our approach significantly outperforms the state-of-the-arts, yielding 9.6% relative gain in VOT2016 dataset and 35.9% relative gain in UAV20L dataset. The proposed tracker can perform at 160 FPS on short-term benchmarks and 110 FPS on long-term benchmarks.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Semantics
KW  - Tracking (position)
U2  - Distractor-aware
U2  - Incremental learning
U2  - Long-term tracking
U2  - Off-line training
U2  - Sampling strategies
U2  - Tracking approaches
U2  - Visual object tracking
U2  - Visual Tracking
DO  - 10.1007/978-3-030-01240-3_7
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_7
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Planematch: patch coplanarity prediction for robust RGB-D reconstruction
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Shi, Yifei
A1  - Xu, Kai
A1  - Niecner, Matthias
A1  - Rusinkiewicz, Szymon
A1  - Funkhouser, Thomas
AD  - Princeton University, Princeton, United StatesNational University of Defense Technology, Changsha, ChinaTechnical University of Munich, Munich, GermanyGoogle, Mountain View, United States
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006083
SP  - 767
EP  - 784
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We introduce a novel RGB-D patch descriptor designed for detecting coplanar surfaces in SLAM reconstruction. The core of our method is a deep convolutional neural network that takes in RGB, depth, and normal information of a planar patch in an image and outputs a descriptor that can be used to find coplanar patches from other images. We train the network on 10 million triplets of coplanar and non-coplanar patches, and evaluate on a new coplanarity benchmark created from commodity RGB-D scans. Experiments show that our learned descriptor outperforms alternatives extended for this new task by a significant margin. In addition, we demonstrate the benefits of coplanarity matching in a robust RGBD reconstruction formulation. We find that coplanarity constraints detected with our method are sufficient to get reconstruction results comparable to state-of-the-art frameworks on most scenes, but outperform other methods on established benchmarks when combined with traditional keypoint matching.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Deep neural networks
KW  - Neural networks
U2  - Coplanarity
U2  - Deep convolutional neural networks
U2  - Key point matching
U2  - Loop closure
U2  - Non-coplanar
U2  - Planar patch
U2  - RGB-D registration
U2  - State of the art
DO  - 10.1007/978-3-030-01237-3_46
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_46
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Recurrent tubelet proposal and recognition networks for action detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Dong
A1  - Qiu, Zhaofan
A1  - Dai, Qi
A1  - Yao, Ting
A1  - Mei, Tao
AD  - University of Science and Technology of China, Hefei, ChinaMicrosoft Research, Beijing, ChinaJD AI Research, Beijing, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977395
SP  - 306
EP  - 322
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Detecting actions in videos is a challenging task as video is an information intensive media with complex variations. Existing approaches predominantly generate action proposals for each individual frame or fixed-length clip independently, while overlooking temporal context across them. Such temporal contextual relations are vital for action detection as an action is by nature a sequence of movements. This motivates us to leverage the localized action proposals in previous frames when determining action regions in the current one. Specifically, we present a novel deep architecture called Recurrent Tubelet Proposal and Recognition (RTPR) networks to incorporate temporal context for action detection. The proposed RTPR consists of two correlated networks, i.e., Recurrent Tubelet Proposal (RTP) networks and Recurrent Tubelet Recognition (RTR) networks. The RTP initializes action proposals of the start frame through a Region Proposal Network and then estimates the movements of proposals in next frame in a recurrent manner. The action proposals of different frames are linked to form the tubelet proposals. The RTR capitalizes on a multi-channel architecture, where in each channel, a tubelet proposal is fed into a CNN plus LSTM to recurrently recognize action in the tubelet. We conduct extensive experiments on four benchmark datasets and demonstrate superior results over state-of-the-art methods. More remarkably, we obtain mAP of 98.6%, 81.3%, 77.9% and 22.3% with gains of 2.9%, 4.3%, 0.7% and 3.9% over the best competitors on UCF-Sports, J-HMDB, UCF-101 and AVA, respectively.  Springer Nature Switzerland AG 2018.
KW  - Long short-term memory
KW  - Computer vision
KW  - Network architecture
KW  - Tubes (components)
U2  - Action recognition
U2  - Benchmark datasets
U2  - Deep architectures
U2  - Multi channel
U2  - Region proposals
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01231-1_19
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_19
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Fast light field reconstruction with deep coarse-to-fine modeling of spatial-angular clues
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yeung, Henry Wing Fung
A1  - Hou, Junhui
A1  - Chen, Jie
A1  - Chung, Yuk Ying
A1  - Chen, Xiaoming
AD  - School of Information Technologies, University of Sydney, Sydney, AustraliaDepartment of Computer Science, City University of Hong Kong, Kowloon, Hong KongSchool of Electrical and Electronics Engineering, Nanyang Technological University, Singapore, SingaporeSchool of Information Science and Technology, University of Science and Technology of China, Hefei, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977427
SP  - 138
EP  - 154
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Densely-sampled light fields (LFs) are beneficial to many applications such as depth inference and post-capture refocusing. However, it is costly and challenging to capture them. In this paper, we propose a learning based algorithm to reconstruct a densely-sampled LF fast and accurately from a sparsely-sampled LF in one forward pass. Our method uses computationally efficient convolutions to deeply characterize the high dimensional spatial-angular clues in a coarse-to-fine manner. Specifically, our end-to-end model first synthesizes a set of intermediate novel sub-aperture images (SAIs) by exploring the coarse characteristics of the sparsely-sampled LF input with spatial-angular alternating convolutions. Then, the synthesized intermediate novel SAIs are efficiently refined by further recovering the fine relations from all SAIs via guided residual learning and stride-2 4-D convolutions. Experimental results on extensive real-world and synthetic LF images show that our model can provide more than 3 dB advantage in reconstruction quality in average than the state-of-the-art methods while being computationally faster by a factor of 30. Besides, more accurate depth can be inferred from the reconstructed densely-sampled LFs by our method.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Computer vision
KW  - Convolution
KW  - Neural networks
U2  - Computationally efficient
U2  - Convolutional neural network
U2  - Learning-based algorithms
U2  - Light fields
U2  - Reconstruction quality
U2  - State-of-the-art methods
U2  - Super resolution
U2  - View synthesis
DO  - 10.1007/978-3-030-01231-1_9
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_9
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning warped guidance for blind face restoration
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Xiaoming
A1  - Liu, Ming
A1  - Ye, Yuting
A1  - Zuo, Wangmeng
A1  - Lin, Liang
A1  - Yang, Ruigang
AD  - School of Computer Science and Technology, Harbin Institute of Technology, Harbin, ChinaSchool of Data and Computer Science, Sun Yat-sen University, Guangzhou, ChinaDepartment of Computer Science, University of Kentucky, Lexington, United States
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008936
SP  - 278
EP  - 296
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper studies the problem of blind face restoration from an unconstrained blurry, noisy, low-resolution, or compressed image (i.e., degraded observation). For better recovery of fine facial details, we modify the problem setting by taking both the degraded observation and a high-quality guided image of the same identity as input to our guided face restoration network (GFRNet). However, the degraded observation and guided image generally are different in pose, illumination and expression, thereby making plain CNNs (e.g., U-Net) fail to recover fine and identity-aware facial details. To tackle this issue, our GFRNet model includes both a warping subnetwork (WarpNet) and a reconstruction subnetwork (RecNet). The WarpNet is introduced to predict flow field for warping the guided image to correct pose and expression (i.e., warped guidance), while the RecNet takes the degraded observation and warped guidance as input to produce the restoration result. Due to that the ground-truth flow field is unavailable, landmark loss together with total variation regularization are incorporated to guide the learning of WarpNet. Furthermore, to make the model applicable to blind restoration, our GFRNet is trained on the synthetic data with versatile settings on blur kernel, noise level, downsampling scale factor, and JPEG quality factor. Experiments show that our GFRNet not only performs favorably against the state-of-the-art image and face restoration methods, but also generates visually photo-realistic results on real degraded facial images.  Springer Nature Switzerland AG 2018.
KW  - Image reconstruction
KW  - Computer vision
KW  - Flow fields
KW  - Restoration
U2  - Blind image restoration
U2  - Blind restoration
U2  - Compressed images
U2  - Face hallucination
U2  - Restoration methods
U2  - Restoration network
U2  - State of the art
U2  - Total variation regularization
DO  - 10.1007/978-3-030-01261-8_17
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_17
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Semi-supervised generative adversarial hashing for image retrieval
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Guanan
A1  - Hu, Qinghao
A1  - Cheng, Jian
A1  - Hou, Zengguang
AD  - The State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, ChinaNational Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, ChinaUniversity of Chinese Academy of Sciences, Beijing, ChinaCenter for Excellence in Brain Science and Intelligence Technology, Beijing, China
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006178
SP  - 491
EP  - 507
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - With explosive growth of image and video data on the Internet, hashing technique has been extensively studied for large-scale visual search. Benefiting from the advance of deep learning, deep hashing methods have achieved promising performance. However, those deep hashing models are usually trained with supervised information, which is rare and expensive in practice, especially class labels. In this paper, inspired by the idea of generative models and the minimax two-player game, we propose a novel semi-supervised generative adversarial hashing (SSGAH) approach. Firstly, we unify a generative model, a discriminative model and a deep hashing model in a framework for making use of triplet-wise information and unlabeled data. Secondly, we design novel structure of the generative model and the discriminative model to learn the distribution of triplet-wise information in a semi-supervised way. In addition, we propose a semi-supervised ranking loss and an adversary ranking loss to learn binary codes which preserve semantic similarity for both labeled data and unlabeled data. Finally, by optimizing the whole model in an adversary training way, the learned binary codes can capture better semantic information of all data. Extensive empirical evaluations on two widely-used benchmark datasets show that our proposed approach significantly outperforms state-of-the-art hashing methods.  Springer Nature Switzerland AG 2018.
KW  - Deep learning
KW  - Binary codes
KW  - Computer vision
KW  - Game theory
KW  - Image retrieval
KW  - Information retrieval
KW  - Semantics
U2  - Benchmark datasets
U2  - Discriminative models
U2  - Empirical evaluations
U2  - GANs
U2  - Hashing
U2  - Hashing techniques
U2  - Semantic information
U2  - Semantic similarity
DO  - 10.1007/978-3-030-01267-0_29
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_29
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Macro-micro adversarial network for human parsing
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Luo, Yawei
A1  - Zheng, Zhedong
A1  - Zheng, Liang
A1  - Guan, Tao
A1  - Yu, Junqing
A1  - Yang, Yi
AD  - School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, ChinaCAI, University of Technology Sydney, Sydney, AustraliaSingapore University of Technology and Design, Singapore, Singapore
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977498
SP  - 424
EP  - 440
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In human parsing, the pixel-wise classification loss has drawbacks in its low-level local inconsistency and high-level semantic inconsistency. The introduction of the adversarial network tackles the two problems using a single discriminator. However, the two types of parsing inconsistency are generated by distinct mechanisms, so it is difficult for a single discriminator to solve them both. To address the two kinds of inconsistencies, this paper proposes the Macro-Micro Adversarial Net (MMAN). It has two discriminators. One discriminator, Macro D, acts on the low-resolution label map and penalizes semantic inconsistency, e.g., misplaced body parts. The other discriminator, Micro D, focuses on multiple patches of the high-resolution label map to address the local inconsistency, e.g., blur and hole. Compared with traditional adversarial networks, MMAN not only enforces local and semantic consistency explicitly, but also avoids the poor convergence problem of adversarial networks when handling high resolution images. In our experiment, we validate that the two discriminators are complementary to each other in improving the human parsing accuracy. The proposed framework is capable of producing competitive parsing performance compared with the state-of-the-art methods, i.e., mIoU = 46.81% and 59.91% on LIP and PASCAL-Person-Part, respectively. On a relatively small dataset PPSS, our pre-trained model demonstrates impressive generalization ability. The code is publicly available at https://github.com/RoyalVane/MMAN.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Semantics
U2  - Adversarial networks
U2  - Generalization ability
U2  - High resolution image
U2  - Human parsing
U2  - Inconsistency
U2  - Macro micro
U2  - Semantic inconsistencies
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01240-3_26
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_26
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Predicting Gaze in Egocentric Video by Learning Task-Dependent Attention Transition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Huang, Yifei
A1  - Cai, Minjie
A1  - Li, Zhenqiang
A1  - Sato, Yoichi
AD  - The University of Tokyo, Tokyo, JapanHunan University, Changsha, China
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004598
SP  - 789
EP  - 804
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present a new computational model for gaze prediction in egocentric videos by exploring patterns in temporal shift of gaze fixations (attention transition) that are dependent on egocentric manipulation tasks. Our assumption is that the high-level context of how a task is completed in a certain way has a strong influence on attention transition and should be modeled for gaze prediction in natural dynamic scenes. Specifically, we propose a hybrid model based on deep neural networks which integrates task-dependent attention transition with bottom-up saliency prediction. In particular, the task-dependent attention transition is learned with a recurrent neural network to exploit the temporal context of gaze fixations, e.g. looking at a cup after moving gaze away from a grasped bottle. Experiments on public egocentric activity datasets show that our model significantly outperforms state-of-the-art gaze prediction methods and is able to learn meaningful transition of human attention.  2018, Springer Nature Switzerland AG.
KW  - Forecasting
KW  - Bottles
KW  - Computer vision
KW  - Deep neural networks
KW  - Recurrent neural networks
U2  - Attention transition
U2  - Bottom-up saliencies
U2  - Computational model
U2  - Egocentric video
U2  - Manipulation task
U2  - Natural dynamics
U2  - Prediction methods
U2  - State of the art
DO  - 10.1007/978-3-030-01225-0_46
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_46
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - The devil of face recognition is in the noise
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Fei
A1  - Chen, Liren
A1  - Li, Cheng
A1  - Huang, Shiyao
A1  - Chen, Yanjie
A1  - Qian, Chen
A1  - Loy, Chen Change
AD  - SenseTime Research, Beijing, ChinaUniversity of California San Diego, San Diego, United StatesNanyang Technological University, Singapore, Singapore
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977521
SP  - 780
EP  - 795
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The growing scale of face recognition datasets empowers us to train strong convolutional networks for face recognition. While a variety of architectures and loss functions have been devised, we still have a limited understanding of the source and consequence of label noise inherent in existing datasets. We make the following contributions: (1) We contribute cleaned subsets of popular face databases, i.e., MegaFace and MS-Celeb-1M datasets, and build a new large-scale noise-controlled IMDb-Face dataset. (2) With the original datasets and cleaned subsets, we profile and analyze label noise properties of MegaFace and MS-Celeb-1M. We show that a few orders more samples are needed to achieve the same accuracy yielded by a clean subset. (3) We study the association between different types of noise, i.e., label flips and outliers, with the accuracy of face recognition models. (4) We investigate ways to improve data cleanliness, including a comprehensive user study on the influence of data labeling strategies to annotation accuracy. The IMDb-Face dataset has been released on https://github.com/fwang91/IMDb-Face.  Springer Nature Switzerland AG 2018.
KW  - Face recognition
KW  - Computer vision
U2  - Convolutional networks
U2  - Data labeling
U2  - Face database
U2  - Loss functions
U2  - Noise properties
U2  - Noise-controlled
U2  - Recognition models
U2  - User study
DO  - 10.1007/978-3-030-01240-3_47
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_47
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Find and focus: Retrieve and localize video events with natural language queries
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Shao, Dian
A1  - Xiong, Yu
A1  - Zhao, Yue
A1  - Huang, Qingqiu
A1  - Qiao, Yu
A1  - Lin, Dahua
AD  - CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Shatin, Hong KongSIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Beijing, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977484
SP  - 202
EP  - 218
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The thriving of video sharing services brings new challenges to video retrieval, e.g. the rapid growth in video duration and content diversity. Meeting such challenges calls for new techniques that can effectively retrieve videos with natural language queries. Existing methods along this line, which mostly rely on embedding videos as a whole, remain far from satisfactory for real-world applications due to the limited expressive power. In this work, we aim to move beyond this limitation by delving into the internal structures of both sides, the queries and the videos. Specifically, we propose a new framework called Find and Focus (FIFO), which not only performs top-level matching (paragraph vs. video), but also makes part-level associations, localizing a video clip for each sentence in the query with the help of a focusing guide. These levels are complementary  the top-level matching narrows the search while the part-level localization refines the results. On both ActivityNet Captions and modified LSMDC datasets, the proposed framework achieves remarkable performance gains (Project Page: https://ycxioooong.github.io/projects/fifo).  Springer Nature Switzerland AG 2018.
KW  - Natural language processing systems
KW  - Computer vision
U2  - Expressive power
U2  - Internal structure
U2  - Level matching
U2  - Natural language queries
U2  - Performance Gain
U2  - Video events
U2  - Video retrieval
U2  - Video sharing
DO  - 10.1007/978-3-030-01240-3_13
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_13
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Visual Tracking via Spatially Aligned Correlation Filters Network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Mengdan
A1  - Wang, Qiang
A1  - Xing, Junliang
A1  - Gao, Jin
A1  - Peng, Peixi
A1  - Hu, Weiming
A1  - Maybank, Steve
AD  - CAS Center for Excellence in Brain Science and Intelligence Technology, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, ChinaBirkbeck College, University of London, London, United Kingdom
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977817
SP  - 484
EP  - 500
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Correlation filters based trackers rely on a periodic assumption of the search sample to efficiently distinguish the target from the background. This assumption however yields undesired boundary effects and restricts aspect ratios of search samples. To handle these issues, an end-to-end deep architecture is proposed to incorporate geometric transformations into a correlation filters based network. This architecture introduces a novel spatial alignment module, which provides continuous feedback for transforming the target from the border to the center with a normalized aspect ratio. It enables correlation filters to work on well-aligned samples for better tracking. The whole architecture not only learns a generic relationship between object geometric transformations and object appearances, but also learns robust representations coupled to correlation filters in case of various geometric transformations. This lightweight architecture permits real-time speed. Experiments show our tracker effectively handles boundary effects and aspect ratio variations, achieving state-of-the-art tracking results on recent benchmarks.  2018, Springer Nature Switzerland AG.
KW  - Aspect ratio
KW  - Computer vision
KW  - Deep learning
KW  - Geometry
KW  - Mathematical transformations
KW  - Network architecture
U2  - Correlation filters
U2  - Deep architectures
U2  - Generic relationship
U2  - Geometric transformations
U2  - Lightweight architecture
U2  - Object appearance
U2  - Spatial alignment
U2  - Visual Tracking
DO  - 10.1007/978-3-030-01219-9_29
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_29
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Facial expression recognition with inconsistently annotated datasets
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zeng, Jiabei
A1  - Shan, Shiguang
A1  - Chen, Xilin
AD  - Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing; 100190, ChinaUniversity of Chinese Academy of Sciences, Beijing; 100190, ChinaCAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008933
SP  - 227
EP  - 243
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Annotation errors and bias are inevitable among different facial expression datasets due to the subjectiveness of annotating facial expressions. Ascribe to the inconsistent annotations, performance of existing facial expression recognition (FER) methods cannot keep improving when the training set is enlarged by merging multiple datasets. To address the inconsistency, we propose an Inconsistent Pseudo Annotations to Latent Truth (IPA2LT) framework to train a FER model from multiple inconsistently labeled datasets and large scale unlabeled data. In IPA2LT, we assign each sample more than one labels with human annotations or model predictions. Then, we propose an end-to-end LTNet with a scheme of discovering the latent truth from the inconsistent pseudo labels and the input face images. To our knowledge, IPA2LT serves as the first work to solve the training problem with inconsistently labeled FER datasets. Experiments on synthetic data validate the effectiveness of the proposed method in learning from inconsistent labels. We also conduct extensive experiments in FER and show that our method outperforms other state-of-the-art and optional methods under a rigorous evaluation protocol involving 7 FER datasets.  Springer Nature Switzerland AG 2018.
KW  - Face recognition
KW  - Computer vision
U2  - Annotated datasets
U2  - Annotation errors
U2  - Facial expression recognition
U2  - Facial Expressions
U2  - Human annotations
U2  - Multiple data sets
U2  - Rigorous evaluation
U2  - State of the art
DO  - 10.1007/978-3-030-01261-8_14
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_14
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Hard-Aware Point-to-Set Deep Metric for Person Re-identification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yu, Rui
A1  - Dou, Zhiyong
A1  - Bai, Song
A1  - Zhang, Zhaoxiang
A1  - Xu, Yongchao
A1  - Bai, Xiang
AD  - Huazhong University of Science and Technology, Wuhan, ChinaInstitute of Automation, Chinese Academy of Sciences, Beijing, China
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978917
SP  - 196
EP  - 212
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Person re-identification (re-ID) is a highly challenging task due to large variations of pose, viewpoint, illumination, and occlusion. Deep metric learning provides a satisfactory solution to person re-ID by training a deep network under supervision of metric loss, e.g., triplet loss. However, the performance of deep metric learning is greatly limited by traditional sampling methods. To solve this problem, we propose a Hard-Aware Point-to-Set (HAP2S) loss with a soft hard-mining scheme. Based on the point-to-set triplet loss framework, the HAP2S loss adaptively assigns greater weights to harder samples. Several advantageous properties are observed when compared with other state-of-the-art loss functions: (1) Accuracy: HAP2S loss consistently achieves higher re-ID accuracies than other alternatives on three large-scale benchmark datasets; (2) Robustness: HAP2S loss is more robust to outliers than other losses; (3) Flexibility: HAP2S loss does not rely on a specific weight function, i.e., different instantiations of HAP2S loss are equally effective. (4) Generality: In addition to person re-ID, we apply the proposed method to generic deep metric learning benchmarks including CUB-200-2011 and Cars196, and also achieve state-of-the-art results.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Computer vision
KW  - Scales (weighing instruments)
U2  - Benchmark datasets
U2  - Deep networks
U2  - Loss functions
U2  - Metric learning
U2  - Person re identifications
U2  - Sampling method
U2  - Satisfactory solutions
U2  - State of the art
DO  - 10.1007/978-3-030-01270-0_12
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_12
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Unpaired Image Captioning by Language Pivoting
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Gu, Jiuxiang
A1  - Joty, Shafiq
A1  - Cai, Jianfei
A1  - Wang, Gang
AD  - ROSE Lab, Nanyang Technological University, Singapore, SingaporeSCSE, Nanyang Technological University, Singapore, SingaporeAlibaba AI Labs, Hangzhou, China
VL  - 11205 LNCS
PY  - 2018
U1  - 20184305977710
SP  - 519
EP  - 535
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Image captioning is a multimodal task involving computer vision and natural language processing, where the goal is to learn a mapping from the image to its natural language description. In general, the mapping function is learned from a training set of image-caption pairs. However, for some language, large scale image-caption paired corpus might not be available. We present an approach to this unpaired image captioning problem by language pivoting. Our method can effectively capture the characteristics of an image captioner from the pivot language (Chinese) and align it to the target language (English) using another pivot-target (Chinese-English) sentence parallel corpus. We evaluate our method on two image-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative comparisons against several baseline approaches demonstrate the effectiveness of our method.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Linguistics
KW  - Mapping
KW  - Natural language processing systems
U2  - Benchmark datasets
U2  - Image captioning
U2  - Mapping functions
U2  - Natural languages
U2  - Parallel corpora
U2  - Quantitative comparison
U2  - Target language
U2  - Unpaired learning
DO  - 10.1007/978-3-030-01246-5_31
L2  - http://dx.doi.org/10.1007/978-3-030-01246-5_31
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep adaptive attention for joint facial action unit detection and face alignment
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Shao, Zhiwen
A1  - Liu, Zhilei
A1  - Cai, Jianfei
A1  - Ma, Lizhuang
AD  - Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, ChinaCollege of Intellengence and Computing, Tianjin University, Tianjin, ChinaSchool of Computer Science and Engineering, Nanyang Technological University, Singapore, SingaporeSchool of Computer Science and Software Engineering, East China Normal University, Shanghai, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008965
SP  - 725
EP  - 740
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Facial action unit (AU) detection and face alignment are two highly correlated tasks since facial landmarks can provide precise AU locations to facilitate the extraction of meaningful local features for AU detection. Most existing AU detection works often treat face alignment as a preprocessing and handle the two tasks independently. In this paper, we propose a novel end-to-end deep learning framework for joint AU detection and face alignment, which has not been explored before. In particular, multi-scale shared features are learned firstly, and high-level features of face alignment are fed into AU detection. Moreover, to extract precise local features, we propose an adaptive attention learning module to refine the attention map of each AU adaptively. Finally, the assembled local features are integrated with face alignment features and global features for AU detection. Experiments on BP4D and DISFA benchmarks demonstrate that our framework significantly outperforms the state-of-the-art methods for AU detection.  Springer Nature Switzerland AG 2018.
KW  - Face recognition
KW  - Alignment
KW  - Computer vision
KW  - Deep learning
KW  - Feature extraction
U2  - Adaptive attention learning
U2  - Face alignment
U2  - High-level features
U2  - Highly-correlated
U2  - Joint learning
U2  - Learning frameworks
U2  - Learning modules
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01261-8_43
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_43
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Universal sketch perceptual grouping
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Ke
A1  - Pang, Kaiyue
A1  - Song, Jifei
A1  - Song, Yi-Zhe
A1  - Xiang, Tao
A1  - Hospedales, Timothy M.
A1  - Zhang, Honggang
AD  - Beijing University of Posts and Telecommunications, Beijing, ChinaSketchX, Queen Mary University of London, London, United KingdomThe University of Edinburgh, Edinburgh, United Kingdom
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006072
SP  - 593
EP  - 609
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this work we aim to develop a universal sketch grouper. That is, a grouper that can be applied to sketches of any category in any domain to group constituent strokes/segments into semantically meaningful object parts. The first obstacle to this goal is the lack of large-scale datasets with grouping annotation. To overcome this, we contribute the largest sketch perceptual grouping (SPG) dataset to date, consisting of 20, 000 unique sketches evenly distributed over 25 object categories. Furthermore, we propose a novel deep universal perceptual grouping model. The model is learned with both generative and discriminative losses. The generative losses improve the generalisation ability of the model to unseen object categories and datasets. The discriminative losses include a local grouping loss and a novel global grouping loss to enforce global grouping consistency. We show that the proposed model significantly outperforms the state-of-the-art groupers. Further, we show that our grouper is useful for a number of sketch analysis tasks including sketch synthesis and fine-grained sketch-based image retrieval (FG-SBIR).  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Image retrieval
U2  - Dataset
U2  - Grouping model
U2  - Large-scale datasets
U2  - Object categories
U2  - Perceptual grouping
U2  - Sketch-based image retrievals
U2  - State of the art
U2  - Universal grouper
DO  - 10.1007/978-3-030-01237-3_36
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_36
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Large Scale Urban Scene Modeling from MVS Meshes
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhu, Lingjie
A1  - Shen, Shuhan
A1  - Gao, Xiang
A1  - Hu, Zhanyi
AD  - NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, ChinaUniversity of Chinese Academy of Sciences, Beijing, China
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978895
SP  - 640
EP  - 655
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper we present an efficient modeling framework for large scale urban scenes. Taking surface meshes derived from multi-view-stereo systems as input, our algorithm outputs simplified models with semantics at different levels of detail (LODs). Our key observation is that urban building is usually composed of planar roof tops connected with vertical walls. There are two major steps in our framework: segmentation and building modeling. The scene is first segmented into four classes with a Markov random field combining height and image features. In the following modeling step, various 2D line segments sketching the roof boundaries are detected and slice the plane into faces. Through assigning each face with a roof plane, the final model is constructed by extruding the faces to the corresponding planes. By combining geometric and appearance cues together, the proposed method is robust and fast compared to the state-of-the-art algorithms.  2018, Springer Nature Switzerland AG.
KW  - Stereo image processing
KW  - Computer vision
KW  - Image segmentation
KW  - Markov processes
KW  - Roofs
KW  - Semantics
U2  - Building model
U2  - Levels of detail
U2  - Markov Random Fields
U2  - Model framework
U2  - Multi-view stereo
U2  - Segment-based
U2  - State-of-the-art algorithms
U2  - Urban buildings
DO  - 10.1007/978-3-030-01252-6_38
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_38
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wu, Zuxuan
A1  - Han, Xintong
A1  - Lin, Yen-Liang
A1  - Uzunbas, Mustafa Gokhan
A1  - Goldstein, Tom
A1  - Lim, Ser Nam
A1  - Davis, Larry S.
AD  - University of Maryland, College Park, United StatesMalong Technologies, Shenzhen, ChinaGE Global Research, Niskayuna, United StatesFacebook, Menlo Park, United States
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976869
SP  - 535
EP  - 552
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Harvesting dense pixel-level annotations to train deep neural networks for semantic segmentation is extremely expensive and unwieldy at scale. While learning from synthetic data where labels are readily available sounds promising, performance degrades significantly when testing on novel realistic data due to domain discrepancies. We present Dual Channel-wise Alignment Networks (DCAN), a simple yet effective approach to reduce domain shift at both pixel-level and feature-level. Exploring statistics in each channel of CNN feature maps, our framework performs channel-wise feature alignment, which preserves spatial structures and semantic information, in both an image generator and a segmentation network. In particular, given an image from the source domain and unlabeled samples from the target domain, the generator synthesizes new images on-the-fly to resemble samples from the target domain in appearance and the segmentation network further refines high-level features before predicting semantic maps, both of which leverage feature statistics of sampled images from the target domain. Unlike much recent and concurrent work relying on adversarial training, our framework is lightweight and easy to train. Extensive experiments on adapting models trained on synthetic segmentation benchmarks to real urban scenes demonstrate the effectiveness of the proposed framework.  2018, Springer Nature Switzerland AG.
KW  - Image segmentation
KW  - Alignment
KW  - Computer vision
KW  - Deep neural networks
KW  - Pixels
KW  - Semantics
U2  - Effective approaches
U2  - Feature alignment
U2  - High-level features
U2  - Image generators
U2  - Semantic information
U2  - Semantic segmentation
U2  - Spatial structure
U2  - Unlabeled samples
DO  - 10.1007/978-3-030-01228-1_32
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_32
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep co-training for semi-supervised image recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Qiao, Siyuan
A1  - Shen, Wei
A1  - Zhang, Zhishuai
A1  - Wang, Bo
A1  - Yuille, Alan
AD  - Johns Hopkins University, Baltimore, United StatesShanghai University, Shanghai, ChinaHikvision Research Institute, Hangzhou, China
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406005951
SP  - 142
EP  - 159
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we study the problem of semi-supervised image recognition, which is to learn classifiers using both labeled and unlabeled images. We present Deep Co-Training, a deep learning based method inspired by the Co-Training framework. The original Co-Training learns two classifiers on two views which are data from different sources that describe the same instances. To extend this concept to deep learning, Deep Co-Training trains multiple deep neural networks to be the different views and exploits adversarial examples to encourage view difference, in order to prevent the networks from collapsing into each other. As a result, the co-trained networks provide different and complementary information about the data, which is necessary for the Co-Training framework to achieve good results. We test our method on SVHN, CIFAR-10/100 and ImageNet datasets, and our method outperforms the previous state-of-the-art methods by a large margin.  Springer Nature Switzerland AG 2018.
KW  - Deep neural networks
KW  - Computer vision
KW  - Image recognition
KW  - Supervised learning
U2  - Co-training
U2  - Deep networks
U2  - Large margins
U2  - Learning-based methods
U2  - Semi- supervised learning
U2  - Semi-supervised
U2  - State-of-the-art methods
U2  - Two views
DO  - 10.1007/978-3-030-01267-0_9
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_9
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Penalizing top performers: Conservative loss for semantic segmentation adaptation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhu, Xinge
A1  - Zhou, Hui
A1  - Yang, Ceyuan
A1  - Shi, Jianping
A1  - Lin, Dahua
AD  - CUHK-SenseTime Joint Lab, CUHK, Hong KongSenseTime Research, Beijing, China
VL  - 11211 LNCS
PY  - 2018
U1  - 20184305977612
SP  - 587
EP  - 603
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Due to the expensive and time-consuming annotations (e.g., segmentation) for real-world images, recent works in computer vision resort to synthetic data. However, the performance on the real image often drops significantly because of the domain shift between the synthetic data and the real images. In this setting, domain adaptation brings an appealing option. The effective approaches of domain adaptation shape the representations that (1) are discriminative for the main task and (2) have good generalization capability for domain shift. To this end, we propose a novel loss function, i.e., Conservative Loss, which penalizes the extreme good and bad cases while encouraging the moderate examples. More specifically, it enables the network to learn features that are discriminative by gradient descent and are invariant to the change of domains via gradient ascend method. Extensive experiments on synthetic to real segmentation adaptation show our proposed method achieves state of the art results. Ablation studies give more insights into properties of the Conservative Loss. Exploratory experiments and discussion demonstrate that our Conservative Loss has good flexibility rather than restricting an exact form.  Springer Nature Switzerland AG 2018.
KW  - Image segmentation
KW  - Computer vision
KW  - Semantics
U2  - Domain adaptation
U2  - Effective approaches
U2  - Generalization capability
U2  - Gradient ascend
U2  - Gradient descent
U2  - Real-world image
U2  - Semantic segmentation
U2  - State of the art
DO  - 10.1007/978-3-030-01234-2_35
L2  - http://dx.doi.org/10.1007/978-3-030-01234-2_35
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - AugGAN: Cross domain adaptation with GAN-based data augmentation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Huang, Sheng-Wei
A1  - Lin, Che-Tsung
A1  - Chen, Shu-Ping
A1  - Wu, Yen-Yi
A1  - Hsu, Po-Hao
A1  - Lai, Shang-Hong
AD  - Department of Computer Science, National Tsing Hua University, Hsinchu, TaiwanIntelligent Mobility Division, Mechanical and Mechatronics Systems Research Laboratories, Industrial Technology Research Institute, Zhudong, Taiwan
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977518
SP  - 731
EP  - 744
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Deep learning based image-to-image translation methods aim at learning the joint distribution of the two domains and finding transformations between them. Despite recent GAN (Generative Adversarial Network) based methods have shown compelling results, they are prone to fail at preserving image-objects and maintaining translation consistency, which reduces their practicality on tasks such as generating large-scale training data for different domains. To address this problem, we purpose a structure-aware image-to-image translation network, which is composed of encoders, generators, discriminators and parsing nets for the two domains, respectively, in a unified framework. The purposed network generates more visually plausible images compared to competing methods on different image-translation tasks. In addition, we quantitatively evaluate different methods by training Faster-RCNN and YOLO with datasets generated from the image-translation results and demonstrate significant improvement on the detection accuracies by using the proposed image-object preserving network.  Springer Nature Switzerland AG 2018.
KW  - Image enhancement
KW  - Computer vision
KW  - Deep learning
KW  - Image segmentation
KW  - Object detection
KW  - Semantics
U2  - Adversarial networks
U2  - Data augmentation
U2  - Detection accuracy
U2  - Different domains
U2  - Domain adaptation
U2  - Image translation
U2  - Joint distributions
U2  - Semantic segmentation
DO  - 10.1007/978-3-030-01240-3_44
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_44
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Bi-Real Net: Enhancing the performance of 1-bit CNNs with improved representational capability and advanced training algorithm
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Zechun
A1  - Wu, Baoyuan
A1  - Luo, Wenhan
A1  - Yang, Xin
A1  - Liu, Wei
A1  - Cheng, Kwang-Ting
AD  - Hong Kong University of Science and Technology, Hong KongTencent AI Lab, Beijing, ChinaHuazhong University of Science and Technology, Wuhan, China
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006195
SP  - 747
EP  - 763
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this work, we study the 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While being efficient, the classification accuracy of the current 1-bit CNNs is much worse compared to their counterpart real-valued CNN models on the large-scale dataset, like ImageNet. To minimize the performance gap between the 1-bit and real-valued CNN models, we propose a novel model, dubbed Bi-Real net, which connects the real activations (after the 1-bit convolution and/or BatchNorm layer, before the sign function) to activations of the consecutive block, through an identity shortcut. Consequently, compared to the standard 1-bit CNN, the representational capability of the Bi-Real net is significantly enhanced and the additional cost on computation is negligible. Moreover, we develop a specific training algorithm including three technical novelties for 1-bit CNNs. Firstly, we derive a tight approximation to the derivative of the non-differentiable sign function with respect to activation. Secondly, we propose a magnitude-aware gradient with respect to the weight for updating the weight parameters. Thirdly, we pre-train the real-valued CNN model with a clip function, rather than the ReLU function, to better initialize the Bi-Real net. Experiments on ImageNet show that the Bi-Real net with the proposed training algorithm achieves 56.4% and 62.2% top-1 accuracy with 18 layers and 34 layers, respectively. Compared to the state-of-the-arts (e.g., XNOR Net), Bi-Real net achieves up to 10% higher top-1 accuracy with more memory saving and lower computational cost.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Chemical activation
KW  - Classification (of information)
KW  - Convolution
KW  - Neural networks
U2  - Classification accuracy
U2  - Computational costs
U2  - Convolutional neural network
U2  - Large-scale dataset
U2  - Non-differentiable
U2  - Representational capabilities
U2  - Training algorithms
U2  - Weight parameters
DO  - 10.1007/978-3-030-01267-0_44
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_44
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - MVSNet: Depth inference for unstructured multi-view stereo
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yao, Yao
A1  - Luo, Zixin
A1  - Li, Shiwei
A1  - Fang, Tian
A1  - Quan, Long
AD  - The Hong Kong University of Science and Technology, Hong KongShenzhen Zhuke Innovation Technology (Altizure), Shenzhen, China
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006084
SP  - 785
EP  - 801
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present an end-to-end deep learning architecture for depth map inference from multi-view images. In the network, we first extract deep visual image features, and then build the 3D cost volume upon the reference camera frustum via the differentiable homography warping. Next, we apply 3D convolutions to regularize and regress the initial depth map, which is then refined with the reference image to generate the final output. Our framework flexibly adapts arbitrary N-view inputs using a variance-based cost metric that maps multiple features into one cost feature. The proposed MVSNet is demonstrated on the large-scale indoor DTU dataset. With simple post-processing, our method not only significantly outperforms previous state-of-the-arts, but also is several times faster in runtime. We also evaluate MVSNet on the complex outdoor Tanks and Temples dataset, where our method ranks first before April 18, 2018 without any fine-tuning, showing the strong generalization ability of MVSNet.  Springer Nature Switzerland AG 2018.
KW  - Stereo image processing
KW  - Computer vision
KW  - Deep learning
U2  - Depth Map
U2  - Generalization ability
U2  - Learning architectures
U2  - Multi-view image
U2  - Multi-view stereo
U2  - Multiple features
U2  - Post processing
U2  - State of the art
DO  - 10.1007/978-3-030-01237-3_47
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_47
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Online Multi-Object Tracking with Dual Matching Attention Networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhu, Ji
A1  - Yang, Hua
A1  - Liu, Nian
A1  - Kim, Minyoung
A1  - Zhang, Wenjun
A1  - Yang, Ming-Hsuan
AD  - Shanghai Jiao Tong University, Shanghai, ChinaVisbody Inc., Shanghai, ChinaNorthwestern Polytechnical University, Xian, ChinaMassachusetts Institute of Technology, Cambridge, United StatesUniversity of California, Merced, United StatesGoogle Inc., Menlo Park, United States
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976859
SP  - 379
EP  - 396
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose an online Multi-Object Tracking (MOT) approach which integrates the merits of single object tracking and data association methods in a unified framework to handle noisy detections and frequent interactions between targets. Specifically, for applying single object tracking in MOT, we introduce a cost-sensitive tracking loss based on the state-of-the-art visual tracker, which encourages the model to focus on hard negative distractors during online learning. For data association, we propose Dual Matching Attention Networks (DMAN) with both spatial and temporal attention mechanisms. The spatial attention module generates dual attention maps which enable the network to focus on the matching patterns of the input image pair, while the temporal attention module adaptively allocates different levels of attention to different samples in the tracklet to suppress noisy observations. Experimental results on the MOT benchmark datasets show that the proposed algorithm performs favorably against both online and offline trackers in terms of identity-preserving metrics.  2018, Springer Nature Switzerland AG.
KW  - Tracking (position)
KW  - Association reactions
KW  - Computer vision
U2  - Attention mechanisms
U2  - Benchmark datasets
U2  - Cost-sensitive
U2  - Matching patterns
U2  - Multi-object tracking
U2  - Noisy observations
U2  - Spatial attention
U2  - Unified framework
DO  - 10.1007/978-3-030-01228-1_23
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_23
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Stroke controllable fast style transfer with adaptive receptive fields
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Jing, Yongcheng
A1  - Liu, Yang
A1  - Yang, Yezhou
A1  - Feng, Zunlei
A1  - Yu, Yizhou
A1  - Tao, Dacheng
A1  - Song, Mingli
AD  - College of Computer Science and Technology, Zhejiang University, Hangzhou, ChinaAlibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, ChinaArizona State University, Tempe, United StatesDeepwise AI Lab, Beijing, ChinaUBTECH Sydney AI Centre, SIT, FEIT, University of Sydney, Sydney, Australia
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008934
SP  - 244
EP  - 260
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - The Fast Style Transfer methods have been recently proposed to transfer a photograph to an artistic style in real-time. This task involves controlling the stroke size in the stylized results, which remains an open challenge. In this paper, we present a stroke controllable style transfer network that can achieve continuous and spatial stroke size control. By analyzing the factors that influence the stroke size, we propose to explicitly account for the receptive field and the style image scales. We propose a StrokePyramid module to endow the network with adaptive receptive fields, and two training strategies to achieve faster convergence and augment new stroke sizes upon a trained model respectively. By combining the proposed runtime control strategies, our network can achieve continuous changes in stroke sizes and produce distinct stroke sizes in different spatial regions within the same output image.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Artificial intelligence
KW  - Computer science
KW  - Computers
U2  - Faster convergence
U2  - Neural Style Transfer
U2  - Receptive fields
U2  - Runtime control
U2  - Spatial regions
U2  - Training strategy
U2  - Transfer method
U2  - Transfer network
DO  - 10.1007/978-3-030-01261-8_15
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_15
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Fully motion-aware network for video object detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Shiyao
A1  - Zhou, Yucong
A1  - Yan, Junjie
A1  - Deng, Zhidong
AD  - State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Department of Computer Science, Tsinghua University, Beijing; 100084, ChinaSenseTime Research Institute, Beijing, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008954
SP  - 557
EP  - 573
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Video objection detection is challenging in the presence of appearance deterioration in certain video frames. One of typical solutions is to enhance per-frame features through aggregating neighboring frames. But the features of objects are usually not spatially calibrated across frames due to motion from object and camera. In this paper, we propose an end-to-end model called fully motion-aware network (MANet), which jointly calibrates the features of objects on both pixel-level and instance-level in a unified framework. The pixel-level calibration is flexible in modeling detailed motion while the instance-level calibration captures more global motion cues in order to be robust to occlusion. To our best knowledge, MANet is the first work that can jointly train the two modules and dynamically combine them according to the motion patterns. It achieves leading performance on the large-scale ImageNet VID dataset.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Calibration
KW  - Deterioration
KW  - Mobile ad hoc networks
KW  - Motion compensation
KW  - Object detection
KW  - Object recognition
KW  - Pixels
U2  - End to end
U2  - End-to-end models
U2  - Frame features
U2  - Instance-level
U2  - Motion pattern
U2  - Pixel level
U2  - Unified framework
U2  - Video object detections
DO  - 10.1007/978-3-030-01261-8_33
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_33
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Single Image Highlight Removal with a Sparse and Low-Rank Reflection Model
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Guo, Jie
A1  - Zhou, Zuojian
A1  - Wang, Limin
AD  - State Key Lab for Novel Software Technology, Nanjing University, Nanjing, ChinaSchool of Information Technology, Nanjing University of Chinese Medicine, Nanjing, China
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004566
SP  - 282
EP  - 298
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We propose a sparse and low-rank reflection model for specular highlight detection and removal using a single input image. This model is motivated by the observation that the specular highlight of a natural image usually has large intensity but is rather sparsely distributed while the remaining diffuse reflection can be well approximated by a linear combination of several distinct colors with a sparse and low-rank weighting matrix. We further impose the non-negativity constraint on the weighting matrix as well as the highlight component to ensure that the model is purely additive. With this reflection model, we reformulate the task of highlight removal as a constrained nuclear norm and l1 -norm minimization problem which can be solved effectively by the augmented Lagrange multiplier method. Experimental results show that our method performs well on both synthetic images and many real-world examples and is competitive with previous methods, especially in some challenging scenarios featuring natural illumination, hue-saturation ambiguity and strong noises.  2018, Springer Nature Switzerland AG.
KW  - Lagrange multipliers
KW  - Computer vision
U2  - Augmented lagrange multiplier methods
U2  - Diffuse reflection
U2  - L1-norm minimizations
U2  - Low-rank
U2  - Natural illumination
U2  - Non-negativity constraints
U2  - Sparse
U2  - Sparse and low ranks
DO  - 10.1007/978-3-030-01225-0_17
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_17
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Partial adversarial domain adaptation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Cao, Zhangjie
A1  - Ma, Lijia
A1  - Long, Mingsheng
A1  - Wang, Jianmin
AD  - School of Software, Tsinghua University, Beijing, ChinaNational Engineering Laboratory for Big Data Software, Beijing National Research Center for Information Science and Technology, Beijing, China
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406005867
SP  - 139
EP  - 155
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Domain adversarial learning aligns the feature distributions across the source and target domains in a two-player minimax game. Existing domain adversarial networks generally assume identical label space across different domains. In the presence of big data, there is strong motivation of transferring deep models from existing big domains to unknown small domains. This paper introduces partial domain adaptation as a new domain adaptation scenario, which relaxes the fully shared label space assumption to that the source label space subsumes the target label space. Previous methods typically match the whole source domain to the target domain, which are vulnerable to negative transfer for the partial domain adaptation problem due to the large mismatch between label spaces. We present Partial Adversarial Domain Adaptation (PADA), which simultaneously alleviates negative transfer by down-weighing the data of outlier source classes for training both source classifier and domain adversary, and promotes positive transfer by matching the feature distributions in the shared label space. Experiments show that PADA exceeds state-of-the-art results for partial domain adaptation tasks on several datasets.  Springer Nature Switzerland AG 2018.
KW  - Big data
KW  - Computer vision
U2  - Adversarial learning
U2  - Adversarial networks
U2  - Different domains
U2  - Domain adaptation
U2  - Existing domains
U2  - Feature distribution
U2  - Negative transfers
U2  - State of the art
DO  - 10.1007/978-3-030-01237-3_9
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_9
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Quantization mimic: Towards very tiny CNN for object detection
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wei, Yi
A1  - Pan, Xinyu
A1  - Qin, Hongwei
A1  - Ouyang, Wanli
A1  - Yan, Junjie
AD  - Tsinghua University, Beijing, ChinaThe Chinese University of Hong Kong, Hong KongSenseTime, Beijing, ChinaThe University of Sydney, SenseTime Computer Vision Research Group, Sydney; NSW, Australia
VL  - 11212 LNCS
PY  - 2018
U1  - 20184406006051
SP  - 274
EP  - 290
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a simple and general framework for training very tiny CNNs (e.g. VGG with the number of channels reduced to 1/32) for object detection. Due to limited representation ability, it is challenging to train very tiny networks for complicated tasks like detection. To the best of our knowledge, our method, called Quantization Mimic, is the first one focusing on very tiny networks. We utilize two types of acceleration methods: mimic and quantization. Mimic improves the performance of a student network by transfering knowledge from a teacher network. Quantization converts a full-precision network to a quantized one without large degradation of performance. If the teacher network is quantized, the search scope of the student network will be smaller. Using this feature of the quantization, we propose Quantization Mimic. It first quantizes the large network, then mimic a quantized small network. The quantization operation can help student network to better match the feature maps from teacher network. To evaluate our approach, we carry out experiments on various popular CNNs including VGG and Resnet, as well as different detection frameworks including Faster R-CNN and R-FCN. Experiments on Pascal VOC and WIDER FACE verify that our Quantization Mimic algorithm can be applied on various settings and outperforms state-of-the-art model acceleration methods given limited computing resouces.  Springer Nature Switzerland AG 2018.
KW  - Object detection
KW  - Acceleration control
KW  - Computer vision
KW  - Object recognition
KW  - Students
KW  - Teaching
U2  - Acceleration method
U2  - Detection framework
U2  - Mimic
U2  - MIMIC algorithm
U2  - Model compression
U2  - Quantization
U2  - Quantization operations
U2  - State of the art
DO  - 10.1007/978-3-030-01237-3_17
L2  - http://dx.doi.org/10.1007/978-3-030-01237-3_17
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Online Dictionary Learning for Approximate Archetypal Analysis
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Mei, Jieru
A1  - Wang, Chunyu
A1  - Zeng, Wenjun
AD  - Microsoft Research Asia, Beijing, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977819
SP  - 501
EP  - 516
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Archetypal analysis is an unsupervised learning approach which represents data by convex combinations of a set of archetypes. The archetypes generally correspond to the extremal points in the dataset and are learned by requiring them to be convex combinations of the training data. In spite of its nice property of interpretability, the method is slow. We propose a variant of archetypal analysis which scales gracefully to large datasets. The core idea is to decouple the binding between data and archetypes and require them to be unit normalized. Geometrically, the method learns a convex hull inside the unit sphere and represents the data by their projections on the closest surfaces of the convex hull. By minimizing the representation error, the method pushes the convex hull surfaces close to the regions of the sphere where the data reside. The vertices of the convex hull are the learned archetypes. We apply the method to human faces and poses to validate its effectiveness in the context of reconstructions and classifications.  2018, Springer Nature Switzerland AG.
KW  - Computational geometry
KW  - Computer vision
KW  - E-learning
U2  - Archetypal analysis
U2  - Convex combinations
U2  - Convex hull
U2  - Interpretability
U2  - Large datasets
U2  - Online dictionary learning
U2  - Sparsity
U2  - Training data
DO  - 10.1007/978-3-030-01219-9_30
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_30
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Face recognition with contrastive convolution
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Han, Chunrui
A1  - Shan, Shiguang
A1  - Kan, Meina
A1  - Wu, Shuzhe
A1  - Chen, Xilin
AD  - Key Lab of Intelligent Information Processing of Chinese Academy of Sciences, Institute of Computing Technology, CAS, Beijing; 100190, ChinaUniversity of Chinese Academy of Sciences, Beijing; 100049, ChinaCAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977318
SP  - 120
EP  - 135
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In current face recognition approaches with convolutional neural network (CNN), a pair of faces to compare are independently fed into the CNN for feature extraction. For both faces the same kernels are applied and hence the representation of a face stays fixed regardless of whom it is compared with. As for us humans, however, one generally focuses on varied characteristics of a face when comparing it with distinct persons as shown in Fig. 1. Inspired, we propose a novel CNN structure with what we referred to as contrastive convolution, which specifically focuses on the distinct characteristics between the two faces to compare, i.e., those contrastive characteristics. Extensive experiments on the challenging LFW, and IJB-A show that our proposed contrastive convolution significantly improves the vanilla CNN and achieves quite promising performance in face verification task.  Springer Nature Switzerland AG 2018.
KW  - Face recognition
KW  - Computer vision
KW  - Convolution
KW  - Neural networks
U2  - Convolutional neural network
U2  - Convolutional Neural Networks (CNN)
U2  - Face Verification
U2  - Kernel generator
DO  - 10.1007/978-3-030-01240-3_8
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_8
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Integral human pose regression
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Sun, Xiao
A1  - Xiao, Bin
A1  - Wei, Fangyin
A1  - Liang, Shuang
A1  - Wei, Yichen
AD  - Microsoft Research, Beijing, ChinaPeking University, Beijing, ChinaTongji University, Shanghai, China
VL  - 11210 LNCS
PY  - 2018
U1  - 20184305977411
SP  - 536
EP  - 553
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - State-of-the-art human pose estimation methods are based on heat map representation. In spite of the good performance, the representation has a few issues in nature, such as non-differentiable post-processing and quantization error. This work shows that a simple integral operation relates and unifies the heat map representation and joint regression, thus avoiding the above issues. It is differentiable, efficient, and compatible with any heat map based methods. Its effectiveness is convincingly validated via comprehensive ablation experiments under various settings, specifically on 3D pose estimation, for the first time.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Deep learning
KW  - Regression analysis
U2  - 3D pose estimation
U2  - Ablation experiments
U2  - Human pose estimations
U2  - Integral regression
U2  - Non-differentiable
U2  - Post processing
U2  - Quantization errors
U2  - State of the art
DO  - 10.1007/978-3-030-01231-1_33
L2  - http://dx.doi.org/10.1007/978-3-030-01231-1_33
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Salient objects in clutter: Bringing salient object detection to the foreground
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Fan, Deng-Ping
A1  - Cheng, Ming-Ming
A1  - Liu, Jiang-Jiang
A1  - Gao, Shang-Hua
A1  - Hou, Qibin
A1  - Borji, Ali
AD  - College of Computer Science, Nankai University, Tianjin, ChinaCRCV, University of Central Florida, Orlando; FL, United States
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006160
SP  - 196
EP  - 212
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We provide a comprehensive evaluation of salient object detection (SOD) models. Our analysis identifies a serious design bias of existing SOD datasets which assumes that each image contains at least one clearly outstanding salient object in low clutter. The design bias has led to a saturated high performance for state-of-the-art SOD models when evaluated on existing datasets. The models, however, still perform far from being satisfactory when applied to real-world daily scenes. Based on our analyses, we first identify 7 crucial aspects that a comprehensive and balanced dataset should fulfill. Then, we propose a new high quality dataset and update the previous saliency benchmark. Specifically, our SOC (Salient Objects in Clutter) dataset, includes images with salient and non-salient objects from daily object categories. Beyond object category annotations, each salient image is accompanied by attributes that reflect common challenges in real-world scenes. Finally, we report attribute-based performance assessment on our dataset.  Springer Nature Switzerland AG 2018.
KW  - Object detection
KW  - Benchmarking
KW  - Clutter (information theory)
KW  - Computer vision
KW  - Object recognition
KW  - Radar clutter
U2  - Attribute
U2  - Attribute-based
U2  - Comprehensive evaluation
U2  - Dataset
U2  - Object categories
U2  - Performance assessment
U2  - Salient object detection
U2  - State of the art
DO  - 10.1007/978-3-030-01267-0_12
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_12
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Dapeng
A1  - Li, Hongsheng
A1  - Liu, Xihui
A1  - Shen, Yantao
A1  - Shao, Jing
A1  - Yuan, Zejian
A1  - Wang, Xiaogang
AD  - CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Hong KongSenseTime Research, Hong KongXian Jiaotong University, Xian, China
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978947
SP  - 56
EP  - 73
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Person re-identification is an important task that requires learning discriminative visual features for distinguishing different person identities. Diverse auxiliary information has been utilized to improve the visual feature learning. In this paper, we propose to exploit natural language description as additional training supervisions for effective visual features. Compared with other auxiliary information, language can describe a specific person from more compact and semantic visual aspects, thus is complementary to the pixel-level image data. Our method not only learns better global visual feature with the supervision of the overall description but also enforces semantic consistencies between local visual and linguistic features, which is achieved by building global and local image-language associations. The global image-language association is established according to the identity labels, while the local association is based upon the implicit correspondences between image regions and noun phrases. Extensive experiments demonstrate the effectiveness of employing language as training supervisions with the two association schemes. Our method achieves state-of-the-art performance without utilizing any auxiliary information during testing and shows better performance than other joint embedding methods for the image-language association.  2018, Springer Nature Switzerland AG.
KW  - Visual languages
KW  - Computer vision
KW  - Image enhancement
KW  - Semantics
U2  - Association schemes
U2  - Auxiliary information
U2  - Global visual features
U2  - Image texts
U2  - Person re identifications
U2  - Semantic consistency
U2  - State-of-the-art performance
U2  - Visual representations
DO  - 10.1007/978-3-030-01270-0_4
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_4
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning Monocular Depth by Distilling Cross-Domain Stereo Networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Guo, Xiaoyang
A1  - Li, Hongsheng
A1  - Yi, Shuai
A1  - Ren, Jimmy
A1  - Wang, Xiaogang
AD  - CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong, Hong KongSenseTime Research, Beijing, China
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978887
SP  - 506
EP  - 523
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Monocular depth estimation aims at estimating a pixelwise depth map for a single image, which has wide applications in scene understanding and autonomous driving. Existing supervised and unsupervised methods face great challenges. Supervised methods require large amounts of depth measurement data, which are generally difficult to obtain, while unsupervised methods are usually limited in estimation accuracy. Synthetic data generated by graphics engines provide a possible solution for collecting large amounts of depth data. However, the large domain gaps between synthetic and realistic data make directly training with them challenging. In this paper, we propose to use the stereo matching network as a proxy to learn depth from synthetic data and use predicted stereo disparity maps for supervising the monocular depth estimation network. Cross-domain synthetic data could be fully utilized in this novel framework. Different strategies are proposed to ensure learned depth perception capability well transferred across different domains. Our extensive experiments show state-of-the-art results of monocular depth estimation on KITTI dataset.  2018, Springer Nature Switzerland AG.
KW  - Stereo image processing
KW  - Computer vision
KW  - Depth perception
U2  - Autonomous driving
U2  - Depth Estimation
U2  - Different domains
U2  - Perception capability
U2  - Scene understanding
U2  - Stereo matching
U2  - Supervised methods
U2  - Unsupervised method
DO  - 10.1007/978-3-030-01252-6_30
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_30
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Fine-Grained Video Categorization with Redundancy Reduction Attention
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhu, Chen
A1  - Tan, Xiao
A1  - Zhou, Feng
A1  - Liu, Xiao
A1  - Yue, Kaiyu
A1  - Ding, Errui
A1  - Ma, Yi
AD  - University of Maryland, College Park, United StatesDepartment of Computer Vision Technology (VIS), Baidu Inc., Beijing, ChinaBaidu Research, Sunnyvale, United StatesUniversity of California, Berkeley, United States
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976830
SP  - 139
EP  - 155
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - For fine-grained categorization tasks, videos could serve as a better source than static images as videos have a higher chance of containing discriminative patterns. Nevertheless, a video sequence could also contain a lot of redundant and irrelevant frames. How to locate critical information of interest is a challenging task. In this paper, we propose a new network structure, known as Redundancy Reduction Attention (RRA), which learns to focus on multiple discriminative patterns by suppressing redundant feature channels. Specifically, it firstly summarizes the video by weight-summing all feature vectors in the feature maps of selected frames with a spatio-temporal soft attention, and then predicts which channels to suppress or to enhance according to this summary with a learned non-linear transform. Suppression is achieved by modulating the feature maps and threshing out weak activations. The updated feature maps are then used in the next iteration. Finally, the video is classified based on multiple summaries. The proposed method achieves outstanding performances in multiple video classification datasets. Furthermore, we have collected two large-scale video datasets, YouTube-Birds and YouTube-Cars, for future researches on fine-grained video categorization. The datasets are available at http://www.cs.umd.edu/~chenzhu/fgvc.  2018, Springer Nature Switzerland AG.
KW  - Classification (of information)
KW  - Computer vision
KW  - Iterative methods
KW  - Mathematical transformations
KW  - Redundancy
U2  - Attention mechanisms
U2  - Feature vectors
U2  - Multiple videos
U2  - Network structures
U2  - Redundancy reductions
U2  - Redundant features
U2  - Video categorization
U2  - Video sequences
DO  - 10.1007/978-3-030-01228-1_9
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_9
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep variational metric learning
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Lin, Xudong
A1  - Duan, Yueqi
A1  - Dong, Qiyuan
A1  - Lu, Jiwen
A1  - Zhou, Jie
AD  - Tsinghua University, Beijing; 100084, China
VL  - 11219 LNCS
PY  - 2018
U1  - 20184406006193
SP  - 714
EP  - 729
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Deep metric learning has been extensively explored recently, which trains a deep neural network to produce discriminative embedding features. Most existing methods usually enforce the model to be indiscriminating to intra-class variance, which makes the model over-fitting to the training set to minimize loss functions on these specific changes and leads to low generalization power on unseen classes. However, these methods ignore a fact that in the central latent space, the distribution of variance within classes is actually independent on classes. In this paper, we propose a deep variational metric learning (DVML) framework to explicitly model the intra-class variance and disentangle the intra-class invariance, namely, the class centers. With the learned distribution of intra-class variance, we can simultaneously generate discriminative samples to improve robustness. Our method is applicable to most of existing metric learning algorithms, and extensive experiments on three benchmark datasets including CUB-200-2011, Cars196 and Stanford Online Products show that our DVML significantly boosts the performance of currently popular deep metric learning methods.  Springer Nature Switzerland AG 2018.
KW  - Learning algorithms
KW  - Benchmarking
KW  - Computer vision
KW  - Deep neural networks
U2  - Auto encoders
U2  - Benchmark datasets
U2  - Class Centers
U2  - Discriminative samples generating
U2  - Loss functions
U2  - Metric learning
U2  - Online products
U2  - Training sets
DO  - 10.1007/978-3-030-01267-0_42
L2  - http://dx.doi.org/10.1007/978-3-030-01267-0_42
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Attribute-guided face generation using conditional cycleGAN
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Lu, Yongyi
A1  - Tai, Yu-Wing
A1  - Tang, Chi-Keung
AD  - The Hong Kong University of Science and Technology, Hong KongTencent Youtu, Shenzhen, China
VL  - 11216 LNCS
PY  - 2018
U1  - 20184305977440
SP  - 293
EP  - 308
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We are interested in attribute-guided face generation: given a low-res face input image, an attribute vector that can be extracted from a high-res image (attribute image), our new method generates a high-res face image for the low-res input that satisfies the given attributes. To address this problem, we condition the CycleGAN and propose conditional CycleGAN, which is designed to (1) handle unpaired training data because the training low/high-res and high-res attribute images may not necessarily align with each other, and to (2) allow easy control of the appearance of the generated face via the input attributes. We demonstrate high-quality results on the attribute-guided conditional CycleGAN, which can synthesize realistic face images with appearance easily controlled by user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using the attribute image as identity to produce the corresponding conditional vector and by incorporating a face verification network, the attribute-guided network becomes the identity-guided conditional CycleGAN which produces high-quality and interesting results on identity transfer. We demonstrate three applications on identity-guided conditional CycleGAN: identity-preserving face superresolution, face swapping, and frontal face generation, which consistently show the advantage of our new method.  Springer Nature Switzerland AG 2018.
KW  - Quality control
KW  - Computer vision
KW  - Cosmetics
KW  - Image segmentation
U2  - Attribute
U2  - Attribute vectors
U2  - Face generation
U2  - Face super-resolution
U2  - Face swapping
U2  - Face Verification
U2  - Frontal faces
U2  - Training data
DO  - 10.1007/978-3-030-01258-8_18
L2  - http://dx.doi.org/10.1007/978-3-030-01258-8_18
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Neural Network Encapsulation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Li, Hongyang
A1  - Guo, Xiaoyang
A1  - Dai, Bo
A1  - Ouyang, Wanli
A1  - Wang, Xiaogang
AD  - The Chinese University of Hong Kong, Shatin, Hong KongSenseTime Computer Vision Research Group, The University of Sydney, Sydney, Australia
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978871
SP  - 266
EP  - 282
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - A capsule is a collection of neurons which represents different variants of a pattern in the network. The routing scheme ensures only certain capsules which resemble lower counterparts in the higher layer should be activated. However, the computational complexity becomes a bottleneck for scaling up to larger networks, as lower capsules need to correspond to each and every higher capsule. To resolve this limitation, we approximate the routing process with two branches: a master branch which collects primary information from its direct contact in the lower layer and an aide branch that replenishes master based on pattern variants encoded in other lower capsules. Compared with previous iterative and unsupervised routing scheme, these two branches are communicated in a fast, supervised and one-time pass fashion. The complexity and runtime of the model are therefore decreased by a large margin. Motivated by the routing to make higher capsule have agreement with lower capsule, we extend the mechanism as a compensation for the rapid loss of information in nearby layers. We devise a feedback agreement unit to send back higher capsules as feedback. It could be regarded as an additional regularization to the network. The feedback agreement is achieved by comparing the optimal transport divergence between two distributions (lower and higher capsules). Such an add-on witnesses a unanimous gain in both capsule and vanilla networks. Our proposed EncapNet performs favorably better against previous state-of-the-arts on CIFAR10/100, SVHN and a subset of ImageNet.  2018, Springer Nature Switzerland AG.
KW  - Complex networks
KW  - Computer vision
KW  - Encapsulation
KW  - Network architecture
KW  - Network routing
KW  - Routing protocols
U2  - Direct contact
U2  - Feature learning
U2  - Larger networks
U2  - Network architecture design
U2  - Optimal transport
U2  - Routing process
U2  - Routing scheme
U2  - State of the art
DO  - 10.1007/978-3-030-01252-6_16
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_16
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Faces as lighting probes via unsupervised deep highlight extraction
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yi, Renjiao
A1  - Zhu, Chenyang
A1  - Tan, Ping
A1  - Lin, Stephen
AD  - Simon Fraser University, Burnaby, CanadaNational University of Defense Technology, Changsha, ChinaMicrosoft Research, Beijing, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977492
SP  - 321
EP  - 338
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We present a method for estimating detailed scene illumination using human faces in a single image. In contrast to previous works that estimate lighting in terms of low-order basis functions or distant point lights, our technique estimates illumination at a higher precision in the form of a non-parametric environment map. Based on the observation that faces can exhibit strong highlight reflections from a broad range of lighting directions, we propose a deep neural network for extracting highlights from faces, and then trace these reflections back to the scene to acquire the environment map. Since real training data for highlight extraction is very limited, we introduce an unsupervised scheme for finetuning the network on real images, based on the consistent diffuse chromaticity of a given face seen in multiple real images. In tracing the estimated highlights to the environment, we reduce the blurring effect of skin reflectance on reflected light through a deconvolution determined by prior knowledge on face material properties. Comparisons to previous techniques for highlight extraction and illumination estimation show the state-of-the-art performance of this approach on a variety of indoor and outdoor scenes.  Springer Nature Switzerland AG 2018.
KW  - Deep neural networks
KW  - Computer vision
KW  - Extraction
KW  - Lighting
KW  - Unsupervised learning
U2  - Basis functions
U2  - Blurring effect
U2  - Illumination estimation
U2  - Non-parametric
U2  - Outdoor scenes
U2  - Prior knowledge
U2  - Reflected light
U2  - State-of-the-art performance
DO  - 10.1007/978-3-030-01240-3_20
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_20
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Efficient global point cloud registration by matching rotation invariant features through translation search
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Yinlong
A1  - Wang, Chen
A1  - Song, Zhijian
A1  - Wang, Manning
AD  - Digital Medical Research Center, School of Basic Medical Science, Fudan University, Shanghai; 200032, ChinaShanghai Key Laboratory of Medical Imaging Computing and Computer Assisted Intervention, Shanghai; 200032, China
VL  - 11216 LNCS
PY  - 2018
U1  - 20184305977451
SP  - 460
EP  - 474
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Three-dimensional rigid point cloud registration has many applications in computer vision and robotics. Local methods tend to fail, causing global methods to be needed, when the relative transformation is large or the overlap ratio is small. Most existing global methods utilize BnB optimization over the 6D parameter space of SE(3). Such methods are usually very slow because the time complexity of BnB optimization is exponential in the dimensionality of the parameter space. In this paper, we decouple the optimization of translation and rotation, and we propose a fast BnB algorithm to globally optimize the 3D translation parameter first. The optimal rotation is then calculated by utilizing the global optimal translation found by the BnB algorithm. The separate optimization of translation and rotation is realized by using a newly proposed rotation invariant feature. Experiments on challenging data sets demonstrate that the proposed method outperforms state-of-the-art global methods in terms of both speed and accuracy.  Springer Nature Switzerland AG 2018.
KW  - Translation (languages)
KW  - Computer vision
KW  - Global optimization
KW  - Rotation
KW  - Surface measurement
U2  - Optimal rotations
U2  - Overlap ratios
U2  - Parameter spaces
U2  - Point cloud registration
U2  - Relative transformation
U2  - Rotation invariant features
U2  - State of the art
U2  - Time complexity
DO  - 10.1007/978-3-030-01258-8_28
L2  - http://dx.doi.org/10.1007/978-3-030-01258-8_28
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Data-Driven Sparse Structure Selection for Deep Neural Networks
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Huang, Zehao
A1  - Wang, Naiyan
AD  - TuSimple, Beijing, China
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978924
SP  - 317
EP  - 334
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Deep convolutional neural networks have liberated its extraordinary power on various tasks. However, it is still very challenging to deploy state-of-the-art models into real-world applications due to their high computational complexity. How can we design a compact and effective network without massive experiments and expert knowledge? In this paper, we propose a simple and effective framework to learn and prune deep models in an end-to-end manner. In our framework, a new type of parameter  scaling factor is first introduced to scale the outputs of specific structures, such as neurons, groups or residual blocks. Then we add sparsity regularizations on these factors, and solve this optimization problem by a modified stochastic Accelerated Proximal Gradient (APG) method. By forcing some of the factors to zero, we can safely remove the corresponding structures, thus prune the unimportant parts of a CNN. Comparing with other structure selection methods that may need thousands of trials or iterative fine-tuning, our method is trained fully end-to-end in one training pass without bells and whistles. We evaluate our method, Sparse Structure Selection with several state-of-the-art CNNs, and demonstrate very promising results with adaptive depth and width selection. Code is available at: https://github.com/huangzehao/sparse-structure-selection.  2018, Springer Nature Switzerland AG.
KW  - Deep neural networks
KW  - Computer vision
KW  - Iterative methods
KW  - Neural networks
KW  - Signaling
KW  - Stochastic systems
U2  - Deep convolutional neural networks
U2  - Deep networks
U2  - Expert knowledge
U2  - Optimization problems
U2  - Sparse
U2  - Sparsity regularizations
U2  - State of the art
U2  - Structure selection
DO  - 10.1007/978-3-030-01270-0_19
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_19
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Deep regression tracking with shrinkage loss
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Lu, Xiankai
A1  - Ma, Chao
A1  - Ni, Bingbing
A1  - Yang, Xiaokang
A1  - Reid, Ian
A1  - Yang, Ming-Hsuan
AD  - Shanghai Jiao Tong University, Shanghai, ChinaThe University of Adelaide, Adelaide, AustraliaInception Institute of Artificial Intelligence, Abu Dhabi, United Arab EmiratesSJTU-UCLA Joint Center for Machine Perception and Inference, Shanghai, ChinaUniversity of California at Merced, Merced, United StatesGoogle Inc., Menlo Park, United States
VL  - 11218 LNCS
PY  - 2018
U1  - 20184406020309
SP  - 369
EP  - 386
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Regression trackers directly learn a mapping from regularly dense samples of target objects to soft labels, which are usually generated by a Gaussian function, to estimate target positions. Due to the potential for fast-tracking and easy implementation, regression trackers have recently received increasing attention. However, state-of-the-art deep regression trackers do not perform as well as discriminative correlation filters (DCFs) trackers. We identify the main bottleneck of training regression networks as extreme foreground-background data imbalance. To balance training data, we propose a novel shrinkage loss to penalize the importance of easy training data. Additionally, we apply residual connections to fuse multiple convolutional layers as well as their output response maps. Without bells and whistles, the proposed deep regression tracking method performs favorably against state-of-the-art trackers, especially in comparison with DCFs trackers, on five benchmark datasets including OTB-2013, OTB-2015, Temple-128, UAV-123 and VOT-2016.  2018, Springer Nature Switzerland AG.
KW  - Regression analysis
KW  - Aircraft detection
KW  - Computer vision
KW  - Dispersion compensation
KW  - Shrinkage
KW  - Signaling
U2  - Balance training
U2  - Benchmark datasets
U2  - Correlation filters
U2  - Gaussian functions
U2  - Object Tracking
U2  - Output response
U2  - State of the art
U2  - Tracking method
DO  - 10.1007/978-3-030-01264-9_22
L2  - http://dx.doi.org/10.1007/978-3-030-01264-9_22
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Part-Activated Deep Reinforcement Learning for Action Prediction
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Chen, Lei
A1  - Lu, Jiwen
A1  - Song, Zhanjie
A1  - Zhou, Jie
AD  - Tianjin University, Tianjin, ChinaTsinghua University, Beijing, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977814
SP  - 435
EP  - 451
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - In this paper, we propose a part-activated deep reinforcement learning (PA-DRL) method for action prediction. Most existing methods for action prediction utilize the evolution of whole frames to model actions, which cannot avoid the noise of the current action, especially in the early prediction. Moreover, the loss of structural information of human body diminishes the capacity of features to describe actions. To address this, we design the PA-DRL to exploit the structure of the human body by extracting skeleton proposals under a deep reinforcement learning framework. Specifically, we extract features from different parts of the human body individually and activate the action-related parts in features to enhance the representation. Our method not only exploits the structure information of the human body, but also considers the saliency part for expressing actions. We evaluate our method on three popular action prediction datasets: UT-Interaction, BIT-Interaction and UCF101. Our experimental results demonstrate that our method achieves the performance with state-of-the-arts.  2018, Springer Nature Switzerland AG.
KW  - Deep learning
KW  - Computer vision
KW  - Forecasting
KW  - Musculoskeletal system
KW  - Reinforcement learning
U2  - Action prediction
U2  - Early prediction
U2  - Human bodies
U2  - Skeleton
U2  - State of the art
U2  - Structural information
U2  - Structure information
DO  - 10.1007/978-3-030-01219-9_26
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_26
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Dependency-Aware Attention Control for Unconstrained Face Recognition with Image Sets
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Liu, Xiaofeng
A1  - Kumar, B. V. K. Vijaya
A1  - Yang, Chao
A1  - Tang, Qingming
A1  - You, Jane
AD  - Carnegie Mellon University, Pittsburgh; PA; 15213, United StatesUniversity of Southern California, Los Angeles; CA; 90089, United StatesToyota Technological Institute at Chicago, Chicago; IL; 60637, United StatesThe Hong Kong Polytechnic University, Hong Kong
VL  - 11215 LNCS
PY  - 2018
U1  - 20184305978891
SP  - 573
EP  - 590
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - This paper targets the problem of image set-based face verification and identification. Unlike traditional single media (an image or video) setting, we encounter a set of heterogeneous contents containing orderless images and videos. The importance of each image is usually considered either equal or based on their independent quality assessment. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in the latent space. Specifically, we first present a dependency-aware attention control (DAC) network, which resorts to actor-critic reinforcement learning for sequential attention decision of each image embedding to fully exploit the rich correlation cues among the unordered images. Moreover, we introduce its sample-efficient variant with off-policy experience replay to speed up the learning process. The pose-guided representation scheme can further boost the performance at the extremes of the pose variation.  2018, Springer Nature Switzerland AG.
KW  - Face recognition
KW  - Computer vision
KW  - Deep learning
KW  - Markov processes
KW  - Reinforcement learning
U2  - Actor critic
U2  - Actor-critic reinforcement learning
U2  - Attention control
U2  - Face Verification
U2  - Markov Decision Processes
U2  - Quality assessment
U2  - Representation schemes
U2  - Set-to-set
DO  - 10.1007/978-3-030-01252-6_34
L2  - http://dx.doi.org/10.1007/978-3-030-01252-6_34
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - FishEyeRecNet: A multi-context collaborative deep network for fisheye image rectification
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yin, Xiaoqing
A1  - Wang, Xinchao
A1  - Yu, Jun
A1  - Zhang, Maojun
A1  - Fua, Pascal
A1  - Tao, Dacheng
AD  - UBTECH Sydney AI Center, SIT, FEIT, University of Sydney, Sydney, AustraliaNational University of Defense Technology, Changsha, ChinaStevens Institute of Technology, Hoboken, United StatesHangzhou Dianzi University, Hangzhou, ChinaEcole Polytechnique Federale de Lausanne, Lausanne, Switzerland
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978785
SP  - 475
EP  - 490
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Images captured by fisheye lenses violate the pinhole camera assumption and suffer from distortions. Rectification of fisheye images is therefore a crucial preprocessing step for many computer vision applications. In this paper, we propose an end-to-end multi-context collaborative deep network for removing distortions from single fisheye images. In contrast to conventional approaches, which focus on extracting hand-crafted features from input images, our method learns high-level semantics and low-level appearance features simultaneously to estimate the distortion parameters. To facilitate training, we construct a synthesized dataset that covers various scenes and distortion parameter settings. Experiments on both synthesized and real-world datasets show that the proposed model significantly outperforms current state of the art methods. Our code and synthesized dataset will be made publicly available.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Pinhole cameras
KW  - Semantics
U2  - Computer vision applications
U2  - Conventional approach
U2  - Deep networks
U2  - Distortion parameters
U2  - Fisheye images
U2  - High level semantics
U2  - Real-world datasets
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01249-6_29
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_29
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Multi-Scale Structure-Aware Network for Human Pose Estimation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Ke, Lipeng
A1  - Chang, Ming-Ching
A1  - Qi, Honggang
A1  - Lyu, Siwei
AD  - University of Chinese Academy of Sciences, Beijing, ChinaUniversity at Albany, State University of New York, New York City; NY, United States
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005993
SP  - 731
EP  - 746
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We develop a robust multi-scale structure-aware neural network for human pose estimation. This method improves the recent deep conv-deconv hourglass models with four key improvements: (1) multi-scale supervision to strengthen contextual feature learning in matching body keypoints by combining feature heatmaps across scales, (2) multi-scale regression network at the end to globally optimize the structural matching of the multi-scale features, (3) structure-aware loss used in the intermediate supervision and at the regression to improve the matching of keypoints and respective neighbors to infer a higher-order matching configurations, and (4) a keypoint masking training scheme that can effectively fine-tune our network to robustly localize occluded keypoints via adjacent matches. Our method can effectively improve state-of-the-art pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. This multi-scale supervision tightly integrates with the regression network to effectively (i) localize keypoints using the ensemble of multi-scale features, and (ii) infer global pose configuration by maximizing structural consistencies across multiple keypoints and scales. The keypoint masking training enhances these advantages to focus learning on hard occlusion samples. Our method achieves the leading position in the MPII challenge leaderboard among the state-of-the-art methods.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Regression analysis
U2  - Contextual feature
U2  - Higher-order matching
U2  - Human pose estimations
U2  - Multi-scale
U2  - Multi-scale features
U2  - Multi-scale structures
U2  - State-of-the-art methods
U2  - Structural matching
DO  - 10.1007/978-3-030-01216-8_44
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_44
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Learning dynamic memory networks for object tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yang, Tianyu
A1  - Chan, Antoni B.
AD  - Department of Computer Science, City University of Hong Kong, Hong Kong
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977481
SP  - 153
EP  - 169
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Template-matching methods for visual tracking have gained popularity recently due to their comparable performance and fast speed. However, they lack effective ways to adapt to changes in the target objects appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adapt the template to the targets appearance variations during tracking. An LSTM is used as a memory controller, where the input is the search feature map and the outputs are the control signals for the reading and writing process of the memory block. As the location of the target is at first unknown in the search feature map, an attention mechanism is applied to concentrate the LSTM input on the potential target. To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template. Unlike tracking-by-detection methods where the objects information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the targets appearance changes by updating the external memory. Moreover, unlike other tracking methods where the model capacity is fixed after offline training  the capacity of our tracker can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Extensive experiments on OTB and VOT demonstrates that our tracker MemTrack performs favorably against state-of-the-art tracking methods while retaining real-time speed of 50 fps.  Springer Nature Switzerland AG 2018.
KW  - Long short-term memory
KW  - Computer vision
KW  - Object detection
KW  - Target tracking
KW  - Template matching
U2  - Attention mechanisms
U2  - Memory requirements
U2  - Object information
U2  - Potential targets
U2  - Template learning
U2  - Template matching method
U2  - Tracking accuracy
U2  - Tracking by detections
DO  - 10.1007/978-3-030-01240-3_10
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_10
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Towards Human-Level License Plate Recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhuang, Jiafan
A1  - Hou, Saihui
A1  - Wang, Zilei
A1  - Zha, Zheng-Jun
AD  - University of Science and Technology of China, Hefei, China
VL  - 11207 LNCS
PY  - 2018
U1  - 20184305977806
SP  - 314
EP  - 329
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - License plate recognition (LPR) is a fundamental component of various intelligent transport systems, which is always expected to be accurate and efficient enough. In this paper, we propose a novel LPR framework consisting of semantic segmentation and character counting, towards achieving human-level performance. Benefiting from innovative structure, our method can recognize a whole license plate once rather than conducting character detection or sliding window followed by per-character recognition. Moreover, our method can achieve higher recognition accuracy due to more effectively exploiting global information and avoiding sensitive character detection, and is time-saving due to eliminating one-by-one character recognition. Finally, we experimentally verify the effectiveness of the proposed method on two public datasets (AOLP and Media Lab) and our License Plate Dataset. The results demonstrate our method significantly outperforms the previous state-of-the-art methods, and achieves the accuracies of more than 99% for almost all settings.  2018, Springer Nature Switzerland AG.
KW  - Optical character recognition
KW  - Computer vision
KW  - Intelligent systems
KW  - License plates (automobile)
KW  - Neural networks
KW  - Plates (structural components)
KW  - Semantics
KW  - Traffic control
U2  - Character counting
U2  - Convolutional Neural Networks (CNN)
U2  - Human-level performance
U2  - Innovative structures
U2  - Intelligent transport systems
U2  - License plate recognition
U2  - Semantic segmentation
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01219-9_19
L2  - http://dx.doi.org/10.1007/978-3-030-01219-9_19
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Toward characteristic-preserving image-based virtual try-on network
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Wang, Bochao
A1  - Zheng, Huabin
A1  - Liang, Xiaodan
A1  - Chen, Yimin
A1  - Lin, Liang
A1  - Yang, Meng
AD  - Sun Yat-sen University, Guangzhou, ChinaSenseTime Group Limited, Beijing, China
VL  - 11217 LNCS
PY  - 2018
U1  - 20184406008957
SP  - 607
EP  - 623
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Image-based virtual try-on systems for fitting a new in-shop clothes into a person image have attracted increasing research attention, yet is still challenging. A desirable pipeline should not only transform the target clothes into the most fitting shape seamlessly but also preserve well the clothes identity in the generated image, that is, the key characteristics (e.g. texture, logo, embroidery) that depict the original clothes. However, previous image-conditioned generation works fail to meet these critical requirements towards the plausible virtual try-on performance since they fail to handle large spatial misalignment between the input image and target clothes. Prior work explicitly tackled spatial deformation using shape context matching, but failed to preserve clothing details due to its coarse-to-fine strategy. In this work, we propose a new fully-learnable Characteristic-Preserving Virtual Try-On Network (CP-VTON) for addressing all real-world challenges in this task. First, CP-VTON learns a thin-plate spline transformation for transforming the in-shop clothes into fitting the body shape of the target person via a new Geometric Matching Module (GMM) rather than computing correspondences of interest points as prior works did. Second, to alleviate boundary artifacts of warped clothes and make the results more realistic, we employ a Try-On Module that learns a composition mask to integrate the warped clothes and the rendered image to ensure smoothness. Extensive experiments on a fashion dataset demonstrate our CP-VTON achieves the state-of-the-art virtual try-on performance both qualitatively and quantitatively.  Springer Nature Switzerland AG 2018.
KW  - Pipe fittings
KW  - Computer vision
KW  - Mathematical transformations
KW  - Pore pressure
U2  - Characteristic-preserving
U2  - Coarse-to-fine strategy
U2  - Image alignment
U2  - Key characteristics
U2  - Spatial deformation
U2  - Spatial misalignments
U2  - Thin plate spline
U2  - Virtual try-on
DO  - 10.1007/978-3-030-01261-8_36
L2  - http://dx.doi.org/10.1007/978-3-030-01261-8_36
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Structured siamese network for real-time visual tracking
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhang, Yunhua
A1  - Wang, Lijun
A1  - Qi, Jinqing
A1  - Wang, Dong
A1  - Feng, Mengyang
A1  - Lu, Huchuan
AD  - School of Information and Communication Engineering, Dalian University of Technology, Dalian, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977494
SP  - 355
EP  - 370
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Local structures of target objects are essential for robust tracking. However, existing methods based on deep neural networks mostly describe the target appearance from the global view, leading to high sensitivity to non-rigid appearance change and partial occlusion. In this paper, we circumvent this issue by proposing a local structure learning method, which simultaneously considers the local patterns of the target and their structural relationships for more accurate target tracking. To this end, a local pattern detection module is designed to automatically identify discriminative regions of the target objects. The detection results are further refined by a message passing module, which enforces the structural context among local patterns to construct local structures. We show that the message passing module can be formulated as the inference process of a conditional random field (CRF) and implemented by differentiable operations, allowing the entire model to be trained in an end-to-end manner. By considering various combinations of the local structures, our tracker is able to form various types of structure patterns. Target tracking is finally achieved by a matching procedure of the structure patterns between target template and candidates. Extensive evaluations on three benchmark data sets demonstrate that the proposed tracking algorithm performs favorably against state-of-the-art methods while running at a highly efficient speed of 45 fps.  Springer Nature Switzerland AG 2018.
KW  - Target tracking
KW  - Clutter (information theory)
KW  - Computer vision
KW  - Deep learning
KW  - Deep neural networks
KW  - Message passing
KW  - Object detection
KW  - Random processes
KW  - Surface discharges
U2  - Conditional random field
U2  - High sensitivity
U2  - Inference process
U2  - Local pattern detection
U2  - Partial occlusions
U2  - State-of-the-art methods
U2  - Structural relationship
U2  - Tracking algorithm
DO  - 10.1007/978-3-030-01240-3_22
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_22
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - DDRNet: Depth map denoising and refinement for consumer depth cameras using cascaded CNNs
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Yan, Shi
A1  - Wu, Chenglei
A1  - Wang, Lizhen
A1  - Xu, Feng
A1  - An, Liang
A1  - Guo, Kaiwen
A1  - Liu, Yebin
AD  - Tsinghua University, Beijing, ChinaFacebook Reality Labs, Pittsburgh, United StatesGoogle Inc, Mountain View; CA, United States
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978765
SP  - 155
EP  - 171
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Consumer depth sensors are more and more popular and come to our daily lives marked by its recent integration in the latest Iphone X. However, they still suffer from heavy noises which limit their applications. Although plenty of progresses have been made to reduce the noises and boost geometric details, due to the inherent illness and the real-time requirement, the problem is still far from been solved. We propose a cascaded Depth Denoising and Refinement Network (DDRNet) to tackle this problem by leveraging the multi-frame fused geometry and the accompanying high quality color image through a joint training strategy. The rendering equation is exploited in our network in an unsupervised manner. In detail, we impose an unsupervised loss based on the light transport to extract the high-frequency geometry. Experimental results indicate that our network achieves real-time single depth enhancement on various categories of scenes. Thanks to the well decoupling of the low and high frequency information in the cascaded network, we achieve superior performance over the state-of-the-art techniques.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Cameras
KW  - Geometry
KW  - Neural networks
KW  - Unsupervised learning
KW  - Visibility
U2  - Convolutional neural network
U2  - Depth camera
U2  - Depth enhancement
U2  - DynamicFusion
U2  - High quality color
U2  - Low and high frequencies
U2  - Real time requirement
U2  - State-of-the-art techniques
DO  - 10.1007/978-3-030-01249-6_10
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_10
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - BOP: Benchmark for 6D object pose estimation
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Hoda, Toma
A1  - Michel, Frank
A1  - Brachmann, Eric
A1  - Kehl, Wadim
A1  - Buch, Anders Glent
A1  - Kraft, Dirk
A1  - Drost, Bertram
A1  - Vidal, Joel
A1  - Ihrke, Stephan
A1  - Zabulis, Xenophon
A1  - Sahin, Caner
A1  - Manhardt, Fabian
A1  - Tombari, Federico
A1  - Kim, Tae-Kyun
A1  - Matas, Jii
A1  - Rother, Carsten
AD  - CTU in Prague, Prague, Czech RepublicTU Dresden, Dresden, GermanyHeidelberg University, Heidelberg, GermanyToyota Research Institute, Los Altos, United StatesUniversity of Southern Denmark, Odense, DenmarkMVTec Software, Munich, GermanyTaiwan Tech, Taipei, TaiwanFORTH Heraklion, Heraklion, GreeceImperial College London, London, United KingdomTU Munich, Munich, Germany
VL  - 11214 LNCS
PY  - 2018
U1  - 20184305978775
SP  - 19
EP  - 35
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We propose a benchmark for 6D pose estimation of a rigid object from a single RGB-D input image. The training data consists of a texture-mapped 3D object model or images of the object in known 6D poses. The benchmark comprises of: (i) eight datasets in a unified format that cover different practical scenarios, including two new datasets focusing on varying lighting conditions, (ii) an evaluation methodology with a pose-error function that deals with pose ambiguities, (iii) a comprehensive evaluation of 15 diverse recent methods that captures the status quo of the field, and (iv) an online evaluation system that is open for continuous submission of new results. The evaluation shows that methods based on point-pair features currently perform best, outperforming template matching methods, learning-based methods and methods based on 3D local features. The project website is available at bop.felk.cvut.cz.  Springer Nature Switzerland AG 2018.
KW  - Computer vision
KW  - Image segmentation
KW  - Template matching
U2  - 3D object modeling
U2  - Comprehensive evaluation
U2  - Continuous submission
U2  - Evaluation methodologies
U2  - Learning-based methods
U2  - Online evaluation systems
U2  - Pose ambiguities
U2  - Template matching method
DO  - 10.1007/978-3-030-01249-6_2
L2  - http://dx.doi.org/10.1007/978-3-030-01249-6_2
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - PSDF Fusion: Probabilistic signed distance function for on-the-fly 3d data fusion and scene reconstruction
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Dong, Wei
A1  - Wang, Qiuyuan
A1  - Wang, Xin
A1  - Zha, Hongbin
AD  - Key Laboratory of Machine Perception (MOE), School of EECS, Peking University, Beijing, ChinaCooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China
VL  - 11213 LNCS
PY  - 2018
U1  - 20184305977517
SP  - 714
EP  - 730
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - We propose a novel 3D spatial representation for data fusion and scene reconstruction. Probabilistic Signed Distance Function (Probabilistic SDF, PSDF) is proposed to depict uncertainties in the 3D space. It is modeled by a joint distribution describing SDF value and its inlier probability, reflecting input data quality and surface geometry. A hybrid data structure involving voxel, surfel, and mesh is designed to fully exploit the advantages of various prevalent 3D representations. Connected by PSDF, these components reasonably cooperate in a consistent framework. Given sequential depth measurements, PSDF can be incrementally refined with less ad hoc parametric Bayesian updating. Supported by PSDF and the efficient 3D data representation, high-quality surfaces can be extracted on-the-fly, and in return contribute to reliable data fusion using the geometry information. Experiments demonstrate that our system reconstructs scenes with higher model quality and lower redundancy, and runs faster than existing online mesh generation systems.  Springer Nature Switzerland AG 2018.
KW  - Image reconstruction
KW  - Computer graphics
KW  - Computer vision
KW  - Data fusion
KW  - Mesh generation
KW  - Online systems
KW  - Probability distributions
U2  - 3D data representations
U2  - Bayesian updating
U2  - Geometry information
U2  - High quality surface
U2  - Hybrid data structures
U2  - Scene reconstruction
U2  - Signed distance function
U2  - Spatial representations
DO  - 10.1007/978-3-030-01240-3_43
L2  - http://dx.doi.org/10.1007/978-3-030-01240-3_43
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Sun, Yifan
A1  - Zheng, Liang
A1  - Yang, Yi
A1  - Tian, Qi
A1  - Wang, Shengjin
AD  - Department of Electronic Engineering, Tsinghua University, Beijing, ChinaResearch School of Computer Science, Australian National University, Canberra, AustraliaCentre for Artificial Intelligence, University of Technology Sydney, Ultimo, AustraliaHuawei Noahs Ark Lab, University of Texas at San Antonio, San Antonio, United States
VL  - 11208 LNCS
PY  - 2018
U1  - 20184406004581
SP  - 501
EP  - 518
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Employing part-level features offers fine-grained information for pedestrian image description. A prerequisite of part discovery is that each part should be well located. Instead of using external resources like pose estimator, we consider content consistency within each part for precise part location. Specifically, we target at learning discriminative part-informed features for person retrieval and make two contributions. (i) A network named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. With a uniform partition strategy, PCB achieves competitive results with the state-of-the-art methods, proving itself as a strong convolutional baseline for person retrieval. (ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs outliers in each part, which are in fact more similar to other parts. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, surpassing the state of the art by a large margin. Code is available at: https://github.com/syfafterzy/PCB_RPP.  2018, Springer Nature Switzerland AG.
KW  - Convolution
KW  - Computer vision
KW  - Organic pollutants
KW  - Polychlorinated biphenyls
KW  - Statistics
U2  - Content consistency
U2  - External resources
U2  - Image descriptions
U2  - Part levels
U2  - Part refinement
U2  - Person retrieval
U2  - State of the art
U2  - State-of-the-art methods
DO  - 10.1007/978-3-030-01225-0_30
L2  - http://dx.doi.org/10.1007/978-3-030-01225-0_30
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Sun, Ming
A1  - Yuan, Yuchen
A1  - Zhou, Feng
A1  - Ding, Errui
AD  - Department of Computer Vision Technology (VIS), Baidu Inc., Beijing, ChinaBaidu Research, Beijing, China
VL  - 11220 LNCS
PY  - 2018
U1  - 20184305978957
SP  - 834
EP  - 850
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Attention-based learning for fine-grained image recognition remains a challenging task, where most of the existing methods treat each object part in isolation, while neglecting the correlations among them. In addition, the multi-stage or multi-scale mechanisms involved make the existing methods less efficient and hard to be trained end-to-end. In this paper, we propose a novel attention-based convolutional neural network (CNN) which regulates multiple object parts among different input images. Our method first learns multiple attention region features of each input image through the one-squeeze multi-excitation (OSME) module, and then apply the multi-attention multi-class constraint (MAMC) in a metric learning framework. For each anchor feature, the MAMC functions by pulling same-attention same-class features closer, while pushing different-attention or different-class features away. Our method can be easily trained end-to-end, and is highly efficient which requires only one training stage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog species dataset that surpasses similar existing datasets by category coverage, data volume and annotation quality. Extensive experiments are conducted to show the substantial improvements of our method on four benchmark datasets.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Behavioral research
KW  - Image recognition
KW  - Neural networks
U2  - Attention-based learning
U2  - Benchmark datasets
U2  - Convolutional Neural Networks (CNN)
U2  - Fine grained
U2  - Metric learning
U2  - Multi-attention Multi-class constraint
U2  - Multiple objects
U2  - Visual Attention
DO  - 10.1007/978-3-030-01270-0_49
L2  - http://dx.doi.org/10.1007/978-3-030-01270-0_49
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Open-World Stereo Video Matching with Deep RNN
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Zhong, Yiran
A1  - Li, Hongdong
A1  - Dai, Yuchao
AD  - Australian National University, Canberra, AustraliaNorthwestern Polytechnical University, Xian, ChinaAustralian Centre for Robotic Vision, Canberra, AustraliaData61 CSIRO, Canberra, Australia
VL  - 11206 LNCS
PY  - 2018
U1  - 20184406005998
SP  - 104
EP  - 119
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - Deep Learning based stereo matching methods have shown great successes and achieved top scores across different benchmarks. However, like most data-driven methods, existing deep stereo matching networks suffer from some well-known drawbacks such as requiring large amount of labeled training data, and that their performances are fundamentally limited by the generalization ability. In this paper, we propose a novel Recurrent Neural Network (RNN) that takes a continuous (possibly previously unseen) stereo video as input, and directly predicts a depth-map at each frame without a pre-training process, and without the need of ground-truth depth-maps as supervision. Thanks to the recurrent nature (provided by two convolutional-LSTM blocks), our network is able to memorize and learn from its past experiences, and modify its inner parameters (network weights) to adapt to previously unseen or unfamiliar environments. This suggests a remarkable generalization ability of the net, making it applicable in an open world setting. Our method works robustly with changes in scene content, image statistics, and lighting and season conditions etc. By extensive experiments, we demonstrate that the proposed method seamlessly adapts between different scenarios. Equally important, in terms of the stereo matching accuracy, it outperforms state-of-the-art deep stereo approaches on standard benchmark datasets such as KITTI and Middlebury stereo.  2018, Springer Nature Switzerland AG.
KW  - Stereo image processing
KW  - Computer vision
KW  - Convolution
KW  - Deep learning
KW  - Long short-term memory
KW  - Recurrent neural networks
U2  - Convolutional LSTM
U2  - Data-driven methods
U2  - Generalization ability
U2  - Labeled training data
U2  - Open world
U2  - Recurrent neural network (RNN)
U2  - Stereo matching method
U2  - Stereo video
DO  - 10.1007/978-3-030-01216-8_7
L2  - http://dx.doi.org/10.1007/978-3-030-01216-8_7
ER  - 


TY  - CONF
N1  - Compilation and indexing terms, Copyright 2019 Elsevier Inc.
TI  - Rethinking the Form of Latent States in Image Captioning
BT  - 15th European Conference on Computer Vision, ECCV 2018, September 8, 2018  -  September 14, 2018
T3  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)
A1  - Dai, Bo
A1  - Ye, Deming
A1  - Lin, Dahua
AD  - CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong, Shatin, Hong KongDepartment of Computer Science and Technology, Tsinghua University, Beijing, China
VL  - 11209 LNCS
PY  - 2018
U1  - 20184305976853
SP  - 294
EP  - 310
SN  - 03029743
CY  - Munich, Germany
PB  - Springer Verlag
N2  - RNNs and their variants have been widely adopted for image captioning. In RNNs, the production of a caption is driven by a sequence of latent states. Existing captioning models usually represent latent states as vectors, taking this practice for granted. We rethink this choice and study an alternative formulation, namely using two-dimensional maps to encode latent states. This is motivated by the curiosity about a question: how the spatial structures in the latent states affect the resultant captions? Our study on MSCOCO and Flickr30k leads to two significant observations. First, the formulation with 2D states is generally more effective in captioning, consistently achieving higher performance with comparable parameter sizes. Second, 2D states preserve spatial locality. Taking advantage of this, we visually reveal the internal dynamics in the process of caption generation, as well as the connections between input visual domain and output linguistic domain.  2018, Springer Nature Switzerland AG.
KW  - Computer vision
KW  - Artificial intelligence
KW  - Computer science
KW  - Computers
U2  - Image captioning
U2  - Internal dynamics
U2  - Latent state
U2  - Spatial locality
U2  - Spatial structure
U2  - Two-dimensional map
DO  - 10.1007/978-3-030-01228-1_18
L2  - http://dx.doi.org/10.1007/978-3-030-01228-1_18
ER  - 



